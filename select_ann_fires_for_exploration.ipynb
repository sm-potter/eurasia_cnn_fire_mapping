{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9885abc5-3295-4b4d-91a5-beb3ea233ed1",
   "metadata": {},
   "source": [
    "This script will select 50 random fires from annas goodd polygon ids, and ensure there is existing data so I can check the 30 day NDSI composites.  I will need to predicted the combined old, combiined monthly and combined NDSI models on these fires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad83564-1678-4616-bbef-d7db493226aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Step 1: Read the shapefile\n",
    "shapefile_path = '/explore/nobackup/people/spotter5/cnn_mapping/Russia/good_polys_anna.shp'\n",
    "gdf = gpd.read_file(shapefile_path).to_crs(epsg=3413)\n",
    "\n",
    "# Step 2: Calculate the area in square meters\n",
    "gdf['area_m2'] = gdf.geometry.area\n",
    "\n",
    "# Step 3: Sort by area to balance small and large fires, with largest areas first\n",
    "gdf_sorted = gdf.sort_values(by='area_m2', ascending=False).head(500)\n",
    "\n",
    "# Step 4: Split the GeoDataFrame into northern and southern regions\n",
    "gdf_sorted['centroid_y'] = gdf_sorted.geometry.centroid.y\n",
    "median_latitude = gdf_sorted['centroid_y'].median()\n",
    "\n",
    "northern_half = gdf_sorted[gdf_sorted['centroid_y'] >= median_latitude]\n",
    "southern_half = gdf_sorted[gdf_sorted['centroid_y'] < median_latitude]\n",
    "\n",
    "# Step 5: IDs that must be included\n",
    "required_ids = [1679, 3138, 3336, 3381, 4518, 5311, 15063, 15255, 15868, 15894, 16086]\n",
    "\n",
    "# Ensure required IDs are in the northern or southern half as appropriate\n",
    "required_north_ids = [id_ for id_ in required_ids if id_ in northern_half['ID'].tolist()]\n",
    "required_south_ids = [id_ for id_ in required_ids if id_ in southern_half['ID'].tolist()]\n",
    "\n",
    "# Adjust the number of remaining samples to take from each half\n",
    "num_south_samples = 100 - len(required_south_ids)\n",
    "num_north_samples = 100 - len(required_north_ids)\n",
    "\n",
    "sampled_south_ids = random.sample([id_ for id_ in southern_half['ID'].tolist() if id_ not in required_ids], num_south_samples)\n",
    "sampled_north_ids = random.sample([id_ for id_ in northern_half['ID'].tolist() if id_ not in required_ids], num_north_samples)\n",
    "\n",
    "# Combine the sampled IDs with the required IDs\n",
    "sampled_ids_100 = sampled_south_ids + sampled_north_ids + required_south_ids + required_north_ids\n",
    "\n",
    "# Step 6: Filter the corresponding 100 .tif files\n",
    "ndsi_path = '/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_ndsi_composites'\n",
    "all_tif_files = os.listdir(ndsi_path)\n",
    "\n",
    "# Extract the numbers from the file names, filter out files that don't match the pattern\n",
    "selected_tif_files = []\n",
    "for f in all_tif_files:\n",
    "    try:\n",
    "        file_id = int(f.split('_')[1].split('.')[0])\n",
    "        if file_id in sampled_ids_100:\n",
    "            selected_tif_files.append(f)\n",
    "    except (ValueError, IndexError):\n",
    "        continue\n",
    "\n",
    "# Step 7: Randomly sample 50 files from the selected 100\n",
    "sampled_tif_files_50 = random.sample(selected_tif_files, 50)\n",
    "\n",
    "# Extract the IDs corresponding to the sampled 50 files\n",
    "sampled_ids_50 = [int(f.split('_')[1].split('.')[0]) for f in sampled_tif_files_50]\n",
    "\n",
    "# Ensure required IDs are included in the final selection\n",
    "sampled_ids_50.extend([id_ for id_ in required_ids if id_ not in sampled_ids_50])\n",
    "\n",
    "# Step 8: Filter the shapefile to include only the selected 50 IDs\n",
    "filtered_gdf = gdf[gdf['ID'].isin(sampled_ids_50)]\n",
    "\n",
    "\n",
    "# Save the new shapefile\n",
    "output_shapefile_path = '/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_sampled.shp'\n",
    "filtered_gdf.to_file(output_shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf279a4c-2a4c-4286-9a45-d1684d70dfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc64a8-3a5b-416e-b04f-1d1f90852d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
