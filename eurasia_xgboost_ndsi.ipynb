{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07763a67-f813-418e-bb37-9242002d45ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45419 instead\n",
      "  warnings.warn(\n",
      "[14:49:05] Task [xgboost.dask-0]:tcp://127.0.0.1:40407 got rank 0\n",
      "[14:49:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1724807734561/work/src/learner.cc:740: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[14:49:14] [0]\ttest-logloss:0.68826\n",
      "[14:49:14] [1]\ttest-logloss:0.68346\n",
      "[14:49:14] [2]\ttest-logloss:0.67876\n",
      "[14:49:14] [3]\ttest-logloss:0.67416\n",
      "[14:49:14] [4]\ttest-logloss:0.66964\n",
      "[14:49:14] [5]\ttest-logloss:0.66521\n",
      "[14:49:14] [6]\ttest-logloss:0.66086\n",
      "[14:49:14] [7]\ttest-logloss:0.65660\n",
      "[14:49:15] [8]\ttest-logloss:0.65242\n",
      "[14:49:15] [9]\ttest-logloss:0.64831\n",
      "[14:49:15] [10]\ttest-logloss:0.64428\n",
      "[14:49:15] [11]\ttest-logloss:0.64033\n",
      "[14:49:15] [12]\ttest-logloss:0.63645\n",
      "[14:49:15] [13]\ttest-logloss:0.63265\n",
      "[14:49:15] [14]\ttest-logloss:0.62891\n",
      "[14:49:15] [15]\ttest-logloss:0.62524\n",
      "[14:49:15] [16]\ttest-logloss:0.62164\n",
      "[14:49:16] [17]\ttest-logloss:0.61810\n",
      "[14:49:16] [18]\ttest-logloss:0.61462\n",
      "[14:49:16] [19]\ttest-logloss:0.61121\n",
      "[14:49:16] [20]\ttest-logloss:0.60786\n",
      "[14:49:16] [21]\ttest-logloss:0.60457\n",
      "[14:49:16] [22]\ttest-logloss:0.60133\n",
      "[14:49:16] [23]\ttest-logloss:0.59816\n",
      "[14:49:16] [24]\ttest-logloss:0.59503\n",
      "[14:49:17] [25]\ttest-logloss:0.59197\n",
      "[14:49:17] [26]\ttest-logloss:0.58895\n",
      "[14:49:17] [27]\ttest-logloss:0.58599\n",
      "[14:49:17] [28]\ttest-logloss:0.58308\n",
      "[14:49:17] [29]\ttest-logloss:0.58022\n",
      "[14:49:17] [30]\ttest-logloss:0.57741\n",
      "[14:49:17] [31]\ttest-logloss:0.57464\n",
      "[14:49:17] [32]\ttest-logloss:0.57192\n",
      "[14:49:18] [33]\ttest-logloss:0.56925\n",
      "[14:49:18] [34]\ttest-logloss:0.56663\n",
      "[14:49:18] [35]\ttest-logloss:0.56405\n",
      "[14:49:18] [36]\ttest-logloss:0.56151\n",
      "[14:49:18] [37]\ttest-logloss:0.55901\n",
      "[14:49:18] [38]\ttest-logloss:0.55656\n",
      "[14:49:18] [39]\ttest-logloss:0.55414\n",
      "[14:49:18] [40]\ttest-logloss:0.55177\n",
      "[14:49:18] [41]\ttest-logloss:0.54943\n",
      "[14:49:19] [42]\ttest-logloss:0.54714\n",
      "[14:49:19] [43]\ttest-logloss:0.54488\n",
      "[14:49:19] [44]\ttest-logloss:0.54266\n",
      "[14:49:19] [45]\ttest-logloss:0.54047\n",
      "[14:49:19] [46]\ttest-logloss:0.53832\n",
      "[14:49:19] [47]\ttest-logloss:0.53621\n",
      "[14:49:19] [48]\ttest-logloss:0.53412\n",
      "[14:49:19] [49]\ttest-logloss:0.53208\n",
      "[14:49:20] [50]\ttest-logloss:0.53006\n",
      "[14:49:20] [51]\ttest-logloss:0.52808\n",
      "[14:49:20] [52]\ttest-logloss:0.52613\n",
      "[14:49:20] [53]\ttest-logloss:0.52421\n",
      "[14:49:20] [54]\ttest-logloss:0.52233\n",
      "[14:49:20] [55]\ttest-logloss:0.52047\n",
      "[14:49:20] [56]\ttest-logloss:0.51864\n",
      "[14:49:20] [57]\ttest-logloss:0.51684\n",
      "[14:49:21] [58]\ttest-logloss:0.51507\n",
      "[14:49:21] [59]\ttest-logloss:0.51333\n",
      "[14:49:21] [60]\ttest-logloss:0.51161\n",
      "[14:49:21] [61]\ttest-logloss:0.50992\n",
      "[14:49:21] [62]\ttest-logloss:0.50826\n",
      "[14:49:21] [63]\ttest-logloss:0.50662\n",
      "[14:49:21] [64]\ttest-logloss:0.50501\n",
      "[14:49:21] [65]\ttest-logloss:0.50343\n",
      "[14:49:21] [66]\ttest-logloss:0.50187\n",
      "[14:49:22] [67]\ttest-logloss:0.50033\n",
      "[14:49:22] [68]\ttest-logloss:0.49882\n",
      "[14:49:22] [69]\ttest-logloss:0.49733\n",
      "[14:49:22] [70]\ttest-logloss:0.49587\n",
      "[14:49:22] [71]\ttest-logloss:0.49442\n",
      "[14:49:22] [72]\ttest-logloss:0.49300\n",
      "[14:49:22] [73]\ttest-logloss:0.49160\n",
      "[14:49:22] [74]\ttest-logloss:0.49022\n",
      "[14:49:23] [75]\ttest-logloss:0.48887\n",
      "[14:49:23] [76]\ttest-logloss:0.48753\n",
      "[14:49:23] [77]\ttest-logloss:0.48622\n",
      "[14:49:23] [78]\ttest-logloss:0.48493\n",
      "[14:49:23] [79]\ttest-logloss:0.48365\n",
      "[14:49:23] [80]\ttest-logloss:0.48240\n",
      "[14:49:23] [81]\ttest-logloss:0.48116\n",
      "[14:49:23] [82]\ttest-logloss:0.47994\n",
      "[14:49:24] [83]\ttest-logloss:0.47874\n",
      "[14:49:24] [84]\ttest-logloss:0.47756\n",
      "[14:49:24] [85]\ttest-logloss:0.47640\n",
      "[14:49:24] [86]\ttest-logloss:0.47525\n",
      "[14:49:24] [87]\ttest-logloss:0.47412\n",
      "[14:49:24] [88]\ttest-logloss:0.47301\n",
      "[14:49:24] [89]\ttest-logloss:0.47192\n",
      "[14:49:24] [90]\ttest-logloss:0.47084\n",
      "[14:49:25] [91]\ttest-logloss:0.46978\n",
      "[14:49:25] [92]\ttest-logloss:0.46873\n",
      "[14:49:25] [93]\ttest-logloss:0.46770\n",
      "[14:49:25] [94]\ttest-logloss:0.46669\n",
      "[14:49:25] [95]\ttest-logloss:0.46569\n",
      "[14:49:25] [96]\ttest-logloss:0.46470\n",
      "[14:49:25] [97]\ttest-logloss:0.46374\n",
      "[14:49:25] [98]\ttest-logloss:0.46278\n",
      "[14:49:26] [99]\ttest-logloss:0.46184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82661075\n",
      "Precision: 0.8786492619480638\n",
      "Recall: 0.7578885714840009\n",
      "F1 Score: 0.8138134302422567\n",
      "IoU: 0.6860754041486603\n",
      "Classification metrics saved to /explore/nobackup/people/spotter5/cnn_mapping/nbac_training/xgboost/xgboost_ea_stratified_sampled_results_ndsi.csv\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask_ml.model_selection as dcv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Set up Dask client to use multiple GPUs. Note to set temp space\n",
    "local_directory='/explore/nobackup/people/spotter5/temp_dir',\n",
    "client = Client(\n",
    "                n_workers=1, threads_per_worker=1, processes=True, memory_limit='28GB')\n",
    "\n",
    "\n",
    "# Set up a Dask cluster that assigns each worker to a separate GPU\n",
    "# cluster = LocalCUDACluster(\n",
    "#     n_workers=2,  # Number of GPUs you have\n",
    "#     threads_per_worker=1,  # One thread per worker\n",
    "#     memory_limit='28GB',  # Set memory limit for each worker\n",
    "#     local_directory='/explore/nobackup/people/spotter5/temp_dir'  # Temporary directory\n",
    "# )\n",
    "\n",
    "# Connect to the Dask client\n",
    "# client = Client(cluster)\n",
    "# Create an output directory if it doesn't exist\n",
    "out_path = '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/xgboost'\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# Load the data as a Dask DataFrame\n",
    "# df = dd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_na.parquet', \n",
    "#                      columns=['dNBR', 'dNDVI', 'dNDII', 'y'])\n",
    "\n",
    "# # Shuffle the dataframe (keeps it in Dask format)\n",
    "# df = df.shuffle(on='dNBR')\n",
    "\n",
    "# # Sample 100,000 rows from the Dask DataFrame\n",
    "# df = df.sample(frac=100000 / len(df), random_state=42)\n",
    "\n",
    "# sampled_out_path = os.path.join(out_path, 'sampled_100k.parquet')\n",
    "# df.to_parquet(sampled_out_path, write_index = False)\n",
    "# print(f\"Sampled data saved to {sampled_out_path}\")\n",
    "\n",
    "df = dd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_anna_stratified_sampled_ndsi.parquet')\n",
    "# df2 = dd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_nbac_stratified_sampled.parquet')\n",
    "\n",
    "# df = dd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_anna_sampled_ndsi.parquet')\n",
    "# df2 = dd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_nbac_sampled.parquet')\n",
    "# df1 = df1.repartition(npartitions=10) \n",
    "# df2 = df2.repartition(npartitions=10) \n",
    "\n",
    "# df = dd.concat([df1, df2])\n",
    "# df = df.repartition(npartitions=10)\n",
    "\n",
    "# Specify predictors and target variables, converting directly from Dask DataFrame to Dask Array\n",
    "X = df[['dNBR', 'dNDVI', 'dNDII']].to_dask_array(lengths=True)\n",
    "y = df['y'].to_dask_array(lengths=True)\n",
    "\n",
    "# X = df[['dNBR', 'dNDVI', 'dNDII']].values\n",
    "# y = df['y'].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = dcv.train_test_split(X, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "# X_train, X_test, y_train, y_test = dcv.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to DaskDMatrix (XGBoost specific data structure for distributed training)\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "dtest = xgb.dask.DaskDMatrix(client, X_test, y_test)\n",
    "\n",
    "# Set up XGBoost parameters for GPU training\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 8,\n",
    "    'n_estimators': 1000,\n",
    "    'tree_method': 'hist',  # Use histogram-based method\n",
    "    'eval_metric': 'logloss',  # Metric for binary classification\n",
    "    'device': 'cuda',  # Use CUDA for GPU support\n",
    "}\n",
    "\n",
    "# params = {\n",
    "#     'objective': 'binary:logistic',  # Binary classification\n",
    "#     'learning_rate': 0.1,\n",
    "#     'max_depth': 8,\n",
    "#     'n_estimators': 1000,\n",
    "#     'tree_method': 'gpu_hist',  # Use GPU-accelerated histogram algorithm\n",
    "#     'eval_metric': 'logloss',\n",
    "#     'predictor': 'gpu_predictor'  # Use GPU predictor\n",
    "# }\n",
    "\n",
    "\n",
    "# Train the model with early stopping\n",
    "model = xgb.dask.train(\n",
    "    client, \n",
    "    params, \n",
    "    dtrain, \n",
    "    num_boost_round=100,\n",
    "    evals=[(dtest, 'test')],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Make predictions (the output will be probabilities for binary classification)\n",
    "y_pred_proba = xgb.dask.predict(client, model, X_test)\n",
    "\n",
    "# Convert predicted probabilities to binary predictions\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays for sklearn metrics\n",
    "y_pred_np = y_pred.compute()\n",
    "y_test_np = y_test.compute()\n",
    "\n",
    "# Calculate classification metrics using sklearn\n",
    "accuracy = accuracy_score(y_test_np, y_pred_np)\n",
    "precision = precision_score(y_test_np, y_pred_np, average='binary')\n",
    "recall = recall_score(y_test_np, y_pred_np, average='binary')\n",
    "f1 = f1_score(y_test_np, y_pred_np, average='binary')\n",
    "# Calculate IoU using confusion matrix\n",
    "cm = confusion_matrix(y_test_np, y_pred_np)\n",
    "TP = cm[1, 1]  # True Positives\n",
    "FP = cm[0, 1]  # False Positives\n",
    "FN = cm[1, 0]  # False Negatives\n",
    "IoU = TP / (TP + FP + FN)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"IoU: {IoU}\")\n",
    "\n",
    "# Save the classification metrics to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'IoU'],\n",
    "    'Value': [accuracy, precision, recall, f1, IoU]\n",
    "})\n",
    "\n",
    "results.to_csv(os.path.join(out_path, 'xgboost_ea_stratified_sampled_results_ndsi.csv'), index=False)\n",
    "print(f\"Classification metrics saved to {os.path.join(out_path, 'xgboost_ea_stratified_sampled_results_ndsi.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c11f11-3552-413b-bd22-38fc4f5832c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.917146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.869897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.500310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.635259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IoU</td>\n",
       "      <td>0.465480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.917146\n",
       "1  Precision  0.869897\n",
       "2     Recall  0.500310\n",
       "3   F1 Score  0.635259\n",
       "4        IoU  0.465480"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdbfcd-83fc-454a-86c0-a647f1f29096",
   "metadata": {},
   "source": [
    "Above trains a small file, do it in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda66a07-4456-4fde-a94b-7b47e2c67862",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing partition 1 of 11642\n",
      "Processing partition 2 of 11642\n",
      "Processing partition 3 of 11642\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set up Dask client for distributed training\n",
    "client = Client(n_workers=4, threads_per_worker=1, processes=True, memory_limit='28GB')\n",
    "\n",
    "# Output directory\n",
    "out_path = '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/xgboost'\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# Read the Parquet directory with specific columns to control memory usage\n",
    "df = dd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_na.parquet', \n",
    "                     columns=['dNBR', 'dNDVI', 'dNDII', 'y'])\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 8,\n",
    "    'eval_metric': 'logloss',  # Metric for binary classification\n",
    "    'tree_method': 'hist',  # Use histogram-based method for efficient training\n",
    "    'device': 'cuda',  # Use GPU for training\n",
    "}\n",
    "\n",
    "# Get the number of partitions\n",
    "npartitions = df.npartitions  # Number of partitions in the dataset\n",
    "\n",
    "# Initialize booster\n",
    "booster = None\n",
    "\n",
    "# Process data in chunks (by partition)\n",
    "for i in range(npartitions):\n",
    "    print(f\"Processing partition {i + 1} of {npartitions}\")\n",
    "    \n",
    "    # Compute a single partition at a time\n",
    "    df_partition = df.get_partition(i).compute()\n",
    "\n",
    "    # Split the partition into features (X) and target (y)\n",
    "    X_batch = df_partition[['dNBR', 'dNDVI', 'dNDII']].values\n",
    "    y_batch = df_partition['y'].values\n",
    "    \n",
    "    # Convert to DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(X_batch, label=y_batch)\n",
    "    \n",
    "    # If this is the first partition, train the initial model\n",
    "    if booster is None:\n",
    "        booster = xgb.train(params, dtrain, num_boost_round=10)  # Initial training on the first batch\n",
    "    else:\n",
    "        # Update the model with the next partition\n",
    "        booster.update(dtrain, iteration=i)\n",
    "\n",
    "# After training on all partitions, make predictions on the test data\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = dcv.train_test_split(df[['dNBR', 'dNDVI', 'dNDII']].compute(), df['y'].compute(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test data to DMatrix\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proba = booster.predict(dtest)\n",
    "\n",
    "# Convert predicted probabilities to binary predictions\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate classification metrics using sklearn\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "# Calculate IoU using confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TP = cm[1, 1]  # True Positives\n",
    "FP = cm[0, 1]  # False Positives\n",
    "FN = cm[1, 0]  # False Negatives\n",
    "\n",
    "# IoU = True Positives / (True Positives + False Positives + False Negatives)\n",
    "IoU = TP / (TP + FP + FN)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"IoU: {IoU}\")\n",
    "\n",
    "# Save the classification metrics to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'IoU'],\n",
    "    'Value': [accuracy, precision, recall, f1, IoU]\n",
    "})\n",
    "\n",
    "results.to_csv(os.path.join(out_path, 'xgboost_classification_results_batch.csv'), index=False)\n",
    "print(f\"Classification metrics saved to {os.path.join(out_path, 'xgboost_classification_results_batch.csv')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36908bc0-94c3-4632-9841-7ca92a568d30",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:464: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n",
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/distributed/client.py:3162: UserWarning: Sending large graph of size 30.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/distributed/client.py:3162: UserWarning: Sending large graph of size 30.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'evals_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train the model with Dask, capturing evaluation results\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Get the trained booster model\u001b[39;00m\n\u001b[1;32m     44\u001b[0m booster \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbooster\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'evals_result'"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set up Dask client for distributed training\n",
    "client = Client(local_directory='/explore/nobackup/people/spotter5/temp_dir', n_workers=4, threads_per_worker=1, processes=True, memory_limit='28GB')\n",
    "\n",
    "# Output directory\n",
    "out_path = '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/xgboost'\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# Read the Parquet directory lazily (without loading it all into memory)\n",
    "df = dd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_na.parquet', \n",
    "                     columns=['dNBR', 'dNDVI', 'dNDII', 'y'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['dNBR', 'dNDVI', 'dNDII']], df['y'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to DaskDMatrix for XGBoost\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "dtest = xgb.dask.DaskDMatrix(client, X_test, y_test)\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 8,\n",
    "    'eval_metric': 'logloss',  # Metric for binary classification\n",
    "    'tree_method': 'hist',  # Use histogram-based method\n",
    "    'device': 'cuda',  # Use GPU for training\n",
    "}\n",
    "\n",
    "# Dictionary to store evaluation results\n",
    "evals_result = {}\n",
    "\n",
    "# Train the model with Dask, capturing evaluation results\n",
    "output = xgb.dask.train(client, params, dtrain, num_boost_round=100, evals=[(dtest, 'test')], evals_result=evals_result, verbose_eval=10)\n",
    "\n",
    "# Get the trained booster model\n",
    "booster = output['booster']\n",
    "\n",
    "# Print loss for each iteration\n",
    "for i, logloss in enumerate(evals_result['test']['logloss']):\n",
    "    print(f\"Iteration {i + 1}: Log Loss = {logloss}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proba = xgb.dask.predict(client, booster, X_test)\n",
    "\n",
    "# Convert predicted probabilities to binary predictions\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays for sklearn metrics\n",
    "y_pred_np = y_pred.compute()\n",
    "y_test_np = y_test.compute()\n",
    "\n",
    "# Calculate classification metrics using sklearn\n",
    "accuracy = accuracy_score(y_test_np, y_pred_np)\n",
    "precision = precision_score(y_test_np, y_pred_np, average='binary')\n",
    "recall = recall_score(y_test_np, y_pred_np, average='binary')\n",
    "f1 = f1_score(y_test_np, y_pred_np, average='binary')\n",
    "\n",
    "# Calculate IoU using confusion matrix\n",
    "cm = confusion_matrix(y_test_np, y_pred_np)\n",
    "TP = cm[1, 1]  # True Positives\n",
    "FP = cm[0, 1]  # False Positives\n",
    "FN = cm[1, 0]  # False Negatives\n",
    "\n",
    "# IoU = True Positives / (True Positives + False Positives + False Negatives)\n",
    "IoU = TP / (TP + FP + FN)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"IoU: {IoU}\")\n",
    "\n",
    "# Save the classification metrics to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'IoU'],\n",
    "    'Value': [accuracy, precision, recall, f1, IoU]\n",
    "})\n",
    "\n",
    "results.to_csv(os.path.join(out_path, 'xgboost_classification_results_batch.csv'), index=False)\n",
    "print(f\"Classification metrics saved to {os.path.join(out_path, 'xgboost_classification_results_batch.csv')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554174ae-ed6e-4548-a41e-2604fc4264c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11642"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "# Read the data in chunks using Dask DataFrame (partitioning the data to avoid loading the full dataset into memory)\n",
    "df = dd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_na.parquet', \n",
    "                     columns=['dNBR', 'dNDVI', 'dNDII', 'y'])\n",
    "df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44a07a34-5926-4630-b1c2-cdc11aceee8d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e504e79b-4aca-4b27-a914-cf11a49747b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [14:50:58] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1724807734561/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU support is enabled.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "from xgboost import DMatrix\n",
    "\n",
    "# Check if GPU support is available\n",
    "params = {'tree_method': 'gpu_hist'}\n",
    "dtrain = DMatrix([[1, 2], [3, 4]], label=[1, 0])\n",
    "bst = xgb.train(params, dtrain, num_boost_round=2)\n",
    "\n",
    "print(\"GPU support is enabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7d103f-3c7e-4f7e-a55b-dfcf95555880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000000, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_anna_ndsi.parquet',   columns=['dNBR', 'dNDVI', 'dNDII', 'y'])\n",
    "\n",
    "# df.shape\n",
    "\n",
    "df = df.sample(n = 100000000)\n",
    "\n",
    "df.to_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_anna_sampled.parquet', index = False)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80168533-b29b-4106-8143-fe3e5afd7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_anna_ndsi.parquet',   columns=['dNBR', 'dNDVI', 'dNDII', 'y'])\n",
    "\n",
    "\n",
    "# Sample 100,000,000 rows from each class (y=0 and y=1)\n",
    "df_0 = df[df['y'] == 0].sample(n=int(100000000/2), random_state=42)\n",
    "df_1 = df[df['y'] == 1].sample(n= int(100000000/2), random_state=42)\n",
    "\n",
    "# Concatenate the stratified samples\n",
    "df_stratified = pd.concat([df_0, df_1])\n",
    "\n",
    "# Shuffle the DataFrame (optional)\n",
    "df_stratified = df_stratified.sample(frac=1, random_state=42)\n",
    "\n",
    "# Save the stratified sample to a new parquet file\n",
    "df_stratified.to_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_anna_stratified_sampled_ndsi.parquet', \n",
    "                         index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50acb21d-20f1-4dfe-a0a0-e97e1007926e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a2d50-efa0-489e-a941-fba46027d232",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
