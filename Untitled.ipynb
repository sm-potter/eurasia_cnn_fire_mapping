{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ea8520b-9802-4026-95be-f7923da476e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#get all the pathways\n",
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_training_files.csv')['Files'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_validation_files.csv')['Files'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "# training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_training_files.csv')['Files'].tolist()\n",
    "# validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_validation_files.csv')['Files'].tolist()\n",
    "# testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "good_frame= pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_no_grass.csv')\n",
    "\n",
    "\n",
    "def get_good(in_list, good_frame):\n",
    "    \n",
    "    final = []\n",
    "    \n",
    "    for i in in_list:\n",
    "\n",
    "        #i is the ids we want to keep\n",
    "        try:\n",
    "\n",
    "            #this is the id from all fires\n",
    "            in_id = int(i.split('/')[-1].split('_')[2].replace('.npy', ''))\n",
    "\n",
    "            if in_id in good_frame['ID']:\n",
    "\n",
    "                final.append(i)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return final\n",
    "\n",
    "training_names2 = get_good(training_names, good_ids)\n",
    "validation_names = get_good(validation_names, good_ids)\n",
    "testing_names = get_good(testing_names, good_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8388e7b6-1bcd-4989-be8b-979bc7ef79dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366684, 235160)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_names), len(training_names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46ff25c9-4664-4ac6-a826-c85e9b95564f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(good_ids['ID'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3fd855-27b4-4464-af94-f1d7a58fff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30347a54-5fa8-4e80-b2b0-237894d62c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263792\n",
      "32974\n",
      "32974\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_ndsi_composites_training_files.csv')['Files'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_ndsi_composites_validation_files.csv')['Files'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_ndsi_composites_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_training_files.csv')['Files'].tolist()\n",
    "validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_validation_files.csv')['Files'].tolist()\n",
    "testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "\n",
    "training_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_training_files.csv')['Files'].tolist()\n",
    "validation_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_validation_files.csv')['Files'].tolist()\n",
    "testing_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "print(len(training_names))\n",
    "print(len(validation_names))\n",
    "print(len(testing_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f30825-8b72-4e16-b754-b5c5a3918a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314783\n",
      "39348\n",
      "39348\n"
     ]
    }
   ],
   "source": [
    "print(len(training_names2))\n",
    "print(len(validation_names2))\n",
    "print(len(testing_names2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ccf303-0a47-44b0-8512-5aecac7e5669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43883\n",
      "5485\n",
      "5486\n"
     ]
    }
   ],
   "source": [
    "print(len(training_names3))\n",
    "print(len(validation_names3))\n",
    "print(len(testing_names3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21eed760-0aa1-4170-aece-3c1fa68e9872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/0_2_8906.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/2_3_7406.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/7_6_3395.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/14_10_3509.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/21_25_6777.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/1_2_6546.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/6_3_8748.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/2_1_5670.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/3_3_7317.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/2_1_6556.npy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_names2[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99bca207-dae4-4879-b17a-69b200d4b661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/14_13_3241.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/6_7_3252.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/3_1_5609.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/1_3_3418.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/7_22_9617.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/2_1_8688.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/3_5_7054.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/7_13_8495.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/2_0_8609.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_ndsi_composites_subs_0_128/2_3_9278.npy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_names2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55337fe-9712-466f-a4a2-400fca0eec65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/7_0_697.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/5_0_1108.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/4_6_1168.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/6_9_879.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/13_3_560.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/4_5_361.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/5_4_541.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/3_2_414.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/0_9_423.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/8_0_875.npy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_names3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89bde887-f520-462a-a17b-65a3f4cd0a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/0_3_11.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/4_15_427.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/3_1_548.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/5_7_557.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/2_2_524.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/1_7_1169.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/4_2_1143.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/16_12_469.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/1_10_488.npy',\n",
       " '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_ndsi_composites_subs_0_128/2_7_558.npy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_names3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3875cea1-ae70-4501-b3bc-511653fa6eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(testing_names3).intersection(training_names3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155d5bf2-fbfb-4c1f-84df-4d2d9055d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n",
      "       6      7      8\n",
      "0   21.0  -68.0   -9.0\n",
      "1  974.0  522.0  686.0\n",
      "Number of devices: 1\n",
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\tunet_output_sup0_activation\n",
      "\tunet_output_sup1_activation\n",
      "\tunet_output_final_activation\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'update_0_315/StatefulPartitionedCall' defined at (most recent call last):\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3882244/2635700870.py\", line 406, in <module>\n      history = model_unet_from_scratch.fit(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'update_0_315/StatefulPartitionedCall'\nDetected at node 'update_0_315/StatefulPartitionedCall' defined at (most recent call last):\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3882244/2635700870.py\", line 406, in <module>\n      history = model_unet_from_scratch.fit(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'update_0_315/StatefulPartitionedCall'\n2 root error(s) found.\n  (0) INTERNAL:  libdevice not found at ./libdevice.10.bc\n\t [[{{node update_0_315/StatefulPartitionedCall}}]]\n\t [[div_no_nan_13/ReadVariableOp_1/_352]]\n  (1) INTERNAL:  libdevice not found at ./libdevice.10.bc\n\t [[{{node update_0_315/StatefulPartitionedCall}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_66381]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 406\u001b[0m\n\u001b[1;32m    396\u001b[0m     model_unet_from_scratch\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    397\u001b[0m                                     \u001b[38;5;66;03m# loss = loss,\u001b[39;00m\n\u001b[1;32m    398\u001b[0m                                     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    402\u001b[0m                                       sm\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mIOUScore(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m    403\u001b[0m                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m#fit the model\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_unet_from_scratch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# model_unet_from_scratch.save(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_079_128.h5\")\u001b[39;00m\n\u001b[1;32m    414\u001b[0m model_unet_from_scratch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_unet1.tf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'update_0_315/StatefulPartitionedCall' defined at (most recent call last):\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3882244/2635700870.py\", line 406, in <module>\n      history = model_unet_from_scratch.fit(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'update_0_315/StatefulPartitionedCall'\nDetected at node 'update_0_315/StatefulPartitionedCall' defined at (most recent call last):\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3882244/2635700870.py\", line 406, in <module>\n      history = model_unet_from_scratch.fit(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'update_0_315/StatefulPartitionedCall'\n2 root error(s) found.\n  (0) INTERNAL:  libdevice not found at ./libdevice.10.bc\n\t [[{{node update_0_315/StatefulPartitionedCall}}]]\n\t [[div_no_nan_13/ReadVariableOp_1/_352]]\n  (1) INTERNAL:  libdevice not found at ./libdevice.10.bc\n\t [[{{node update_0_315/StatefulPartitionedCall}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_66381]"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Read in packages\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "import os\n",
    "import segmentation_models as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import concatenate, Conv2DTranspose, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Input, AvgPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_unet_collection import models\n",
    "import geopandas as gpd\n",
    "# import tensorflow_addons as tfa\n",
    "import logging\n",
    "import time\n",
    "# export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX\n",
    "\n",
    "# os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/home/spotter5/.conda/envs/deeplearning/lib'\n",
    "os.environ['XLA_FLAGS'] = f\"--xla_gpu_cuda_data_dir={os.environ.get('CONDA_PREFIX', '')}\"\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# gpu_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "# for device in gpu_devices:\n",
    "#     tensorflow.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gpu_info = get_ipython().getoutput('nvidia-smi')\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#     print('Not connected to a GPU')\n",
    "# else:\n",
    "#     print(gpu_info)\n",
    "\n",
    "\n",
    "min_max = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "\n",
    "min_max = min_max[['6', '7', '8']]\n",
    "\n",
    "print(min_max)\n",
    "#functin to standardize all bands at once\n",
    "\n",
    "\n",
    "#function to standardize\n",
    "def normalize_meanstd(a, axis=None): \n",
    "    # axis param denotes axes along which mean & std reductions are to be performed\n",
    "    mean = np.mean(a, axis=axis, keepdims=True)\n",
    "    std = np.sqrt(((a - mean)**2).mean(axis=axis, keepdims=True))\n",
    "    return (a - mean) / std\n",
    "\n",
    "#function to normalize\n",
    "def normalize(a, axis=None): \n",
    "    # axis param denotes axes along which mean & std reductions are to be performed\n",
    "    minv = np.min(a, axis=axis, keepdims=True)\n",
    "    maxv = np.max(a, axis=axis, keepdims=True)\n",
    "    return (a - minv) / (maxv - minv)\n",
    "\n",
    "\n",
    "#function to get files from storage bucket\n",
    "def get_files(bucket_path):\n",
    "\n",
    "\t\"\"\"argument is the path to where the numpy\n",
    "\tsave files are located, return a list of filenames\n",
    "\t\"\"\"\n",
    "\tall = []\n",
    "\n",
    "\t#list of files\n",
    "\tfiles = os.listdir(bucket_path)\n",
    "\n",
    "\t#get list of filenames we will use, notte I remove images that don't have a target due to clouds\n",
    "\tfile_names = []\n",
    "\tfor f in files:\n",
    "\n",
    "\t\tif f.endswith('.npy'):\n",
    "\n",
    "\n",
    "\t\t\tall.append(os.path.join(bucket_path, f))\n",
    "\treturn(all)\n",
    "\n",
    "\n",
    "#get all the pathways\n",
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_validation_files.csv')['Files'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_training_files.csv')['Files'].tolist()\n",
    "validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_validation_files.csv')['Files'].tolist()\n",
    "testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "\n",
    "training_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/nbac_training_files.csv')['Files'].tolist()\n",
    "validation_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/nbac_validation_files.csv')['Files'].tolist()\n",
    "testing_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/nbac_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# good_ids= pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/raw_files/ak_ca_1985_clip.csv')\n",
    "\n",
    "\n",
    "# def get_good(in_list, good_frame):\n",
    "    \n",
    "#     final = []\n",
    "#     for i in in_list:\n",
    "\n",
    "#         try:\n",
    "\n",
    "#             in_id = int(i.split('/')[-1].split('_')[2].replace('.npy', ''))\n",
    "\n",
    "#             if in_id in good_frame['ID']:\n",
    "\n",
    "#                 final.append(i)\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     return final\n",
    "\n",
    "# training_names = get_good(training_names, good_ids)\n",
    "# validation_names = get_good(validation_names, good_ids)\n",
    "# testing_names = get_good(testing_names, good_ids)\n",
    "\n",
    "\n",
    "training_names = training_names + training_names2 + training_names3 \n",
    "validation_names = validation_names + validation_names2 + validation_names3\n",
    "testing_names = testing_names + testing_names2 + testing_names3\n",
    "\n",
    "training_names = training_names[:50]\n",
    "validation_names = validation_names[:50]\n",
    "testing_names = testing_names[:50]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#function to normalize within range\n",
    "def normalize(start, end, arr):\n",
    "    width = end - start\n",
    "    res = (arr - np.nanmin(arr))/(np.nanmax(arr)- np.nanmin(arr)) * width + start\n",
    "\n",
    "#     res = (arr - arr.min())/(arr.max() - arr.min()) * width + start\n",
    "    return res\n",
    "\n",
    "class img_gen(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    #will need pre defined variables batch_size, img_size, input_img_paths and target_img_paths\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "\t    self.batch_size = batch_size\n",
    "\t    self.img_size = img_size\n",
    "\t    self.input_img_paths = input_img_paths\n",
    "\t    self.target_img_paths = input_img_paths\n",
    "\n",
    "    #number of batches the generator is supposed to produceis the length of the paths divided by the batch siize\n",
    "    def __len__(self):\n",
    "\t    return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (x)\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (y)\n",
    "\t\t\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\") #create matrix of zeros which will have the dimension height, wideth, n_bands), 8 is the n_bands\n",
    "        \n",
    "  \n",
    "         #start populating x by enumerating over the input img paths\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)\n",
    "            \n",
    "            if img.shape[2] == 4:\n",
    "                img = img[:, :, :-1]\n",
    "\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                img = img[:, :, 6:9]\n",
    "                \n",
    "            # img = img * 1000\n",
    "            img = img.astype(float)\n",
    "            img = np.round(img, 3)\n",
    "            img[img == 0] = -999\n",
    "\n",
    "            img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "            img[img == -999] = np.nan\n",
    "\n",
    "            in_shape = img.shape\n",
    "            \n",
    "            #turn to dataframe to normalize\n",
    "            img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "\t\t\t\n",
    "            img = pd.DataFrame(img)\n",
    "\t\t\t\n",
    "            img.columns = min_max.columns\n",
    "\t\t\t\n",
    "            img = pd.concat([min_max, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "            #normalize 0 to 1\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\t\t\t\n",
    "            img = img.iloc[2:]\n",
    "#\n",
    "#             img = img.values.reshape(in_shape)\n",
    "            img = img.values.reshape(in_shape)\n",
    "\n",
    "#             replace nan with -1\n",
    "            img[np.isnan(img)] = -1\n",
    "\n",
    "#apply standardization\n",
    "# img = normalize(img, axis=(0,1))\n",
    "\n",
    "            img = np.round(img, 3)\n",
    "            #populate x\n",
    "            x[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "\n",
    "        #do tthe same thing for y\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "            img = img.astype(int)\n",
    "\n",
    "            img[img < 0] = 0\n",
    "            img[img >1] = 0\n",
    "            img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "            img[np.isnan(img)] = 0\n",
    "            img = img.astype(int)\n",
    "\n",
    "            # img =  tf.keras.utils.to_categorical(img, num_classes = 2)\n",
    "            # y[j] = np.expand_dims(img, 2) \n",
    "            y[j] = img\n",
    "  \n",
    "       \n",
    "    #Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "    # y[j] -= 1\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Read in the images based on the generator\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "#Initialize GPUS with tensorflow\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "gpu_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tensorflow.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gpu_info = get_ipython().getoutput('nvidia-smi')\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#     print('Not connected to a GPU')\n",
    "# else:\n",
    "#     print(gpu_info)\n",
    "    \n",
    "# # watch -n0.5 nvidia-smi\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# devices = device_lib.list_local_devices()\n",
    "\n",
    "\n",
    "#batch size and img size\n",
    "#15 before\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "# img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "#get images\n",
    "train_gen = img_gen(batch_size, img_size, training_names)\n",
    "val_gen = img_gen(batch_size, img_size, validation_names)\n",
    "test_gen = img_gen(batch_size, img_size, testing_names)\n",
    "#\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "tensorflow.keras.backend.clear_session()\n",
    "LR = 1e-3\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam(learning_rate = 1e-3) #this is 1e-3, default or 'rmsprop'\n",
    "\n",
    "    \n",
    "loss= tensorflow.keras.losses.BinaryFocalCrossentropy(\n",
    "    from_logits=False,\n",
    "    gamma = 2.0,\n",
    "    alpha = 0.25)\n",
    "\n",
    "# f.keras.losses.BinaryFocalCrossentropy(gamma=2.0, alpha=0.25)\n",
    "# loss = tensorflow.keras.losses.BinaryFocalCrossentropy(\n",
    "#     apply_class_balancing=False,\n",
    "#     alpha=0.25,\n",
    "#     gamma=2.0,\n",
    "#     from_logits=False,\n",
    "#     label_smoothing=0.0,\n",
    "#     axis=-1,\n",
    "#     reduction=losses_utils.ReductionV2.AUTO,\n",
    "#     name='binary_focal_crossentropy'\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_net1\",\n",
    "#     verbose=1,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    monitor='val_iou_score',\n",
    "    mode = 'max'),\n",
    "    tensorflow.keras.callbacks.EarlyStopping(monitor='val_iou_score', mode = 'max',  patience=20),\n",
    "    tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)]\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    \n",
    "    #one [16,32,64,128]\n",
    "    #two [16,32,64,128,256]\n",
    "    #three [32,64,128,256]\n",
    "    #four [32,64,128,256,512]\n",
    "    #five [16,32,64,128,256,512,1024]\n",
    "\n",
    "\n",
    "    model_unet_from_scratch = models.unet_plus_2d((None, None, 3), filter_num= [16,32,64,128], #make smaller64, 128, 256, 512,[16, 32, 64, 128]\n",
    "                       n_labels=num_classes, \n",
    "                       stack_num_down=2, stack_num_up=2, \n",
    "                       activation='ReLU', \n",
    "                       output_activation='Sigmoid', \n",
    "                       batch_norm=True, pool=False, unpool=False, \n",
    "                       backbone='EfficientNetB7', weights=None, \n",
    "                       freeze_backbone=False, freeze_batch_norm=False, \n",
    "                       deep_supervision = True,\n",
    "                       name='unet')\n",
    "\n",
    "    # model_unet_from_scratch = models.unet_3plus_2d((None, None, 1), n_labels=num_classes, filter_num_down=[16,32,64,128], \n",
    "    #                          filter_num_skip='auto', filter_num_aggregate='auto', \n",
    "    #                         backbone='EfficientNetB7', weights=None, \n",
    "    #                          freeze_backbone=False,\n",
    "    #                          stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='Sigmoid',\n",
    "    #                          batch_norm=True, pool='max', unpool=False, deep_supervision=True, name='unet')\n",
    "\t\n",
    "#     model.set_weights(listOfNumpyArrays)\n",
    "    model_unet_from_scratch.compile(loss='binary_crossentropy',\n",
    "                                    # loss = loss,\n",
    "                                    optimizer='adam',\n",
    "                                    metrics=[sm.metrics.Precision(threshold=0.5),\n",
    "                                      sm.metrics.Recall(threshold=0.5),\n",
    "                                      sm.metrics.FScore(threshold=0.5), \n",
    "                                      sm.metrics.IOUScore(threshold=0.5),\n",
    "                                      'accuracy'])\n",
    "\n",
    "#fit the model\n",
    "history = model_unet_from_scratch.fit(\n",
    "    train_gen,\n",
    "    epochs=1,\n",
    "    callbacks = callbacks,\n",
    "    validation_data=val_gen,\n",
    "    verbose = 0) \n",
    "\n",
    "# model_unet_from_scratch.save(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_079_128.h5\")\n",
    "model_unet_from_scratch.save(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_unet1.tf\")\n",
    "\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "#save output\n",
    "# result = pd.DataFrame({'Precision': history_dict[\"precision\"],\n",
    "#                        'Val_Precision': history_dict['val_precision'],\n",
    "#                        'Recall': history_dict[\"recall\"],\n",
    "#                        'Val_Recall': history_dict['recall'],\n",
    "#                        'F1': history_dict[\"f1-score\"],\n",
    "#                        'Val_F1': history_dict['val_f1-score'],\n",
    "#                        'IOU': history_dict[\"iou_score\"],\n",
    "#                        'Val_IOU': history_dict['val_iou_score'],\n",
    "#                        'Loss': history_dict['loss'],\n",
    "#                        'Val_Loss': history_dict['val_loss']})\n",
    "\n",
    "print(history_dict.keys())\n",
    "# result = pd.DataFrame({'Precision': history_dict[\"precision\"],\n",
    "#                        'Val_Precision': history_dict['val_precision'],\n",
    "#                        'Recall': history_dict[\"recall\"],\n",
    "#                        'Val_Recall': history_dict['recall'],\n",
    "#                        'F1': history_dict[\"f1-score\"],\n",
    "#                        'Val_F1': history_dict['val_f1-score'],\n",
    "#                        'IOU': history_dict[\"iou_score\"],\n",
    "#                        'Val_IOU': history_dict['val_iou_score'],\n",
    "#                        'Loss': history_dict['loss'],\n",
    "#                        'Val_Loss': history_dict['val_loss'],\n",
    "#                        'Accuracy': history_dict['accuracy'],\n",
    "#                        'Val_Accuracy': history_dict['val_accuracy']})\n",
    "\n",
    "\n",
    "# result.to_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/combined_good_net1.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# # Record the end time\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate the time difference in seconds\n",
    "# time_difference_seconds = end_time - start_time\n",
    "\n",
    "# # Convert seconds to hours\n",
    "# time_difference_hours = time_difference_seconds / 3600  # 1 hour = 3600 seconds\n",
    "\n",
    "# print(f\"Time taken: {time_difference_hours:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "003a5645-a97d-49c4-9f18-327fe76c7c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/spotter5/.conda/envs/deeplearning'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('CONDA_PREFIX', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73252655-c047-4b69-aa18-4d1bcde2010b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
