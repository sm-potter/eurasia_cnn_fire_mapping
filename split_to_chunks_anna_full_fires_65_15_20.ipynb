{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e64633b-961c-46f4-ade9-24384ff8bb0e",
   "metadata": {},
   "source": [
    "Read in packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00387a58-86f6-4282-bf6b-23d2bef9e01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import glob\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from sklearn.utils import shuffle\n",
    "from MightyMosaic import MightyMosaic\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638de7f-1d23-46d7-8ec0-f08acc351a63",
   "metadata": {},
   "source": [
    "First split the uniqe ids into three sets, based only on fire ids,  I will use these new files to train new models for the anna_good and anna_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3c5e19-48eb-456d-b852-8e35935ee1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5031, 4) (1161, 4) (1548, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1855320/3950127243.py:26: UserWarning: Geometry column does not contain geometry.\n",
      "  in_shape['geometry'] = in_shape['geometry'].apply(lambda geom: geom.wkt)\n"
     ]
    }
   ],
   "source": [
    "#path to the good ids\n",
    "# good_ids = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "\n",
    "# good_ids[0]\n",
    "\n",
    "# df = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_poly_check_ee.csv')\n",
    "\n",
    "#path to the original tif files\n",
    "# tif_path =  '/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85'\n",
    "\n",
    "#make sure the original tif files are in the good files\n",
    "\n",
    "\n",
    "#first read in the files which are yes, no or maybe from anna\n",
    "df = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_poly_check_ee.csv')\n",
    "\n",
    "df = df.rename(columns = {'Image': 'ID'})\n",
    "\n",
    "df = df[df['ID'] .isin (['29690000000000-0000000000', '29690000000000-0000023296']) == False]\n",
    "\n",
    "df['ID'] = df['ID'].astype(int)\n",
    "\n",
    "# #shapefile which has annas original ids\n",
    "in_shape = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_polygons.shp')\n",
    "\n",
    "in_shape['geometry'] = in_shape['geometry'].apply(lambda geom: geom.wkt)\n",
    "\n",
    "# Now, convert the GeoDataFrame to a Pandas DataFrame\n",
    "in_shape = pd.DataFrame(in_shape)\n",
    "in_shape = in_shape[['Year', 'ID', 'area']]\n",
    "\n",
    "#left join so I can have year and all info togeter\n",
    "in_shape = pd.merge(in_shape, df, on = 'ID', how = 'inner')\n",
    "\n",
    "in_shape['Year'] = in_shape['Year'].astype(int)\n",
    "\n",
    "# in_shape = in_shape[(in_shape['Year'] == 2019) & (in_shape['Keep'] == 'Yes')]\n",
    "#filter only keep files\n",
    "df = in_shape[in_shape['Keep'] == 'Yes']\n",
    "\n",
    "# in_shape.head()\n",
    "#for all these ids split into 80% training, 10% validation, 10% testing. \n",
    "# Split the dataset into training (80%) and temporary testing (20%)\n",
    "train_df, temp_test_df = train_test_split(df, test_size=0.35, random_state=42)\n",
    "\n",
    "# Split the temporary testing dataset into validation (50%) and testing (50%) \n",
    "# which corresponds to 10% of the original dataset each\n",
    "val_df, test_df = train_test_split(temp_test_df, test_size=0.5714, random_state=42)\n",
    "\n",
    "# Save the datasets to CSV files\n",
    "# train_df.to_csv('train_data.csv', index=False)\n",
    "# val_df.to_csv('validation_data.csv', index=False)\n",
    "# test_df.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "train_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_good_training_files_full_fire_65_15_20.csv', index = False)\n",
    "val_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_good_validation_files_full_fire_65_15_20.csv', index = False)\n",
    "test_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files_full_fire_65_15_20.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbd161-d6cf-443d-a463-0548927ba170",
   "metadata": {},
   "source": [
    "Now I need to make new files which will have the specific chunks in them to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe82edd-4862-45c7-97de-574b934e8805",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m training_names2:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in training_names2:\n",
    "    \n",
    "    if os.path.exists(i):\n",
    "        continue\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d8bab-1362-456c-97d0-0b64647c0e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
