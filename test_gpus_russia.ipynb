{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2580dca-0875-4796-9a3a-e85d27e36d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       6      7      8\n",
      "0   21.0  -68.0   -9.0\n",
      "1  974.0  522.0  686.0\n",
      "Tue Jan 16 16:16:57 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              42W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  | 00000000:62:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              40W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              42W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              40W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Number of devices: 4\n",
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\tunet_output_sup0_activation\n",
      "\tunet_output_sup1_activation\n",
      "\tunet_output_final_activation\n"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Read in packages\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "import os\n",
    "import segmentation_models as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import concatenate, Conv2DTranspose, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Input, AvgPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_unet_collection import models\n",
    "import tensorflow_addons as tfa\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# gpu_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "# for device in gpu_devices:\n",
    "#     tensorflow.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gpu_info = get_ipython().getoutput('nvidia-smi')\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#     print('Not connected to a GPU')\n",
    "# else:\n",
    "#     print(gpu_info)\n",
    "\n",
    "\n",
    "min_max = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "\n",
    "min_max = min_max[['6', '7', '8']]\n",
    "\n",
    "print(min_max)\n",
    "#functin to standardize all bands at once\n",
    "\n",
    "\n",
    "#function to standardize\n",
    "def normalize_meanstd(a, axis=None): \n",
    "    # axis param denotes axes along which mean & std reductions are to be performed\n",
    "    mean = np.mean(a, axis=axis, keepdims=True)\n",
    "    std = np.sqrt(((a - mean)**2).mean(axis=axis, keepdims=True))\n",
    "    return (a - mean) / std\n",
    "\n",
    "#function to normalize\n",
    "def normalize(a, axis=None): \n",
    "    # axis param denotes axes along which mean & std reductions are to be performed\n",
    "    minv = np.min(a, axis=axis, keepdims=True)\n",
    "    maxv = np.max(a, axis=axis, keepdims=True)\n",
    "    return (a - minv) / (maxv - minv)\n",
    "\n",
    "\n",
    "#function to get files from storage bucket\n",
    "def get_files(bucket_path):\n",
    "\n",
    "\t\"\"\"argument is the path to where the numpy\n",
    "\tsave files are located, return a list of filenames\n",
    "\t\"\"\"\n",
    "\tall = []\n",
    "\n",
    "\t#list of files\n",
    "\tfiles = os.listdir(bucket_path)\n",
    "\n",
    "\t#get list of filenames we will use, notte I remove images that don't have a target due to clouds\n",
    "\tfile_names = []\n",
    "\tfor f in files:\n",
    "\n",
    "\t\tif f.endswith('.npy'):\n",
    "\n",
    "\n",
    "\t\t\tall.append(os.path.join(bucket_path, f))\n",
    "\treturn(all)\n",
    "\n",
    "\n",
    "#get all the pathways\n",
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_training_files.csv')['Files'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_validation_files.csv')['Files'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "# training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_training_files.csv')['Files'].tolist()\n",
    "# validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_validation_files.csv')['Files'].tolist()\n",
    "# testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/mtbs_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "# good_ids= pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/raw_files/ak_ca_1985_clip.csv')\n",
    "\n",
    "\n",
    "# def get_good(in_list, good_frame):\n",
    "    \n",
    "#     final = []\n",
    "#     for i in in_list:\n",
    "\n",
    "#         try:\n",
    "\n",
    "#             in_id = int(i.split('/')[-1].split('_')[2].replace('.npy', ''))\n",
    "\n",
    "#             if in_id in good_frame['ID']:\n",
    "\n",
    "#                 final.append(i)\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     return final\n",
    "\n",
    "# training_names = get_good(training_names, good_ids)\n",
    "# validation_names = get_good(validation_names, good_ids)\n",
    "# testing_names = get_good(testing_names, good_ids)\n",
    "\n",
    "\n",
    "# training_names = training_names + training_names2\n",
    "# validation_names = validation_names + validation_names2\n",
    "# testing_names = testing_names + testing_names2\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#function to normalize within range\n",
    "def normalize(start, end, arr):\n",
    "    width = end - start\n",
    "    res = (arr - np.nanmin(arr))/(np.nanmax(arr)- np.nanmin(arr)) * width + start\n",
    "\n",
    "#     res = (arr - arr.min())/(arr.max() - arr.min()) * width + start\n",
    "    return res\n",
    "\n",
    "class img_gen(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    #will need pre defined variables batch_size, img_size, input_img_paths and target_img_paths\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "\t    self.batch_size = batch_size\n",
    "\t    self.img_size = img_size\n",
    "\t    self.input_img_paths = input_img_paths\n",
    "\t    self.target_img_paths = input_img_paths\n",
    "\n",
    "    #number of batches the generator is supposed to produceis the length of the paths divided by the batch siize\n",
    "    def __len__(self):\n",
    "\t    return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (x)\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (y)\n",
    "\t\t\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\") #create matrix of zeros which will have the dimension height, wideth, n_bands), 8 is the n_bands\n",
    "        \n",
    "  \n",
    "         #start populating x by enumerating over the input img paths\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, :-1]\n",
    "\n",
    "            # img = img * 1000\n",
    "            img = img.astype(float)\n",
    "            img = np.round(img, 3)\n",
    "            img[img == 0] = -999\n",
    "\n",
    "            img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "            img[img == -999] = np.nan\n",
    "\n",
    "            in_shape = img.shape\n",
    "            \n",
    "            #turn to dataframe to normalize\n",
    "            img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "\t\t\t\n",
    "            img = pd.DataFrame(img)\n",
    "\t\t\t\n",
    "            img.columns = min_max.columns\n",
    "\t\t\t\n",
    "            img = pd.concat([min_max, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "            #normalize 0 to 1\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\t\t\t\n",
    "            img = img.iloc[2:]\n",
    "#\n",
    "#             img = img.values.reshape(in_shape)\n",
    "            img = img.values.reshape(in_shape)\n",
    "\n",
    "#             replace nan with -1\n",
    "            img[np.isnan(img)] = -1\n",
    "\n",
    "#apply standardization\n",
    "# img = normalize(img, axis=(0,1))\n",
    "\n",
    "            img = np.round(img, 3)\n",
    "            #populate x\n",
    "            x[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "\n",
    "        #do tthe same thing for y\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "            img = img.astype(int)\n",
    "\n",
    "            img[img < 0] = 0\n",
    "            img[img >1] = 0\n",
    "            img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "            img[np.isnan(img)] = 0\n",
    "            img = img.astype(int)\n",
    "\n",
    "            # img =  tf.keras.utils.to_categorical(img, num_classes = 2)\n",
    "            # y[j] = np.expand_dims(img, 2) \n",
    "            y[j] = img\n",
    "  \n",
    "       \n",
    "    #Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "    # y[j] -= 1\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Read in the images based on the generator\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "#Initialize GPUS with tensorflow\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "gpu_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tensorflow.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gpu_info = get_ipython().getoutput('nvidia-smi')\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#     print('Not connected to a GPU')\n",
    "# else:\n",
    "#     print(gpu_info)\n",
    "    \n",
    "# # watch -n0.5 nvidia-smi\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# devices = device_lib.list_local_devices()\n",
    "\n",
    "\n",
    "#batch size and img size\n",
    "#15 before\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "# img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "#get images\n",
    "train_gen = img_gen(batch_size, img_size, training_names)\n",
    "val_gen = img_gen(batch_size, img_size, validation_names)\n",
    "test_gen = img_gen(batch_size, img_size, testing_names)\n",
    "#\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "tensorflow.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam() #this is 1e-3, default or 'rmsprop'\n",
    "LR = 0.0005\n",
    "    \n",
    "loss= tensorflow.keras.losses.BinaryFocalCrossentropy(\n",
    "    from_logits=False,\n",
    "    gamma = 2.0,\n",
    "    alpha = 0.25)\n",
    "\n",
    "# f.keras.losses.BinaryFocalCrossentropy(gamma=2.0, alpha=0.25)\n",
    "# loss = tensorflow.keras.losses.BinaryFocalCrossentropy(\n",
    "#     apply_class_balancing=False,\n",
    "#     alpha=0.25,\n",
    "#     gamma=2.0,\n",
    "#     from_logits=False,\n",
    "#     label_smoothing=0.0,\n",
    "#     axis=-1,\n",
    "#     reduction=losses_utils.ReductionV2.AUTO,\n",
    "#     name='binary_focal_crossentropy'\n",
    "# )\n",
    "\n",
    "\n",
    "callbacks = [tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia.tf\",\n",
    "#     verbose=1,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    monitor='val_mean_iou',\n",
    "    mode = 'max'),\n",
    "    tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience=10)]\n",
    "    \n",
    "tensorflow.keras.callbacks.ReduceLROnPlateau(monitor = 'loss', mode = 'min', patience = 10, min_delta=0.001, min_LR = LR/25, verbose = 1)\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    \n",
    "    #one [16,32,64,128]\n",
    "    #two [16,32,64,128,256]\n",
    "    #three [32,64,128,256]\n",
    "    #four [32,64,128,256,512]\n",
    "    #five [16,32,64,128,256,512,1024]\n",
    "\n",
    "\n",
    "    model_unet_from_scratch = models.unet_plus_2d((128, 128, 3), filter_num= [16,32,64,128], #make smaller64, 128, 256, 512,[16, 32, 64, 128]\n",
    "                       n_labels=num_classes, \n",
    "                       stack_num_down=2, stack_num_up=2, \n",
    "                       activation='ReLU', \n",
    "                       output_activation='Sigmoid', \n",
    "                       batch_norm=True, pool=False, unpool=False, \n",
    "                       backbone='EfficientNetB7', weights=None, \n",
    "                       freeze_backbone=False, freeze_batch_norm=False, \n",
    "                       deep_supervision = True,\n",
    "                       name='unet')\n",
    "\n",
    "    # model_unet_from_scratch = models.unet_3plus_2d((None, None, 1), n_labels=num_classes, filter_num_down=[16,32,64,128], \n",
    "    #                          filter_num_skip='auto', filter_num_aggregate='auto', \n",
    "    #                         backbone='EfficientNetB7', weights=None, \n",
    "    #                          freeze_backbone=False,\n",
    "    #                          stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='Sigmoid',\n",
    "    #                          batch_norm=True, pool='max', unpool=False, deep_supervision=True, name='unet')\n",
    "\t\n",
    "#     model.set_weights(listOfNumpyArrays)\n",
    "    model_unet_from_scratch.compile(loss='binary_crossentropy',\n",
    "                                    # loss = loss,\n",
    "                                    optimizer='adam',\n",
    "                                    metrics=[sm.metrics.Precision(threshold=0.5),\n",
    "                                      sm.metrics.Recall(threshold=0.5),\n",
    "                                      sm.metrics.FScore(threshold=0.5), \n",
    "                                      sm.metrics.IOUScore(threshold=0.5),\n",
    "                                      'accuracy'])\n",
    "\n",
    "#fit the model\n",
    "history = model_unet_from_scratch.fit(\n",
    "    train_gen,\n",
    "    epochs=35,\n",
    "    callbacks = callbacks,\n",
    "    validation_data=val_gen,\n",
    "    verbose = 0) \n",
    "\n",
    "# model_unet_from_scratch.save(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_079_128.h5\")\n",
    "model_unet_from_scratch.save(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia.tf\")\n",
    "\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "#save output\n",
    "# result = pd.DataFrame({'Precision': history_dict[\"precision\"],\n",
    "#                        'Val_Precision': history_dict['val_precision'],\n",
    "#                        'Recall': history_dict[\"recall\"],\n",
    "#                        'Val_Recall': history_dict['recall'],\n",
    "#                        'F1': history_dict[\"f1-score\"],\n",
    "#                        'Val_F1': history_dict['val_f1-score'],\n",
    "#                        'IOU': history_dict[\"iou_score\"],\n",
    "#                        'Val_IOU': history_dict['val_iou_score'],\n",
    "#                        'Loss': history_dict['loss'],\n",
    "#                        'Val_Loss': history_dict['val_loss']})\n",
    "\n",
    "result = pd.DataFrame({'Precision': history_dict[\"unet_output_final_activation_precision\"],\n",
    "                       'Val_Precision': history_dict['val_unet_output_final_activation_precision'],\n",
    "                       'Recall': history_dict[\"unet_output_final_activation_recall\"],\n",
    "                       'Val_Recall': history_dict['val_unet_output_final_activation_recall'],\n",
    "                       'F1': history_dict[\"unet_output_final_activation_f1-score\"],\n",
    "                       'Val_F1': history_dict['val_unet_output_final_activation_f1-score'],\n",
    "                       'IOU': history_dict[\"unet_output_final_activation_iou_score\"],\n",
    "                       'Val_IOU': history_dict['val_unet_output_final_activation_iou_score'],\n",
    "                       'Loss': history_dict['unet_output_final_activation_loss'],\n",
    "                       'Val_Loss': history_dict['val_unet_output_final_activation_loss'],\n",
    "                      'Accuracy': history_dict['unet_output_final_activation_accuracy'],\n",
    "                       'Val_Accuracy': history_dict['val_unet_output_final_activation_accuracy']})\n",
    "\n",
    "\n",
    "result.to_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/russia.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time difference in seconds\n",
    "time_difference_seconds = end_time - start_time\n",
    "\n",
    "# Convert seconds to hours\n",
    "time_difference_hours = time_difference_seconds / 3600  # 1 hour = 3600 seconds\n",
    "\n",
    "print(f\"Time taken: {time_difference_hours:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfb983-ebc1-4ce9-86e7-4d85b8f8f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d99ad-4edf-45c9-923a-6d2778d79dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
