{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e64633b-961c-46f4-ade9-24384ff8bb0e",
   "metadata": {},
   "source": [
    "Read in packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00387a58-86f6-4282-bf6b-23d2bef9e01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import glob\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from sklearn.utils import shuffle\n",
    "from MightyMosaic import MightyMosaic\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638de7f-1d23-46d7-8ec0-f08acc351a63",
   "metadata": {},
   "source": [
    "First split the uniqe ids into three sets, based only on fire ids,  I will use these new files to train new models for the anna_good and anna_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a3c5e19-48eb-456d-b852-8e35935ee1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6192, 4) (774, 4) (774, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1012788/818267607.py:26: UserWarning: Geometry column does not contain geometry.\n",
      "  in_shape['geometry'] = in_shape['geometry'].apply(lambda geom: geom.wkt)\n"
     ]
    }
   ],
   "source": [
    "#path to the good ids\n",
    "# good_ids = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "\n",
    "# good_ids[0]\n",
    "\n",
    "# df = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_poly_check_ee.csv')\n",
    "\n",
    "#path to the original tif files\n",
    "# tif_path =  '/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85'\n",
    "\n",
    "#make sure the original tif files are in the good files\n",
    "\n",
    "\n",
    "#first read in the files which are yes, no or maybe from anna\n",
    "df = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_poly_check_ee.csv')\n",
    "\n",
    "df = df.rename(columns = {'Image': 'ID'})\n",
    "\n",
    "df = df[df['ID'] .isin (['29690000000000-0000000000', '29690000000000-0000023296']) == False]\n",
    "\n",
    "df['ID'] = df['ID'].astype(int)\n",
    "\n",
    "# #shapefile which has annas original ids\n",
    "in_shape = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_polygons.shp')\n",
    "\n",
    "in_shape['geometry'] = in_shape['geometry'].apply(lambda geom: geom.wkt)\n",
    "\n",
    "# Now, convert the GeoDataFrame to a Pandas DataFrame\n",
    "in_shape = pd.DataFrame(in_shape)\n",
    "in_shape = in_shape[['Year', 'ID', 'area']]\n",
    "\n",
    "#left join so I can have year and all info togeter\n",
    "in_shape = pd.merge(in_shape, df, on = 'ID', how = 'inner')\n",
    "\n",
    "in_shape['Year'] = in_shape['Year'].astype(int)\n",
    "\n",
    "# in_shape = in_shape[(in_shape['Year'] == 2019) & (in_shape['Keep'] == 'Yes')]\n",
    "#filter only keep files\n",
    "df = in_shape[in_shape['Keep'] == 'Yes']\n",
    "\n",
    "# in_shape.head()\n",
    "#for all these ids split into 80% training, 10% validation, 10% testing. \n",
    "# Split the dataset into training (80%) and temporary testing (20%)\n",
    "train_df, temp_test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary testing dataset into validation (50%) and testing (50%) \n",
    "# which corresponds to 10% of the original dataset each\n",
    "val_df, test_df = train_test_split(temp_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the datasets to CSV files\n",
    "# train_df.to_csv('train_data.csv', index=False)\n",
    "# val_df.to_csv('validation_data.csv', index=False)\n",
    "# test_df.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "train_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_good_training_files_full_fire.csv', index = False)\n",
    "val_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_good_validation_files_full_fire.csv', index = False)\n",
    "test_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files_full_fire.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbd161-d6cf-443d-a463-0548927ba170",
   "metadata": {},
   "source": [
    "Now I need to make new files which will have the specific chunks in them to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7d6993-6b63-42b0-8c95-d8e257664c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162808 26919 25027\n"
     ]
    }
   ],
   "source": [
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_good_training_files_full_fire.csv')['ID'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ann_good_validation_files_full_fire.csv')['ID'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files_full_fire.csv')['ID'].tolist()\n",
    "\n",
    "# print(training_names[:10])\n",
    "\n",
    "# #path to the chunked files\n",
    "# chunked =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_old_subs_0_128') #old is old dnbr method in NA\n",
    "chunked =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_old_subs_0_128')\n",
    "\n",
    "chunked[:10]\n",
    "\n",
    "chunked[0].split('_')[-1].split('.')[0]\n",
    "\n",
    "def filter_chunked(in_names, chunked):\n",
    "    \"\"\"\n",
    "    Filters items in the 'chunked' list based on whether the specified part of\n",
    "    each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "    Parameters:\n",
    "    - training_names: List of integers to filter against.\n",
    "    - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "    Returns:\n",
    "    - List of strings from 'chunked' that match the filtering criteria.\n",
    "    \"\"\"\n",
    "    # Filter the 'chunked' list\n",
    "    filtered_chunked = [\n",
    "        name for name in chunked \n",
    "        if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "    ]\n",
    "    \n",
    "    filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_old_subs_0_128/' + i for i in filtered_chunked]\n",
    "    return filtered_chunked\n",
    "\n",
    "training_names2 = filter_chunked(training_names, chunked)\n",
    "validation_names2 = filter_chunked(validation_names, chunked)\n",
    "testing_names2 = filter_chunked(testing_names, chunked)\n",
    "\n",
    "print(len(training_names2), len(validation_names2), len(testing_names2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec2b8f-62ff-4993-9c7f-3069f8ca571e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
