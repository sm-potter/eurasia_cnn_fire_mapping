{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b58db-ac9f-4f65-b355-120c540997c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1727713/823733968.py:165: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  l5['band.or.si']=pd.Categorical(l5['band.or.si'],categories=bands)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ndsi_intervals_t2/169_199_52\n",
      "Downloading ndsi_intervals_t2/199_229_52\n",
      "Downloading ndsi_intervals_t2/final_52\n",
      "Downloading ndsi_intervals_t2/172_202_168\n",
      "Downloading ndsi_intervals_t2/202_232_168\n",
      "Downloading ndsi_intervals_t2/final_168\n",
      "Downloading ndsi_intervals_t2/179_209_995\n",
      "Downloading ndsi_intervals_t2/209_239_995\n",
      "Downloading ndsi_intervals_t2/final_995\n",
      "Downloading ndsi_intervals_t2/179_209_1016\n",
      "Downloading ndsi_intervals_t2/209_239_1016\n",
      "Downloading ndsi_intervals_t2/final_1016\n",
      "Downloading ndsi_intervals_t2/170_200_1023\n",
      "Downloading ndsi_intervals_t2/200_230_1023\n",
      "Downloading ndsi_intervals_t2/230_240_1023\n",
      "Downloading ndsi_intervals_t2/final_1023\n",
      "Downloading ndsi_intervals_t2/173_203_1498\n",
      "Downloading ndsi_intervals_t2/203_233_1498\n",
      "Downloading ndsi_intervals_t2/233_241_1498\n",
      "Downloading ndsi_intervals_t2/final_1498\n",
      "Downloading ndsi_intervals_t2/153_183_1559\n",
      "Downloading ndsi_intervals_t2/183_213_1559\n",
      "Downloading ndsi_intervals_t2/213_239_1559\n",
      "Downloading ndsi_intervals_t2/final_1559\n",
      "Downloading ndsi_intervals_t2/157_187_1563\n",
      "Downloading ndsi_intervals_t2/187_217_1563\n",
      "Downloading ndsi_intervals_t2/217_247_1563\n",
      "Downloading ndsi_intervals_t2/247_260_1563\n",
      "Downloading ndsi_intervals_t2/final_1563\n",
      "Downloading ndsi_intervals_t2/135_165_1679\n",
      "Downloading ndsi_intervals_t2/165_195_1679\n",
      "Downloading ndsi_intervals_t2/195_225_1679\n",
      "Downloading ndsi_intervals_t2/225_255_1679\n",
      "Downloading ndsi_intervals_t2/255_263_1679\n",
      "Downloading ndsi_intervals_t2/final_1679\n",
      "Downloading ndsi_intervals_t2/171_201_1754\n",
      "Downloading ndsi_intervals_t2/201_231_1754\n",
      "Downloading ndsi_intervals_t2/final_1754\n",
      "Downloading ndsi_intervals_t2/186_216_1778\n",
      "Downloading ndsi_intervals_t2/216_238_1778\n",
      "Downloading ndsi_intervals_t2/final_1778\n",
      "Downloading ndsi_intervals_t2/180_210_2460\n",
      "Downloading ndsi_intervals_t2/210_240_2460\n",
      "Downloading ndsi_intervals_t2/final_2460\n",
      "Downloading ndsi_intervals_t2/181_211_2464\n",
      "Downloading ndsi_intervals_t2/211_241_2464\n",
      "Downloading ndsi_intervals_t2/final_2464\n",
      "Downloading ndsi_intervals_t2/206_236_2488\n",
      "Downloading ndsi_intervals_t2/final_2488\n",
      "Downloading ndsi_intervals_t2/187_217_3026\n",
      "Downloading ndsi_intervals_t2/217_236_3026\n",
      "Downloading ndsi_intervals_t2/final_3026\n",
      "Downloading ndsi_intervals_t2/168_198_3138\n",
      "Downloading ndsi_intervals_t2/198_228_3138\n",
      "Downloading ndsi_intervals_t2/228_255_3138\n",
      "Downloading ndsi_intervals_t2/final_3138\n",
      "Downloading ndsi_intervals_t2/187_217_3249\n",
      "Downloading ndsi_intervals_t2/217_242_3249\n",
      "Downloading ndsi_intervals_t2/final_3249\n",
      "Downloading ndsi_intervals_t2/187_217_3266\n",
      "Downloading ndsi_intervals_t2/217_236_3266\n",
      "Downloading ndsi_intervals_t2/final_3266\n",
      "Downloading ndsi_intervals_t2/162_192_3336\n",
      "Downloading ndsi_intervals_t2/192_222_3336\n",
      "Downloading ndsi_intervals_t2/222_252_3336\n",
      "Downloading ndsi_intervals_t2/final_3336\n",
      "Downloading ndsi_intervals_t2/164_194_3381\n",
      "Downloading ndsi_intervals_t2/194_224_3381\n",
      "Downloading ndsi_intervals_t2/224_254_3381\n",
      "Downloading ndsi_intervals_t2/final_3381\n",
      "Downloading ndsi_intervals_t2/146_176_3645\n",
      "Downloading ndsi_intervals_t2/176_206_3645\n",
      "Downloading ndsi_intervals_t2/206_236_3645\n",
      "Downloading ndsi_intervals_t2/236_260_3645\n",
      "Downloading ndsi_intervals_t2/final_3645\n",
      "Downloading ndsi_intervals_t2/160_190_4518\n",
      "Downloading ndsi_intervals_t2/190_220_4518\n",
      "Downloading ndsi_intervals_t2/220_250_4518\n",
      "Downloading ndsi_intervals_t2/250_280_4518\n",
      "Downloading ndsi_intervals_t2/280_288_4518\n",
      "Downloading ndsi_intervals_t2/final_4518\n",
      "Downloading ndsi_intervals_t2/138_168_5311\n",
      "Downloading ndsi_intervals_t2/168_198_5311\n",
      "Downloading ndsi_intervals_t2/198_228_5311\n",
      "Downloading ndsi_intervals_t2/228_258_5311\n",
      "Downloading ndsi_intervals_t2/258_275_5311\n",
      "Downloading ndsi_intervals_t2/final_5311\n",
      "Downloading ndsi_intervals_t2/145_175_5658\n",
      "Downloading ndsi_intervals_t2/175_205_5658\n",
      "Downloading ndsi_intervals_t2/205_235_5658\n",
      "Downloading ndsi_intervals_t2/235_258_5658\n",
      "Downloading ndsi_intervals_t2/final_5658\n",
      "Downloading ndsi_intervals_t2/144_174_7332\n",
      "Downloading ndsi_intervals_t2/174_204_7332\n",
      "Downloading ndsi_intervals_t2/204_234_7332\n",
      "Downloading ndsi_intervals_t2/234_239_7332\n",
      "Downloading ndsi_intervals_t2/final_7332\n",
      "Downloading ndsi_intervals_t2/153_183_7370\n",
      "Downloading ndsi_intervals_t2/183_213_7370\n",
      "Downloading ndsi_intervals_t2/213_236_7370\n",
      "Downloading ndsi_intervals_t2/final_7370\n",
      "Downloading ndsi_intervals_t2/151_181_8052\n",
      "Downloading ndsi_intervals_t2/181_211_8052\n",
      "Downloading ndsi_intervals_t2/211_237_8052\n",
      "Downloading ndsi_intervals_t2/final_8052\n",
      "Downloading ndsi_intervals_t2/188_218_8256\n",
      "Downloading ndsi_intervals_t2/218_236_8256\n",
      "Downloading ndsi_intervals_t2/final_8256\n",
      "Downloading ndsi_intervals_t2/148_178_8271\n",
      "Downloading ndsi_intervals_t2/178_208_8271\n",
      "Downloading ndsi_intervals_t2/208_238_8271\n",
      "Downloading ndsi_intervals_t2/238_266_8271\n",
      "Downloading ndsi_intervals_t2/final_8271\n",
      "Downloading ndsi_intervals_t2/182_212_8728\n",
      "Downloading ndsi_intervals_t2/212_240_8728\n",
      "Downloading ndsi_intervals_t2/final_8728\n",
      "Downloading ndsi_intervals_t2/187_217_8737\n",
      "Downloading ndsi_intervals_t2/217_236_8737\n",
      "Downloading ndsi_intervals_t2/final_8737\n",
      "Downloading ndsi_intervals_t2/182_212_8753\n",
      "Downloading ndsi_intervals_t2/212_242_8753\n",
      "Downloading ndsi_intervals_t2/final_8753\n",
      "Downloading ndsi_intervals_t2/177_207_8803\n",
      "Downloading ndsi_intervals_t2/207_236_8803\n",
      "Downloading ndsi_intervals_t2/final_8803\n",
      "Downloading ndsi_intervals_t2/176_206_8857\n",
      "Downloading ndsi_intervals_t2/206_236_8857\n",
      "Downloading ndsi_intervals_t2/236_240_8857\n",
      "Downloading ndsi_intervals_t2/final_8857\n",
      "Downloading ndsi_intervals_t2/155_185_9444\n",
      "Downloading ndsi_intervals_t2/185_215_9444\n",
      "Downloading ndsi_intervals_t2/215_241_9444\n",
      "Downloading ndsi_intervals_t2/final_9444\n",
      "Downloading ndsi_intervals_t2/180_210_9710\n",
      "Downloading ndsi_intervals_t2/210_235_9710\n",
      "Downloading ndsi_intervals_t2/final_9710\n",
      "Downloading ndsi_intervals_t2/155_185_9763\n",
      "Downloading ndsi_intervals_t2/185_215_9763\n",
      "Downloading ndsi_intervals_t2/215_245_9763\n",
      "Downloading ndsi_intervals_t2/245_261_9763\n",
      "Downloading ndsi_intervals_t2/final_9763\n",
      "Downloading ndsi_intervals_t2/162_192_9813\n",
      "Downloading ndsi_intervals_t2/192_222_9813\n",
      "Downloading ndsi_intervals_t2/222_248_9813\n",
      "Downloading ndsi_intervals_t2/final_9813\n",
      "Downloading ndsi_intervals_t2/158_188_9827\n",
      "Downloading ndsi_intervals_t2/188_218_9827\n",
      "Downloading ndsi_intervals_t2/218_248_9827\n",
      "Downloading ndsi_intervals_t2/248_260_9827\n",
      "Downloading ndsi_intervals_t2/final_9827\n",
      "Downloading ndsi_intervals_t2/153_183_9835\n",
      "Downloading ndsi_intervals_t2/183_213_9835\n",
      "Downloading ndsi_intervals_t2/213_243_9835\n",
      "Downloading ndsi_intervals_t2/final_9835\n",
      "Downloading ndsi_intervals_t2/127_157_11276\n",
      "Downloading ndsi_intervals_t2/157_187_11276\n",
      "Downloading ndsi_intervals_t2/187_217_11276\n",
      "Downloading ndsi_intervals_t2/217_247_11276\n",
      "Downloading ndsi_intervals_t2/final_11276\n",
      "Downloading ndsi_intervals_t2/184_214_11339\n",
      "Downloading ndsi_intervals_t2/214_235_11339\n",
      "Downloading ndsi_intervals_t2/final_11339\n",
      "Downloading ndsi_intervals_t2/186_216_11448\n",
      "Downloading ndsi_intervals_t2/216_235_11448\n",
      "Downloading ndsi_intervals_t2/final_11448\n",
      "Downloading ndsi_intervals_t2/182_212_12405\n",
      "Downloading ndsi_intervals_t2/212_242_12405\n",
      "Downloading ndsi_intervals_t2/242_253_12405\n",
      "Downloading ndsi_intervals_t2/final_12405\n",
      "Downloading ndsi_intervals_t2/185_215_12421\n",
      "Downloading ndsi_intervals_t2/215_235_12421\n",
      "Downloading ndsi_intervals_t2/final_12421\n",
      "Downloading ndsi_intervals_t2/175_205_12436\n",
      "Downloading ndsi_intervals_t2/205_235_12436\n",
      "Downloading ndsi_intervals_t2/final_12436\n",
      "Downloading ndsi_intervals_t2/188_218_12494\n",
      "Downloading ndsi_intervals_t2/218_235_12494\n",
      "Downloading ndsi_intervals_t2/final_12494\n",
      "Downloading ndsi_intervals_t2/186_216_12594\n",
      "Downloading ndsi_intervals_t2/216_236_12594\n",
      "Downloading ndsi_intervals_t2/final_12594\n",
      "Downloading ndsi_intervals_t2/171_201_12734\n",
      "Downloading ndsi_intervals_t2/201_231_12734\n",
      "Downloading ndsi_intervals_t2/231_242_12734\n",
      "Downloading ndsi_intervals_t2/final_12734\n",
      "Downloading ndsi_intervals_t2/142_172_13459\n",
      "Downloading ndsi_intervals_t2/172_202_13459\n",
      "Downloading ndsi_intervals_t2/202_232_13459\n",
      "Downloading ndsi_intervals_t2/232_253_13459\n",
      "Downloading ndsi_intervals_t2/final_13459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 885\u001b[0m\n\u001b[1;32m    877\u001b[0m max_snow_day_fall \u001b[38;5;241m=\u001b[39m ee\u001b[38;5;241m.\u001b[39mImageCollection([start_year_first_day_no_snow_fall, end_year_first_day_no_snow_fall])\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m#Use reduceRegion to find the maximum value within the region\u001b[39;00m\n\u001b[1;32m    880\u001b[0m max_value \u001b[38;5;241m=\u001b[39m \u001b[43mmax_snow_day\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduceRegion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreducer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mee\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_buffer2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxPixels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e13\u001b[39;49m\n\u001b[0;32m--> 885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalDoy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    887\u001b[0m start_day \u001b[38;5;241m=\u001b[39m max_value \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m#Use reduceRegion to find the maximum value within the region\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/ee/computedobject.py:107\u001b[0m, in \u001b[0;36mComputedObject.getInfo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetInfo\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Any]:\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Fetch and return information about this object.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    The object can evaluate to anything.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/ee/data.py:1108\u001b[0m, in \u001b[0;36mcomputeValue\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1105\u001b[0m body \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpression\u001b[39m\u001b[38;5;124m'\u001b[39m: serializer\u001b[38;5;241m.\u001b[39mencode(obj, for_cloud_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)}\n\u001b[1;32m   1106\u001b[0m _maybe_populate_workload_tag(body)\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_cloud_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_get_cloud_projects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_projects_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprettyPrint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/ee/data.py:402\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes a Cloud API call and translates errors to EEExceptions.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m  EEException if the call fails.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m googleapiclient\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mHttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _translate_cloud_exception(e)\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/googleapiclient/http.py:923\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody))\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# Handle retries for server-side errors.\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m resp, content \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_callbacks:\n\u001b[1;32m    936\u001b[0m     callback(resp)\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/googleapiclient/http.py:191\u001b[0m, in \u001b[0;36m_retry_request\u001b[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     resp, content \u001b[38;5;241m=\u001b[39m \u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Retry on SSL errors and socket timeout errors.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _ssl_SSLError \u001b[38;5;28;01mas\u001b[39;00m ssl_error:\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/google_auth_httplib2.py:218\u001b[0m, in \u001b[0;36mAuthorizedHttp.request\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     body_stream_position \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Make the request.\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m response, content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mredirections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mredirections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnection_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    234\u001b[0m     response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refresh_status_codes\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m _credential_refresh_attempt \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_refresh_attempts\n\u001b[1;32m    236\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/ee/_cloud_api_utils.py:70\u001b[0m, in \u001b[0;36m_Http.request\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m redirections  \u001b[38;5;66;03m# Ignored\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;66;03m# googleapiclient is expecting an httplib2 object, and doesn't include\u001b[39;00m\n\u001b[1;32m     67\u001b[0m   \u001b[38;5;66;03m# requests error in the list of transient errors. Therefore, transient\u001b[39;00m\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;66;03m# requests errors should be converted to kinds that googleapiclient\u001b[39;00m\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;66;03m# consider transient.\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError \u001b[38;5;28;01mas\u001b[39;00m connection_error:\n\u001b[1;32m     74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(connection_error) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconnection_error\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.conda/envs/gee_ml/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import ee\n",
    "import numpy as np\n",
    "from geeml.extract import extractor\n",
    "import pandas as pd\n",
    "import random\n",
    "# import geemap\n",
    "# Authenticate GEE\n",
    "# ee.Authenticate()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\"\n",
    "\n",
    "service_account = 'gee-serdp-upload@appspot.gserviceaccount.com'\n",
    "credentials = ee.ServiceAccountCredentials(service_account, \"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\")\n",
    "ee.Initialize(credentials)\n",
    "# Initialize GEE with high-volume end-point\n",
    "# ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "ee.Initialize()\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import geemap\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import client\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"gee-serdp-upload\"\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\"\n",
    "storage_client = storage.Client.from_service_account_json(\"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\")\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"gee-serdp-upload\"\n",
    "storage_client = storage.Client()\n",
    "# bucket_name = 'smp-scratch/mtbs_1985'\n",
    "bucket_name = 'smp-scratch'\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "\n",
    "# Import assetts of interest\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "geometry = ee.FeatureCollection('users/spotter/fire_cnn/raw/eurasia') #area of interest\n",
    "mod1 = ee.ImageCollection(\"MODIS/061/MOD10A1\") #active fire\n",
    "mod2 = ee.ImageCollection(\"MODIS/061/MYD10A1\") #active fire\n",
    "fire_cci = ee.ImageCollection(\"ESA/CCI/FireCCI/5_1\") #active fire\n",
    "mod_burn = ee.ImageCollection(\"MODIS/061/MCD64A1\") #mcd64a1\n",
    "snow = ee.ImageCollection('MODIS/006/MOD10A1') #modis snow cover\n",
    "# Load the MODIS water mask image and invert it.\n",
    "water_mask = ee.Image('MODIS/MOD44W/MOD44W_005_2000_02_24').select('water_mask').Not()\n",
    "\n",
    "\n",
    "sent_2A = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") #sentinel 2\n",
    "s2Clouds = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY') #cloud masking for sentinel\n",
    "# s2Clouds = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY') #cloud masking for sentinel\n",
    "# lfdb = ee.FeatureCollection(\"users/spotter/fire_cnn/raw/nbac_1985\") #nbac_fire_polygons, this can be any polygon shapefile, final version would be nbac and mtbs\n",
    "lfdb = ee.FeatureCollection(\"users/spotter/fire_cnn/ann_w_id\") #anna polygons \n",
    "lfdb_sub = ee.FeatureCollection(\"users/spotter/fire_cnn/anna_w_id_sampled\") #anna polygons \n",
    "\n",
    "# input_grid = ee.FeatureCollection('users/spotter/fire_cnn/raw/eurasia_dnbr_grid_clip') #grid to loop through to get around memory errors\n",
    "\n",
    "\n",
    "# Cloud masking Sentinel 2\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#probability of clouds\n",
    "MAX_CLOUD_PROBABILITY = 50\n",
    "\n",
    "def sent_maskcloud(image):\n",
    "\n",
    "\n",
    "    image = image.select(['B2', 'B3', 'B4', 'B8', 'B11', 'B12'], ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7'])# rename bands to match landsat\n",
    "\n",
    "    image =  image.toShort()\n",
    "\n",
    "    clouds = ee.Image(image.get('cloud_mask')).select('probability')\n",
    "\n",
    "    isNotCloud = clouds.lt(MAX_CLOUD_PROBABILITY)\n",
    "\n",
    "    image = image.updateMask(isNotCloud)\n",
    "\n",
    "    #reproject 30m but remember b1, b2 and b3 are 10 and the rest are 20\n",
    "    image1 = image.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4'])\n",
    "    image2 = image.select(['SR_B5', 'SR_B7'])\n",
    "\n",
    "\n",
    "    image1 = image1.reproject(\n",
    "    crs = image1.projection().crs(),\n",
    "    scale = 30) #resample for landsat\n",
    "\n",
    "\n",
    "    image2 = image2.reproject(\n",
    "    crs = image2.projection().crs(),\n",
    "    scale = 30) #resample for landsat\n",
    "\n",
    "    image = image1.addBands(image2)\n",
    "\n",
    "    return image\n",
    "\n",
    "#Join S2 SR with cloud probability dataset to add cloud mask.\n",
    "s2SrWithCloudMask = ee.Join.saveFirst('cloud_mask').apply(\n",
    "\n",
    "  primary=sent_2A,\n",
    "  secondary=s2Clouds,\n",
    "  condition=ee.Filter.equals(leftField='system:index', rightField='system:index'))\n",
    "\n",
    "#apply cloud masking\n",
    "sent_2A = ee.ImageCollection(s2SrWithCloudMask).map(sent_maskcloud)\n",
    "\n",
    "\n",
    "# Correct landsat scale factor and sentinel scale factor\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def mask(image):\n",
    "    qa = image.select('QA_PIXEL')\n",
    "    mask = qa.bitwiseAnd(8).eq(0).And(qa.bitwiseAnd(10).eq(0)).And(qa.bitwiseAnd(32).eq(0))\n",
    "    return(image.updateMask(mask))\n",
    "\n",
    "def land_scale(image):\n",
    "\n",
    "    return(image.multiply(0.0000275).add(-0.2))\n",
    "\n",
    "def sent_scale(image):\n",
    "    return(image.multiply(0.0001))\n",
    "\n",
    "\n",
    "# Logan Coefficients to apply\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "coeffs = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/raw_files/boreal_xcal_regression_coefficients.csv\").fillna(0)\n",
    "#l5\n",
    "def landsat_correct(sat, bands):\n",
    "\n",
    "    \"\"\"argument 1 is which sattelite, LANDASAT_5 or LANDSAT_8\n",
    "    argument 2 is bands of interest.  Bands must be in same order as EE,\n",
    "    \n",
    "    regression is of form,\n",
    "    L7 = B0 + (B1 * L5/8) + (B2 * L^2) + (B3 * L^3)\n",
    "    \"\"\"\n",
    "\n",
    "    #bands of interest in order of interest\n",
    "    l5 = coeffs[(coeffs['satellite'] == sat) & (coeffs['band.or.si'] .isin (bands))] \n",
    "\n",
    "    #arrange the band or si column\n",
    "    l5['band.or.si']=pd.Categorical(l5['band.or.si'],categories=bands)\n",
    "    l5=l5.sort_values('band.or.si')\n",
    "\n",
    "    b0 = l5['B0'].values.tolist()\n",
    "    b1 = l5['B1'].values.tolist()\n",
    "    b2 = l5['B2'].values.tolist()\n",
    "    b3 = l5['B3'].values.tolist()\n",
    "\n",
    "    return (b0, b1, b2, b3)\n",
    "\n",
    "#get the corrections, each output is a list at one of the four locations\n",
    "l8_corr = landsat_correct(sat = 'LANDSAT_8', bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'nbr', 'ndvi', 'ndii'])\n",
    "l5_corr = landsat_correct(sat = 'LANDSAT_5', bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'nbr', 'ndvi', 'ndii'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# I might need to convert to float\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def to_float(image):\n",
    "\n",
    "    b1 = image.select('SR_B1').cast({'SR_B1':'float'}) #0\n",
    "    b2 = image.select('SR_B2').cast({'SR_B2':'float'}) #1\n",
    "    b3 = image.select('SR_B3').cast({'SR_B3':'float'}) #2\n",
    "    b4 = image.select('SR_B4').cast({'SR_B4':'float'}) #3\n",
    "    b5 = image.select('SR_B5').cast({'SR_B5':'float'}) #4\n",
    "    b6 = image.select('SR_B7').cast({'SR_B7':'float'}) #5\n",
    "\n",
    "    image = b1.addBands(b2).addBands(b3).addBands(b4).addBands(b5).addBands(b6)\n",
    "\n",
    "    return image\n",
    "\n",
    "def filter_cl(image):\n",
    "    cl = image.select('ConfidenceLevel')\n",
    "    image = image.updateMask(cl.gte(50)).select('BurnDate')\n",
    "    return image\n",
    "\n",
    "def mod_act_map(image):\n",
    "    image = image.addBands(ee.Image(ee.Date(image.get('system:time_start')).getRelative('day','year').add(1)).clamp(1,366)).updateMask(image.select('FireMask').gte(7))\n",
    "    return image.updateMask(image.select('constant').gt(60));\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"gee-serdp-upload\"\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\"\n",
    "storage_client = storage.Client.from_service_account_json(\"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\")\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"gee-serdp-upload\"\n",
    "storage_client = storage.Client()\n",
    "# bucket_name = 'smp-scratch/mtbs_1985'\n",
    "bucket_name = 'smp-scratch'\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "\n",
    "# Function to return the pre_fire and post_fire landsat and sentinel 2 data\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def get_pre_post(pre_start, pre_end, post_start, post_end, geometry):\n",
    "\n",
    "    \"\"\"parameters are:\n",
    "    pre_start: the start date for pre fire imagery\n",
    "    pre_end: the end date for pre fire imagery\n",
    "    post_start: the start date for post fire imagery\n",
    "    post_end: the end date for post_fire imagery\n",
    "    geometry: the geometry to filter by\n",
    "    \"\"\"\n",
    "\n",
    "    #landsat 5\n",
    "    lt5 = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2').filterDate(pre_start, post_end).filterBounds(geometry)\n",
    "    #landsat 7\n",
    "    le7 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2').filterDate(pre_start, post_end).filterBounds(geometry)\n",
    "    #landsat 8\n",
    "    lc8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').filterDate(pre_start, post_end).filterBounds(geometry)\n",
    "    #sentinel 2\n",
    "    sent = sent_2A.filterDate(pre_start, post_end).filterBounds(geometry)\n",
    "\n",
    "    #select bands\n",
    "    pre_lt5 = lt5.filterDate(pre_start, pre_end).map(mask).map(land_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if pre_lt5.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        pre_lt5 = pre_lt5.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        pre_lt5_nbr = pre_lt5.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        pre_lt5_ndvi = pre_lt5.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        pre_lt5_ndii = pre_lt5.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        pre_lt5 = pre_lt5.addBands(pre_lt5_nbr).addBands(pre_lt5_ndvi).addBands(pre_lt5_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        l5_pre_corr = pre_lt5.multiply(l5_corr[1]).add(pre_lt5.pow(2).multiply(l5_corr[2])).add(pre_lt5.pow(3).multiply(l5_corr[3])).add(l5_corr[0])\n",
    "\n",
    "    #-------now do post-fire\n",
    "    #select bands\n",
    "    post_lt5 = lt5.filterDate(post_start, post_end).map(mask).map(land_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if post_lt5.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        post_lt5 = post_lt5.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        post_lt5_nbr = post_lt5.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        post_lt5_ndvi = post_lt5.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        post_lt5_ndii = post_lt5.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        post_lt5 = post_lt5.addBands(post_lt5_nbr).addBands(post_lt5_ndvi).addBands(post_lt5_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        l5_post_corr = post_lt5.multiply(l5_corr[1]).add(post_lt5.pow(2).multiply(l5_corr[2])).add(post_lt5.pow(3).multiply(l5_corr[3])).add(l5_corr[0])\n",
    "\n",
    "\n",
    "      #         #------------------------------------------Landsat 7, no corrections but get things clipped and do pre fire/post_fire stuff\n",
    "\n",
    "\n",
    "    #select bands\n",
    "    pre_le7 = le7.filterDate(pre_start, pre_end).map(mask).map(land_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if pre_le7.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        pre_le7 = pre_le7.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        pre_le7_nbr = pre_le7.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        pre_le7_ndvi = pre_le7.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        pre_le7_ndii = pre_le7.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        pre_le72 = pre_le7.addBands(pre_le7_nbr).addBands(pre_le7_ndvi).addBands(pre_le7_ndii)\n",
    "\n",
    "    #-------now do post-fire\n",
    "    #select bands\n",
    "    post_le7 = le7.filterDate(post_start, post_end).map(mask).map(land_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if post_le7.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        post_le7 = post_le7.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        post_le7_nbr = post_le7.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        post_le7_ndvi = post_le7.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        post_le7_ndii = post_le7.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        post_le72 = post_le7.addBands(post_le7_nbr).addBands(post_le7_ndvi).addBands(post_le7_ndii)\n",
    "\n",
    "    #------------------------------------------Landsat 8 corrections\n",
    "\n",
    "\n",
    "    #-------first do pre-fire\n",
    "\n",
    "    #select bands\n",
    "    pre_lc8 = lc8.filterDate(pre_start, pre_end).map(mask).map(land_scale).select(['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7'],['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']) .map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if pre_lc8.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        pre_lc8 = pre_lc8.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        pre_lc8_nbr = pre_lc8.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        pre_lc8_ndvi = pre_lc8.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        pre_lc8_ndii = pre_lc8.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        pre_lc8 = pre_lc8.addBands(pre_lc8_nbr).addBands(pre_lc8_ndvi).addBands(pre_lc8_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        l8_pre_corr = pre_lc8.multiply(l8_corr[1]).add(pre_lc8.pow(2).multiply(l8_corr[2])).add(pre_lc8.pow(3).multiply(l8_corr[3])).add(l8_corr[0])\n",
    "\n",
    "    #-------now do post-fire\n",
    "    #select bands\n",
    "    post_lc8 = lc8.filterDate(post_start, post_end).map(mask).map(land_scale).select(['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7'],['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']) .map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if post_lc8.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        post_lc8 = post_lc8.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        post_lc8_nbr = post_lc8.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        post_lc8_ndvi = post_lc8.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        post_lc8_ndii = post_lc8.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        post_lc8 = post_lc8.addBands(post_lc8_nbr).addBands(post_lc8_ndvi).addBands(post_lc8_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        l8_post_corr = post_lc8.multiply(l8_corr[1]).add(post_lc8.pow(2).multiply(l8_corr[2])).add(post_lc8.pow(3).multiply(l8_corr[3])).add(l8_corr[0])\n",
    "\n",
    "        # #          #------------------------------------------Sentinel 2 corrections, use landsat 8 coefficients\n",
    "\n",
    "\n",
    "    ##-------first do pre-fire\n",
    "\n",
    "    #select bands\n",
    "    pre_sent = sent_2A.filterDate(pre_start, pre_end).map(sent_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if pre_sent.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "          #take the median\n",
    "        pre_sent = pre_sent.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        pre_sent_nbr = pre_sent.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        pre_sent_ndvi = pre_sent.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        pre_sent_ndii = pre_sent.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        pre_sent = pre_sent.addBands(pre_sent_nbr).addBands(pre_sent_ndvi).addBands(pre_sent_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        sent_pre_corr = pre_sent.multiply(l8_corr[1]).add(pre_sent.pow(2).multiply(l8_corr[2])).add(pre_sent.pow(3).multiply(l8_corr[3])).add(l8_corr[0])\n",
    "\n",
    "    #-------now do post-fire\n",
    "    #select bands\n",
    "    post_sent = sent_2A.filterDate(post_start, post_end).map(sent_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if post_sent.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        post_sent = post_sent.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        post_sent_nbr = post_sent.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        post_sent_ndvi = post_sent.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        post_sent_ndii = post_sent.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        post_sent = post_sent.addBands(post_sent_nbr).addBands(post_sent_ndvi).addBands(post_sent_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        sent_post_corr = post_sent.multiply(l8_corr[1]).add(post_sent.pow(2).multiply(l8_corr[2])).add(post_sent.pow(3).multiply(l8_corr[3])).add(l8_corr[0])\n",
    "\n",
    "\n",
    "    #try to see if image exists, if so append\n",
    "\n",
    "    #----------------------all prefire\n",
    "\n",
    "    #       #empty list for pre-fire, use this to combine if we have land 5, 7, 8 or sentinel\n",
    "    pre_input = []\n",
    "\n",
    "    try:\n",
    "        l5_pre_corr.getInfo()\n",
    "        pre_input.append(l5_pre_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        pre_le72.getInfo()\n",
    "        pre_input.append(pre_le72)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        l8_pre_corr.getInfo()\n",
    "        pre_input.append(l8_pre_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        sent_pre_corr.getInfo()\n",
    "        pre_input.append(sent_pre_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    #----------------------all postfire\n",
    "\n",
    "    #         #       #empty list for post-fire, use this to combine if we have land 5, 7, 8 or sentinel\n",
    "    post_input = []\n",
    "\n",
    "    try:\n",
    "        l5_post_corr.getInfo()\n",
    "        post_input.append(l5_post_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        post_le72.getInfo()\n",
    "        post_input.append(post_le72)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        l8_post_corr.getInfo()\n",
    "        post_input.append(l8_post_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        sent_post_corr.getInfo()\n",
    "        post_input.append(sent_post_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #return the two lists of pre input and post input\n",
    "    return pre_input, post_input\n",
    "\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "#masking snow cover functions\n",
    "#reference https://developers.google.com/earth-engine/tutorials/community/identifying-first-day-no-snow\n",
    "def mask_snow(img):\n",
    "    return img.gte(10)\n",
    "\n",
    "def add_date_bands(img):\n",
    "    global start_date, start_year_var\n",
    "    date = img.date()\n",
    "    cal_doy = date.getRelative('day', 'year')\n",
    "    rel_doy = date.difference(start_date, 'day')\n",
    "    millis = date.millis()\n",
    "    date_bands = ee.Image.constant([cal_doy, rel_doy, millis, start_year_var]).rename(['calDoy', 'relDoy', 'millis', 'year'])\n",
    "    return img.addBands(date_bands).cast({'calDoy': 'int', 'relDoy': 'int', 'millis': 'long', 'year': 'int'}).set('millis', millis)\n",
    "\n",
    "\n",
    "# def process_year(year):\n",
    "#     global start_doy, start_date, start_year_var\n",
    "#     start_year_var = year\n",
    "#     first_doy = ee.Date.fromYMD(year, 1, 1)\n",
    "#     start_date = first_doy.advance(start_doy - 1, 'day')\n",
    "#     end_date = start_date.advance(1, 'year').advance(1, 'day')\n",
    "#     year_col = snow.filterDate(start_date, end_date)\n",
    "#     # no_snow_img = year_col.map(add_date_bands).sort('millis').reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).updateMask(analysis_mask).set('year', year)\n",
    "#     no_snow_img = year_col.map(add_date_bands).sort('millis').reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).set('year', year)\n",
    "\n",
    "#     return no_snow_img.updateMask(no_snow_img.select('snowCover').eq(0))\n",
    "\n",
    "\n",
    "# def process_year_fall(year):\n",
    "#     global start_doy, start_date, start_year_var\n",
    "#     start_year_var = year\n",
    "#     first_doy = ee.Date.fromYMD(year, 1, 1)\n",
    "#     start_date = first_doy.advance(start_doy - 1, 'day')\n",
    "#     end_date = start_date.advance(1, 'year').advance(1, 'day')\n",
    "#     year_col = snow.filterDate(start_date, end_date)\n",
    "#     # no_snow_img = year_col.map(add_date_bands).sort('millis').reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).updateMask(analysis_mask).set('year', year)\n",
    "#     no_snow_img = year_col.map(add_date_bands).sort('millis', False).reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).set('year', year)\n",
    "\n",
    "#     return no_snow_img.updateMask(no_snow_img.select('snowCover').eq(0))\n",
    "\n",
    "def process_year(year):\n",
    "    global start_date, start_year_var\n",
    "    start_year_var = year\n",
    "    # Set the start date to January 1st and end date to July 1st\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, 7, 1)\n",
    "    \n",
    "    # Filter snow data for the given date range\n",
    "    year_col = snow.filterDate(start_date, end_date)\n",
    "    \n",
    "    # Find the last day snow occurs in the range January 1st to July 1st\n",
    "    no_snow_img = year_col.map(add_date_bands).sort('millis').reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).set('year', year)\n",
    "    \n",
    "    # Mask out the areas with snow (snowCover = 0)\n",
    "    return no_snow_img.updateMask(no_snow_img.select('snowCover').eq(0))\n",
    "\n",
    "\n",
    "def process_year_fall(year):\n",
    "    global start_date, start_year_var\n",
    "    start_year_var = year\n",
    "    # Set the start date to August 31st and end date to December 31st\n",
    "    start_date = ee.Date.fromYMD(year, 8, 31)\n",
    "    end_date = ee.Date.fromYMD(year, 12, 31)\n",
    "    \n",
    "    # Filter snow data for the given date range\n",
    "    year_col = snow.filterDate(start_date, end_date)\n",
    "    \n",
    "    # Find the first day snow reappears in the range August 31st to December 31st\n",
    "    no_snow_img = year_col.map(add_date_bands).sort('millis', False).reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).set('year', year)\n",
    "    \n",
    "    # Mask out the areas without snow (snowCover = 0)\n",
    "    return no_snow_img.updateMask(no_snow_img.select('snowCover').eq(0))\n",
    "#good and best snow quality flags\n",
    "# Define a function to mask out poor quality snow cover data based on the NDSI_Snow_Cover_Basic_QA band\n",
    "def mask_quality_snow(image):\n",
    "    # Select the NDSI_Snow_Cover_Basic_QA band\n",
    "    qa = image.select('NDSI_Snow_Cover_Basic_QA')\n",
    "    \n",
    "    # Create a mask for good and best quality (bit 0 and 1 set to 0)\n",
    "    quality_mask = qa.bitwiseAnd(3).eq(0)\n",
    "    \n",
    "    # Update the image with the quality mask\n",
    "    return image.updateMask(quality_mask)\n",
    "\n",
    "snow = snow.map(mask_quality_snow).select('NDSI_Snow_Cover')\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# def filter_landsat_by_snow_days(img):\n",
    "#     cal_doy = img.select('calDoy')\n",
    "#     first_day = first_day_snow.select('calDoy')\n",
    "#     last_day = last_day_snow.select('calDoy')\n",
    "    \n",
    "#     mask = cal_doy.lt(first_day).Or(cal_doy.gt(last_day))\n",
    "    \n",
    "#     return img.updateMask(mask)\n",
    "\n",
    "# def add_date_bands(img):\n",
    "#     date = img.date()\n",
    "#     millis = date.millis()\n",
    "#     cal_doy = date.getRelative('day', 'year')\n",
    "#     date_bands = ee.Image.constant([cal_doy, millis]).rename(['calDoy', 'millis'])\n",
    "#     return img.addBands(date_bands)\n",
    "\n",
    "# # Add date bands to the Landsat collection\n",
    "# landsat_with_date = landsat8.filterDate(start_date, end_date).map(add_date_bands)\n",
    "\n",
    "\n",
    "# Placeholder from chat gpt\n",
    "\n",
    "# Now loop through each fire id, and for each pixel prefire and during fire I will find the last day of year of snow cover.  This will result in 2 images.  I will then take\n",
    "# the maximum value of these two images and use that to start my composites.  For instance if a pixel has a value of 120 (April 1st) and 130 (April 10th) I will use 130 for that pixel.  If the fire \n",
    "# occured in 2019 I will then use prefire as 2019-131 to 2019-161 and postfire as 2020-131 to 2019-161.  This will require looping through individual pixels and re-aggregating the data. \n",
    "# \n",
    "# In addition to this I will continue to use my old compositing method which is 1 year summer pre and one year summer post.  I will than take the max value through the snow-pixel based method and the old method. \n",
    "# \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#get all the ids within the lfdb shapefile\n",
    "all_ids = ee.List(lfdb_sub.distinct([\"ID\"]).aggregate_array(\"ID\"))\n",
    "all_ids = all_ids.getInfo()\n",
    "\n",
    "# all_ids = [52, 3266]\n",
    "all_ids = [11276, 1679, 5311, 15883, 13459, 7332, 5658, 3645, 8271, 8052, 1559, 7370, 9835, 9444, 9763, 1563, 9827, 4518, ]\n",
    "\n",
    "\n",
    "pre_months = ['-06-01']\n",
    "end_months = ['-08-31']\n",
    "\n",
    "all_months = dict(zip(pre_months, end_months))\n",
    "\n",
    "# Specify the folder within your bucket\n",
    "folder_name = 'ndsi_intervals\n",
    "# all_ids = [1]\n",
    "#loop through each fire polygon\n",
    "for i in all_ids:\n",
    "    \n",
    "\n",
    "\n",
    "    #name of output file\n",
    "    # fname = f\"median_{i}.tif\"\n",
    "    fname = f\"{folder_name}/final_{i}\"\n",
    "\n",
    "    #check if file exists on my bucket, if it does skip\n",
    "    stats = storage.Blob(bucket=bucket, name=fname).exists(storage_client)\n",
    "    if stats == False:\n",
    "        \n",
    "        #get the fire polygon of interest\n",
    "        sub_shape = lfdb.filter(ee.Filter.eq(\"ID\", i))\n",
    "\n",
    "        #get all other fire ids that are not this one\n",
    "        not_fires = lfdb.filter(ee.Filter.neq(\"ID\", i))\n",
    "\n",
    "      \n",
    "        #first get the bounding box of the fire\n",
    "        bbox = sub_shape.geometry().bounds()\n",
    "\n",
    "\n",
    "        #offset the bounding box by a random number\n",
    "        # all_rands = [0.00, 0.02, -0.02]\n",
    "        all_rands = [0.00]\n",
    "\n",
    "\n",
    "        rand1 = random.sample(all_rands, 1)[0]\n",
    "        rand2 = random.sample(all_rands, 1)[0]\n",
    "\n",
    "        #offset applied\n",
    "        proj = ee.Projection(\"EPSG:4326\").translate(rand1, rand2)\n",
    "        \n",
    "        #for the bounding box apply the randomly selected offset\n",
    "        final_buffer = ee.Geometry.Polygon(bbox.coordinates(), proj).transform(proj)\n",
    "        \n",
    "        #this is a bit of a hack but we have two different bounding box sizes because when we export we need to use some additonal area to avoid cuttoffs\n",
    "        final_buffer2 = final_buffer.buffer(distance= 5000).bounds()\n",
    "\n",
    "        final_buffer = final_buffer.buffer(distance= 40000)#.bounds().transform(proj='EPSG:3413', maxError=1)\n",
    "        # final_buffer = final_buffer.buffer(distance= 10000)#.bounds().transform(proj='EPSG:3413', maxError=1)\n",
    "\n",
    "    \n",
    "        #get the year of this fire\n",
    "        this_year = ee.Number(sub_shape.aggregate_array('Year').get(0))\n",
    "        \n",
    "        year = this_year.getInfo() \n",
    "        \n",
    "        pre_start = ee.Date.fromYMD(this_year.subtract(1), 6, 1)\n",
    "        pre_end = ee.Date.fromYMD(this_year.subtract(1), 8, 31)\n",
    "        post_start = pre_start.advance(2, 'year')\n",
    "        post_end = pre_end.advance(2, 'year')\n",
    "     \n",
    "        #just getting some date info here to ensure pre fire is one  year before and post fire is one year after the fire year of interest\n",
    "        startYear = pre_start.get('year')\n",
    "\n",
    "        #convert to client side\n",
    "        startYear = startYear.getInfo()  # local string\n",
    "        endYear = str(int(startYear) + 2)\n",
    "        startYear = str(startYear)\n",
    "\n",
    "        \n",
    "        #loop through all the months and use 85th percentile to download all data\n",
    "        all_months_images = []\n",
    "\n",
    "         #loop through all months \n",
    "        for m1, m2 in all_months.items():\n",
    "\n",
    "            if m1 == '-06-01' and m2 == '-08-31':\n",
    "\n",
    "                start_year = year - 1\n",
    "                end_year = year + 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                start_year = year - 1\n",
    "                end_year = year\n",
    "\n",
    "\n",
    "\n",
    "            #get pre dates\n",
    "            pre_start = str(start_year) + m1\n",
    "            pre_end = str(start_year) + m2\n",
    "\n",
    "            #get post dates\n",
    "\n",
    "\n",
    "            post_start = str(end_year) + m1\n",
    "            post_end = str(end_year) + m2\n",
    "\n",
    "\n",
    "\n",
    "            #apply the function to get the pre_fire image and post_fire image\n",
    "            all_imagery = get_pre_post(pre_start, pre_end, post_start, post_end, final_buffer)\n",
    "\n",
    "            #return the pre and post fire input imagery lists\n",
    "            pre_input = all_imagery[0]\n",
    "            post_input = all_imagery[1]\n",
    "\n",
    "            #if the lists each are larger than 1 we have imagery\n",
    "            if (len(pre_input) >0) and (len(post_input) > 0):\n",
    "\n",
    "                #take the median of the image collections\n",
    "                pre_input = ee.ImageCollection(pre_input)\n",
    "                post_input = ee.ImageCollection(post_input)\n",
    "\n",
    "                #get median of images\n",
    "                pre_input = pre_input.median()\n",
    "                post_input= post_input.median()\n",
    "\n",
    "                #difference the bands\n",
    "                raw_bands = pre_input.subtract(post_input).multiply(1000)\n",
    "\n",
    "                b1 = raw_bands.select('SR_B1').cast({'SR_B1':'short'})\n",
    "                b2 = raw_bands.select('SR_B2').cast({'SR_B2':'short'})\n",
    "                b3 = raw_bands.select('SR_B3').cast({'SR_B3':'short'})\n",
    "                b4 = raw_bands.select('SR_B4').cast({'SR_B4':'short'})\n",
    "                b5 = raw_bands.select('SR_B5').cast({'SR_B5':'short'})\n",
    "                b6 = raw_bands.select('SR_B7').cast({'SR_B7':'short'})\n",
    "                b7 = raw_bands.select('NBR').cast({'NBR':'short'})\n",
    "                b8 = raw_bands.select('NDVI').cast({'NDVI':'short'})\n",
    "                b9 = raw_bands.select('NDII').cast({'NDII':'short'})\n",
    "\n",
    "                #if using all bands\n",
    "                raw_bands = b7.addBands(b8).addBands(b9)\n",
    "\n",
    "\n",
    "                raw_bands = raw_bands.clip(final_buffer)\n",
    "\n",
    "                #we need to see which image ids from the entire lfdb are already included in the buffer\n",
    "                lfdb_filtered_orig = lfdb.filterBounds(final_buffer)\n",
    "\n",
    "                #ensure all fires are within the actual year of interest (this_year) and two years prior, otherwise ignore, this is to ensure we don't have nearby fires from previous years\n",
    "                first_year =  int(startYear) + 1\n",
    "                second_year =  int(startYear)\n",
    "                third_year =  int(startYear) - 1\n",
    "                fourth_year = int(startYear) + 2\n",
    "\n",
    "                lfdb_filtered = lfdb_filtered_orig.filter(ee.Filter.eq(\"Year\", year))\n",
    "\n",
    "                bad_filtered = lfdb_filtered_orig.filter(ee.Filter.Or(ee.Filter.eq(\"Year\", second_year), ee.Filter.eq(\"Year\", third_year), ee.Filter.eq(\"Year\", fourth_year)))\n",
    "\n",
    "\n",
    "                #get ids which are in image\n",
    "                all_ids_new = ee.List(lfdb_filtered.distinct([\"ID\"]).aggregate_array(\"ID\")).getInfo()\n",
    "\n",
    "\n",
    "                #remove ids from all dates which we do not need anymore\n",
    "                all_ids2 = [i for i in all_ids if i not in all_ids_new]\n",
    "\n",
    "                #area we have good fires\n",
    "                fire_rast = lfdb_filtered.reduceToImage(properties= ['ID'], reducer = ee.Reducer.first())\n",
    "\n",
    "                #areas we have fires from other years or nearby we don't want to use\n",
    "                bad_fire_rast = bad_filtered.reduceToImage(properties= ['ID'], reducer = ee.Reducer.first())\n",
    "\n",
    "                #change values to 1 for fire of interest\n",
    "                fire_rast = fire_rast.where(fire_rast.gt(0), 1)\n",
    "\n",
    "                #change values for bad fire raster to 1 as well\n",
    "                bad_fire_rast = bad_fire_rast.where(bad_fire_rast.gt(0), 1)\n",
    "\n",
    "                #if the fires overlap we want to keep those locations\n",
    "                bad_fire_rast = bad_fire_rast.where(bad_fire_rast.eq(1).And(fire_rast.eq(1)), 2).unmask(-999)\n",
    "\n",
    "                #rename to y for the fire raster\n",
    "                fire_rast = fire_rast.rename(['y'])\n",
    "\n",
    "                #copy the first values of raw_bands\n",
    "                y = raw_bands.select(['NBR'], ['y'])\n",
    "\n",
    "                #turn all values of y to 0\n",
    "                y  = y.where(y.gt(-10000), 0)\n",
    "\n",
    "                #turn values to 1 where fire_rast is 1\n",
    "                y  = y.where(fire_rast.eq(1), 1)\n",
    "\n",
    "                b10 = y.select('y').cast({'y':'short'})\n",
    "                \n",
    "                #add another band which is day of year, this value is 999\n",
    "                old_composite_day = ee.Image.constant(999).rename('day')\n",
    "                \n",
    "                # raw_bands = raw_bands.addBands(old_composite_day)\n",
    "\n",
    "                #combine all the bands for predictors\n",
    "                raw_bands = raw_bands.updateMask(bad_fire_rast.neq(1))\n",
    "\n",
    "                #add in the target variable\n",
    "                # raw_bands = raw_bands.addBands(b10)\n",
    "                \n",
    "#                 #-------------------------------------------------------------------------now composite based on snow cover, which will require looping through individual pixels\n",
    "                start_doy = 1\n",
    "                start_year_snow = year - 1\n",
    "                end_year_snow = year\n",
    "\n",
    "                #all years we are interested in for snow\n",
    "                snow_years = ee.List.sequence(start_year_snow, end_year_snow + 1)\n",
    "\n",
    "                annual_list = snow_years.map(process_year) #first day no snow\n",
    "                annual_list_fall = snow_years.map(process_year_fall) #first day snow\n",
    "\n",
    "                annual_col = ee.ImageCollection.fromImages(annual_list)\n",
    "                annual_col_fall = ee.ImageCollection.fromImages(annual_list_fall)\n",
    "\n",
    "\n",
    "                #get the images with the last day of snow in the start year and end year\n",
    "                # Subset the year of interest.\n",
    "                start_year_first_day_no_snow = annual_col.filter(ee.Filter.eq('year', start_year)).first().select('calDoy').clip(final_buffer)\n",
    "                end_year_first_day_no_snow = annual_col.filter(ee.Filter.eq('year', end_year)).first().select('calDoy').clip(final_buffer)\n",
    "                \n",
    "                start_year_first_day_no_snow_fall = annual_col_fall.filter(ee.Filter.eq('year', start_year)).first().select('calDoy').clip(final_buffer)\n",
    "                end_year_first_day_no_snow_fall = annual_col_fall.filter(ee.Filter.eq('year', end_year)).first().select('calDoy').clip(final_buffer)\n",
    "\n",
    "\n",
    "                #get the max day across both \n",
    "                max_snow_day = ee.ImageCollection([start_year_first_day_no_snow, end_year_first_day_no_snow]).max()\n",
    "                \n",
    "                #get the minimum for fall\n",
    "                max_snow_day_fall = ee.ImageCollection([start_year_first_day_no_snow_fall, end_year_first_day_no_snow_fall]).min()\n",
    "                \n",
    "                #Use reduceRegion to find the maximum value within the region\n",
    "                max_value = max_snow_day.reduceRegion(\n",
    "                    reducer=ee.Reducer.max(),\n",
    "                    geometry=final_buffer2,\n",
    "                    scale=500,\n",
    "                    maxPixels=1e13\n",
    "                ).getInfo()['calDoy']\n",
    "                \n",
    "                start_day = max_value + 7\n",
    "                \n",
    "                #Use reduceRegion to find the maximum value within the region\n",
    "                min_value = max_snow_day_fall.reduceRegion(\n",
    "                    reducer=ee.Reducer.min(),\n",
    "                    geometry=final_buffer2,\n",
    "                    scale=500,\n",
    "                    maxPixels=1e13\n",
    "                ).getInfo()['calDoy']\n",
    "                \n",
    "                end_day = min_value - 7\n",
    "\n",
    "\n",
    "#                 #get all the unique days\n",
    "#                  # #get all unique burn days from final\n",
    "#                 freqHist = ee.Dictionary(max_snow_day.reduceRegion(reducer = ee.Reducer.frequencyHistogram().unweighted(), geometry = final_buffer2, scale = 500, maxPixels = 1e13).get('calDoy'))\n",
    "#                 freqHist_fall = ee.Dictionary(max_snow_day_fall.reduceRegion(reducer = ee.Reducer.frequencyHistogram().unweighted(), geometry = final_buffer2, scale = 500, maxPixels = 1e13).get('calDoy'))\n",
    "\n",
    "#                 #get all unique burned days\n",
    "#                 all_days = freqHist.keys().getInfo()\n",
    "                \n",
    "#                 all_days_fall = freqHist_fall.keys().getInfo()\n",
    "                \n",
    "                # print('Done')\n",
    "\n",
    "\n",
    "# #                 all_days = ['162']\n",
    "\n",
    "#                 # print(all_days)\n",
    "\n",
    "#                 #keep track of which days are done already\n",
    "                all_days_done = []\n",
    "\n",
    "#                 # #now we need to loop through each one of these dates and get the VI's for each one\n",
    "                all_days_vi = []\n",
    "\n",
    "#                 for day in all_days:\n",
    "                    \n",
    "#                     #keep track of in this day which value is being selected\n",
    "#                     # this_day_for_select = []\n",
    "                    \n",
    "#                     # this_day_for_vi = []\n",
    "                    \n",
    "#                     if day != 'null':\n",
    "\n",
    "#                         # print(f\"This day is {day}\")\n",
    "\n",
    "#                         #generate two lists, which will be the monthly intervals we will loop through given this day, we will never surpass 244 as we don't want to go past september for this\n",
    "#                         # Initialize the starting value and the maximum limit\n",
    "#                         start_value = int(day) + 1\n",
    "#                         max_value = 244\n",
    "#                         increment = 30\n",
    "\n",
    "#                         #never go past september 1 which is 244 - 31, to ensure we have ability to composite\n",
    "#                         if int(day) < (244 - 31):\n",
    "\n",
    "                current_day = start_day\n",
    "                increment = 30\n",
    "\n",
    "                list1 = []\n",
    "                list2 = []\n",
    "\n",
    "                while current_day < end_day:\n",
    "                    next_day = min(current_day + increment, end_day)\n",
    "                    list1.append(current_day)\n",
    "                    list2.append(next_day)\n",
    "                    current_day = next_day\n",
    "\n",
    "#                 print(\"list1 =\", list1)\n",
    "#                 print(\"list2 =\", list2)\n",
    "\n",
    "                # Print the results\n",
    "                # print(\"list1 =\", list1)\n",
    "                # print(\"list2 =\", list2)\n",
    "\n",
    "                #iterate through zipped lists so we can get all necessary composites\n",
    "                all_days_for_composite = dict(zip(list1, list2))\n",
    "\n",
    "                #loop through all days\n",
    "                for d1, d2 in all_days_for_composite.items():\n",
    "\n",
    "                    fname_sub = f\"{folder_name}/{d1}_{d2}_{i}\"\n",
    "\n",
    "\n",
    "                    # print(d1, d2)\n",
    "\n",
    "                    #ensure the month is less than September as I don't want these pixels\n",
    "                    # this_date = ee.Date.fromYMD(year, 1, 1).advance(int(day) -1 , 'day')\n",
    "\n",
    "                    # if this_date.get('month').getInfo() < 9:\n",
    "\n",
    "                    #mask out all other days but this one in max_snow_day\n",
    "                    # this_max_snow_day = max_snow_day.updateMask(max_snow_day.eq(int(day)))\n",
    "\n",
    "                    #we want to get 30 day intervals so long as the last one doesnt suprass September, and save each one of these\n",
    "\n",
    "\n",
    "                    #get the dates for the prefire and post fire composites \n",
    "                    #convert the year and day of interest to yyyy/mm/dd\n",
    "                    this_date_pre_start = ee.Date.fromYMD(start_year_snow, 1, 1).advance(int(d1) +0 , 'day')\n",
    "                    this_date_pre_end = ee.Date.fromYMD(start_year_snow, 1, 1).advance(int(d2) +0 , 'day')\n",
    "\n",
    "\n",
    "                    #post fire dates\n",
    "                    this_date_post_start = ee.Date.fromYMD(end_year_snow, 1, 1).advance(int(d1) +0 , 'day')\n",
    "                    this_date_post_end = ee.Date.fromYMD(end_year_snow, 1, 1).advance(int(d2) +0 , 'day')\n",
    "\n",
    "\n",
    "                    #pre dates for filtering\n",
    "                    snow_pre_year = this_date_pre_start.get('year').getInfo()\n",
    "                    snow_pre_month_start = this_date_pre_start.get('month').getInfo()\n",
    "                    snow_pre_day_start = this_date_pre_start.get('day').getInfo()\n",
    "\n",
    "                    snow_pre_month_end = this_date_pre_end.get('month').getInfo()\n",
    "                    snow_pre_day_end = this_date_pre_end.get('day').getInfo()\n",
    "\n",
    "                    pre_start_snow = f\"{snow_pre_year}-{snow_pre_month_start}-{snow_pre_day_start}\"\n",
    "                    pre_end_snow = f\"{snow_pre_year}-{snow_pre_month_end}-{snow_pre_day_end}\"\n",
    "\n",
    "                    # print(f\"Pre start is {pre_start_snow}\")\n",
    "                    # print(f\"Pre end is {pre_end_snow}\")\n",
    "\n",
    "                    #post dates for filtering\n",
    "                    snow_post_year = this_date_post_start.get('year').getInfo()\n",
    "                    snow_post_month_start = this_date_post_start.get('month').getInfo()\n",
    "                    snow_post_day_start = this_date_post_start.get('day').getInfo()\n",
    "\n",
    "                    snow_post_month_end = this_date_post_end.get('month').getInfo()\n",
    "                    snow_post_day_end = this_date_post_end.get('day').getInfo()\n",
    "\n",
    "                    post_start_snow = f\"{snow_post_year}-{snow_post_month_start}-{snow_post_day_start}\"\n",
    "                    post_end_snow = f\"{snow_post_year}-{snow_post_month_end}-{snow_post_day_end}\"\n",
    "\n",
    "#                             print(f\"Post start is {post_start_snow}\")\n",
    "#                             print(f\"Post end is {post_end_snow}\")\n",
    "\n",
    "\n",
    "                    #geet the imagery\n",
    "                    all_imagery_snow = get_pre_post(pre_start_snow, pre_end_snow, post_start_snow, post_end_snow, final_buffer2)\n",
    "\n",
    "                    #return the pre and post fire input imagery lists\n",
    "                    pre_input_snow = all_imagery_snow[0]\n",
    "                    post_input_snow = all_imagery_snow[1]\n",
    "                    \n",
    "                    #if the lists each are larger than 1 we have imagery\n",
    "                    if (len(pre_input_snow) >0) and (len(post_input_snow) > 0):\n",
    "\n",
    "                        #take the median of the image collections\n",
    "                        pre_input_snow = ee.ImageCollection(pre_input_snow)\n",
    "                        post_input_snow = ee.ImageCollection(post_input_snow)\n",
    "\n",
    "                        #get median of images\n",
    "                        pre_input_snow = pre_input_snow.median()\n",
    "                        post_input_snow= post_input_snow.median()\n",
    "\n",
    "                        #difference the bands\n",
    "                        raw_bands_snow = pre_input_snow.subtract(post_input_snow).multiply(1000)\n",
    "\n",
    "                        b1_snow = raw_bands_snow.select('SR_B1').cast({'SR_B1':'short'})\n",
    "                        b2_snow = raw_bands_snow.select('SR_B2').cast({'SR_B2':'short'})\n",
    "                        b3_snow = raw_bands_snow.select('SR_B3').cast({'SR_B3':'short'})\n",
    "                        b4_snow = raw_bands_snow.select('SR_B4').cast({'SR_B4':'short'})\n",
    "                        b5_snow = raw_bands_snow.select('SR_B5').cast({'SR_B5':'short'})\n",
    "                        b6_snow = raw_bands_snow.select('SR_B7').cast({'SR_B7':'short'})\n",
    "                        b7_snow = raw_bands_snow.select('NBR').cast({'NBR':'short'})\n",
    "                        b8_snow = raw_bands_snow.select('NDVI').cast({'NDVI':'short'})\n",
    "                        b9_snow = raw_bands_snow.select('NDII').cast({'NDII':'short'})\n",
    "\n",
    "                        #make an image of all value of day\n",
    "                        # day_band = ee.Image.constant(int(day)).rename('day')\n",
    "                        #combine all the bands\n",
    "                        # raw_bands = b1.addBands(b2).addBands(b3).addBands(b4).addBands(b5).addBands(b6).addBands(b7).addBands(b8).addBands(b9)\n",
    "\n",
    "                        #select bands of interest, and mask to the fire only areas for this tile and this date\n",
    "                        # raw_bands_snow = b7_snow.addBands(b8_snow).addBands(b9_snow).addBands(day_band)#.updateMask(final_sub)\n",
    "                        raw_bands_snow = b7_snow.addBands(b8_snow).addBands(b9_snow)#.updateMask(final_sub)\n",
    "\n",
    "                        raw_bands_snow = raw_bands_snow.updateMask(bad_fire_rast.neq(1))\n",
    "\n",
    "                        #here I need to remove the bad fires still \n",
    "                        # raw_bands_snow = b7_snow\n",
    "\n",
    "\n",
    "                        #apply the mask\n",
    "                        # raw_bands_snow = raw_bands_snow.updateMask(this_max_snow_day)\n",
    "\n",
    "                        #append to all_days vi\n",
    "                        # this_day_for_vi.append(raw_bands_snow.select('NBR'))\n",
    "                        # this_day_for_select.append(d1)\n",
    "\n",
    "                        task = ee.batch.Export.image.toCloudStorage(\n",
    "                        image = raw_bands_snow.toShort(),\n",
    "                        region=final_buffer2, \n",
    "                        description=f\"{d1}_{d2}_{i}\",\n",
    "                        scale=30,\n",
    "                        crs='EPSG:3413',\n",
    "                        maxPixels=1e13,\n",
    "                        bucket = 'smp-scratch',\n",
    "                        fileNamePrefix=fname_sub)\n",
    "\n",
    "                        task.start()\n",
    "\n",
    "                        print(f\"Downloading {fname_sub}\")\n",
    "\n",
    "\n",
    "                        all_days_vi.append(raw_bands_snow)\n",
    "                        all_days_done.append(d1)\n",
    "\n",
    "\n",
    "            # print(len(this_day_for_select))\n",
    "        #append the old composite method to aLl_days_vi\n",
    "        all_days_vi.append(raw_bands)\n",
    "\n",
    "        #999 is for old compositing method\n",
    "        all_days_done.append(999)\n",
    "\n",
    "    #     #add a band with which day of year is picked\n",
    "    #     def add_index(image, index):\n",
    "    #         return image.addBands(ee.Image(index).rename('index'))\n",
    "\n",
    "    #     # collection_size = len(all_days_vi)\n",
    "    #     # Ensure the collection has exactly 5 images\n",
    "    #     collection = ee.ImageCollection(all_days_vi)\n",
    "    #     collection_list = collection.toList(collection.size())\n",
    "    #     collection_size = collection_list.size().getInfo()\n",
    "\n",
    "    #     indexed_collection = ee.ImageCollection([\n",
    "    #         add_index(ee.Image(collection_list.get(i)), i) for i in range(collection_size)\n",
    "    #     ])\n",
    "\n",
    "    #     # Create a composite image that records the index of the image with the max NBR value\n",
    "    #     max_index_image = indexed_collection.qualityMosaic('NBR').select('index')\n",
    "\n",
    "    #     # Create a lookup image from the values list\n",
    "    #     lookup_image = ee.Image(all_days_done).arrayRepeat(0, collection_size).arrayGet(max_index_image.int()).rename('doy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #here I want to determine for each pixel which date was chosen, this will require converting the images to arrays, and using pure python to select the chosen value\n",
    "        # this_dnbr = all_days_vi.select('NBR')\n",
    "\n",
    "        # this_dnbr_array = this_dnbr.getRegion(final_buffer2, scale = 30).getInfo()\n",
    "\n",
    "\n",
    "\n",
    "        #now combine all days and old compositing method and reduce with max\n",
    "        all_days_vi_image = ee.ImageCollection(all_days_vi).max()\n",
    "\n",
    "\n",
    "        all_days_vi_image = all_days_vi_image.updateMask(bad_fire_rast.neq(1))\n",
    "\n",
    "        #add y back\n",
    "        all_days_vi_image = all_days_vi_image.addBands(b10)\n",
    "\n",
    "\n",
    "        #export image to my cloud storage\n",
    "        task = ee.batch.Export.image.toCloudStorage(\n",
    "                                image = all_days_vi_image.toShort(),\n",
    "                                region=final_buffer2, \n",
    "                                description='median_' + str(i),\n",
    "                                scale=30,\n",
    "                                crs='EPSG:3413',\n",
    "                                maxPixels=1e13,\n",
    "                                bucket = 'smp-scratch',\n",
    "                                fileNamePrefix=fname)\n",
    "\n",
    "           #export image to my cloud storage\n",
    "        # task = ee.batch.Export.image.toCloudStorage(\n",
    "        #                         image = all_days_vi_image.toShort(),\n",
    "        #                         region=final_buffer2, \n",
    "        #                         description='median_no_old' + str(i),\n",
    "        #                         scale=30,\n",
    "        #                         crs='EPSG:3413',\n",
    "        #                         maxPixels=1e13,\n",
    "        #                         bucket = 'smp-scratch')\n",
    "\n",
    "        task.start()\n",
    "\n",
    "        #start download\n",
    "        print(f\"Downloading {fname}\")\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd3d2c-5420-448d-8c45-e4db47f6f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f26cd29-1bb1-405e-ae03-a616ebcd1b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_days_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fecd71c7-ae50-4b1e-ad00-0fed7821600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b46de0fc-dee8-4508-a757-1ad5f82641fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eba5963b11f48aca00ee88560bce0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[20, 0], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=(Togg"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an interactive map\n",
    "Map = geemap.Map()\n",
    "\n",
    "# Assuming all_days_vi_image is an ee.Image with the NBR band\n",
    "# You might want to adjust the visualization parameters to suit your data\n",
    "nbr_params = {\n",
    "    'min': -1,  # Adjust this value based on your NBR data range\n",
    "    'max': 1,   # Adjust this value based on your NBR data range\n",
    "    'palette': ['blue', 'white', 'green']  # Example palette, customize as needed\n",
    "}\n",
    "\n",
    "# Add the NBR band to the map\n",
    "Map.addLayer(all_days_vi_image.select('NBR'), nbr_params, 'NBR Band')\n",
    "\n",
    "# Display the map\n",
    "Map.addLayerControl()  # Add layer control to toggle layers on/off\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "630e1d0d-749c-4012-b02c-f4cacce3220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBR Band Visualization URL: https://earthengine.googleapis.com/v1/projects/earthengine-legacy/thumbnails/535cc80822af840b4a732f189187e255-3f49351ec543a4eee3e104944161e322:getPixels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/v1/projects/earthengine-legacy/thumbnails/535cc80822af840b4a732f189187e255-3f49351ec543a4eee3e104944161e322:getPixels\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming all_days_vi_image is an ee.Image with the NBR band\n",
    "nbr_params = {\n",
    "    'min': -1,\n",
    "    'max': 1,\n",
    "    'palette': ['blue', 'white', 'green']\n",
    "}\n",
    "\n",
    "# Get a URL for the NBR band visualization\n",
    "url = all_days_vi_image.clip(final_buffer2).select('NBR').getThumbURL(nbr_params)\n",
    "\n",
    "# Display the URL\n",
    "print(\"NBR Band Visualization URL:\", url)\n",
    "\n",
    "# Optionally, display the image in the notebook using IPython display\n",
    "from IPython.display import Image\n",
    "Image(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5bdbf-f12d-4c05-9b69-0a3175a4eb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-gee_ml]",
   "language": "python",
   "name": "conda-env-.conda-gee_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
