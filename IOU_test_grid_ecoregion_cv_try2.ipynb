{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c81dcb-8720-4d35-8b18-dbe8be9ac98e",
   "metadata": {},
   "source": [
    "This script takes the models which have I have run a 5-fold cross validation on and will get the mean IOU for each grid cell/ecoregion. \n",
    "It will also export a table which has the final (non-spatial) mean across all five folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b861f7b-b87e-472a-8cf3-2f532cf34bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import tensorflow\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "import segmentation_models as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_unet_collection import models\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "import warnings\n",
    "import glob\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb8904-0c71-4b34-8ccd-d60f008e3680",
   "metadata": {},
   "source": [
    "Functions used to run analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1079e1-f090-44cd-a0c7-cc4d6a170b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image gen class to be used when predicting\n",
    "min_max_vi = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "min_max_vi = min_max_vi[['6', '7', '8']]\n",
    "\n",
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    #will need pre defined variables batch_size, img_size, input_img_paths and target_img_paths\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "\t    self.batch_size = batch_size\n",
    "\t    self.img_size = img_size\n",
    "\t    self.input_img_paths = input_img_paths\n",
    "\t    self.target_img_paths = input_img_paths\n",
    "\n",
    "    #number of batches the generator is supposed to produceis the length of the paths divided by the batch siize\n",
    "    def __len__(self):\n",
    "\t    return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (x)\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (y)\n",
    "\t\t\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\") #create matrix of zeros which will have the dimension height, wideth, n_bands), 8 is the n_bands\n",
    "        \n",
    "  \n",
    "         #start populating x by enumerating over the input img paths\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "\n",
    "           #load image\n",
    "            img =  np.round(np.load(path), 3)\n",
    "            \n",
    "            if img.shape[2] == 4:\n",
    "                \n",
    "                img = img[:, :, :-1]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                img = img[:, :, 6:9]\n",
    "\n",
    "            # img = img * 1000\n",
    "            img = img.astype(float)\n",
    "            img = np.round(img, 3)\n",
    "            img[img == 0] = -999\n",
    "\n",
    "            img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "            img[img == -999] = np.nan\n",
    "\n",
    "            in_shape = img.shape\n",
    "            \n",
    "            #turn to dataframe to normalize\n",
    "            img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "\t\t\t\n",
    "            img = pd.DataFrame(img)\n",
    "\t\t\t\n",
    "            img.columns = min_max_vi.columns\n",
    "\t\t\t\n",
    "            img = pd.concat([min_max_vi, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "            #normalize 0 to 1\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\t\t\t\n",
    "            img = img.iloc[2:]\n",
    "#\n",
    "#             img = img.values.reshape(in_shape)\n",
    "            img = img.values.reshape(in_shape)\n",
    "\n",
    "#             replace nan with -1\n",
    "            img[np.isnan(img)] = -1\n",
    "\n",
    "#apply standardization\n",
    "# img = normalize(img, axis=(0,1))\n",
    "\n",
    "            img = np.round(img, 3)\n",
    "            #populate x\n",
    "            x[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "\n",
    "        #do tthe same thing for y\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "            img = img.astype(int)\n",
    "\n",
    "            img[img < 0] = 0\n",
    "            img[img >1] = 0\n",
    "            img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "            img[np.isnan(img)] = 0\n",
    "            img = img.astype(int)\n",
    "\n",
    "            # img =  tf.keras.utils.to_categorical(img, num_classes = 2)\n",
    "            # y[j] = np.expand_dims(img, 2) \n",
    "            y[j] = img\n",
    "  \n",
    "       \n",
    "    #Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "    # y[j] -= 1\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "#function which prediicts the selected model and saves some metrics\n",
    "def predict_model(model, generator, name, fid, count):\n",
    "    \n",
    "    '''\n",
    "    model: tensorflow model to predict\n",
    "    generator: keras generator with the images to predict on\n",
    "    name: string, model name\\\n",
    "    fid: variable I was looping through\n",
    "    count: count retained earlier\n",
    "    '''\n",
    "    #get the results from the nbac and mtbs model\n",
    "    model_1_res = model.evaluate_generator(generator, 100)\n",
    "\n",
    "    iou = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "\n",
    "    #make new dataframe with scores\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'FID': [fid],\n",
    "        'Count': [count],\n",
    "        'IOU': [iou],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "                        }, index=[0])  # Explicitly setting index to [0] for a single row\n",
    "\n",
    "    return in_df\n",
    "\n",
    "def grid_predict(model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, out_path, fold):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "       model_1: nbac/mtbs\n",
    "       model_2: nbac/mtbs 85\n",
    "       model_3: nbac/mtbs NDSI\n",
    "       model_4: combined\n",
    "       model_5: combined 85\n",
    "       model_6: combined NDSI\n",
    "       model_7: eurasia \n",
    "       model_8: eurasia 85\n",
    "       model_9: eurasia ndsi\n",
    "       out_path: str of where to save\n",
    "    '''\n",
    "    \n",
    "    os.makedirs(out_path, exist_ok = True)\n",
    "    \n",
    "    #for the grids I have two ids, FID which is the fishnet grid cells to loop through, and ID which is teh good anna polygon nids\n",
    "    fish_good = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/model_iou_spatial/grid.shp')\n",
    "    fish_good['Grid_ID'] = fish_good['Grid_ID'].astype(int)\n",
    "    #all the fishnet ids to loop through\n",
    "    all_fid = fish_good['Grid_ID'].unique().tolist()\n",
    "\n",
    "    #get all the testing full pathways to predict on, will need to filter fish good with this\n",
    "    testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "\n",
    "\n",
    "    #now I need to get the chunked files which match the fire ids to make new training, validation and testing times\n",
    "    #path to the chunked files\n",
    "    chunked_85 =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128')\n",
    "    chunked_old =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_old_subs_0_128')\n",
    "    chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "\n",
    "\n",
    "    def filter_chunked_85(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked_ndsi(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "    \n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "    \n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "        \n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked2(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        # filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked_old(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_old_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "\n",
    "    #new dnbr composite method\n",
    "    testing_names_85 = filter_chunked_85(testing_names, chunked_85)\n",
    "    testing_names_old = filter_chunked_old(testing_names, chunked_old)\n",
    "    testing_names_ndsi = filter_chunked_ndsi(testing_names, chunked_ndsi)\n",
    "\n",
    "\n",
    "\n",
    "    #save all dataframes\n",
    "    final = []\n",
    "\n",
    "    for fid in all_fid:\n",
    "\n",
    "        # print(f\"Processing {fid}\")\n",
    "\n",
    "        #sub shapefile for the grid\n",
    "        sub_grid = fish_good[fish_good['Grid_ID'] == fid]\n",
    "\n",
    "        #get the anna fire ids in this fid\n",
    "        anna_in_fid = sub_grid['Fire_ID'].unique().tolist()\n",
    "\n",
    "        count = len(anna_in_fid)\n",
    "\n",
    "        #get full pathway to the anna ids in the fids for new dnbr method\n",
    "        model_test_85 = filter_chunked2(anna_in_fid, testing_names_85)\n",
    "        model_test_old = filter_chunked2(anna_in_fid, testing_names_old)\n",
    "        model_test_ndsi = filter_chunked2(anna_in_fid, testing_names_ndsi)\n",
    "\n",
    "        #get the batch sie\n",
    "        if len(model_test_85) <= 45:\n",
    "\n",
    "            batch_size_85 = len(model_test_85)\n",
    "\n",
    "        else:\n",
    "            batch_size_85 = 45\n",
    "\n",
    "        if len(model_test_old) <= 45:\n",
    "\n",
    "            batch_size_old = len(model_test_old)\n",
    "            \n",
    "        else:\n",
    "            batch_size_old = 45\n",
    "\n",
    "        if len(model_test_ndsi) <= 45:\n",
    "\n",
    "            batch_size_ndsi = len(model_test_ndsi)\n",
    "            \n",
    "        else:\n",
    "            batch_size_ndsi = 45\n",
    "\n",
    "        \n",
    "        if len(model_test_85) > 0 and len(model_test_old) > 0 and len(model_test_ndsi) > 0:\n",
    "\n",
    "            #create an image generator for this fid and then predict\n",
    "            models_vi_gen_85 =  img_gen_vi(batch_size_85, img_size, model_test_85)\n",
    "            models_vi_gen_old =  img_gen_vi(batch_size_old, img_size, model_test_old)\n",
    "            models_vi_gen_ndsi =  img_gen_vi(batch_size_ndsi, img_size, model_test_ndsi)\n",
    "\n",
    "\n",
    "            mtbs_nbac = predict_model(model_1, models_vi_gen_old, 'MTBS_NBAC', fid, count)\n",
    "            mtbs_nbac_85 = predict_model(model_2, models_vi_gen_85, 'MTBS_NBAC_85', fid, count)\n",
    "            mtbs_nbac_ndsi = predict_model(model_3, models_vi_gen_ndsi, 'MTBS_NBAC_NDSI', fid, count)\n",
    "\n",
    "            combined = predict_model(model_4, models_vi_gen_old, 'Combined', fid, count)\n",
    "            combined_85 = predict_model(model_5, models_vi_gen_85, 'Combined_85', fid, count)\n",
    "            combined_ndsi = predict_model(model_6, models_vi_gen_ndsi, 'Combined_NDSI', fid, count)\n",
    "\n",
    "            \n",
    "            russia = predict_model(model_7, models_vi_gen_old, 'Russia', fid, count)\n",
    "            russia_85 = predict_model(model_8, models_vi_gen_85, 'Russia_85', fid, count)\n",
    "            russia_ndsi = predict_model(model_9, models_vi_gen_ndsi, 'Russia_NDSI', fid, count)\n",
    "\n",
    "\n",
    "            final.append(mtbs_nbac)\n",
    "            final.append(mtbs_nbac_85)\n",
    "            final.append(mtbs_nbac_ndsi)\n",
    "\n",
    "            final.append(combined)\n",
    "            final.append(combined_85)\n",
    "            final.append(combined_ndsi)\n",
    "\n",
    "            final.append(russia)\n",
    "            final.append(russia_85)\n",
    "            final.append(russia_ndsi)\n",
    "\n",
    "\n",
    "    final = pd.concat(final).reset_index(drop=True)\n",
    "\n",
    "    final['FID'] = final['FID'].astype(int)\n",
    "\n",
    "    final.to_csv(os.path.join(out_path, f\"{fold}.csv\"), index = False)\n",
    "    \n",
    "    return print(f\"Done Processing Fishnet grid Fold {fold}\")\n",
    "\n",
    "def ecoregion_predict(model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, out_path, fold):\n",
    "    \n",
    "    '''\n",
    "       model_1: nbac/mtbs\n",
    "       model_2: nbac/mtbs 85\n",
    "       model_3: nbac/mtbs NDSI\n",
    "       model_4: combined\n",
    "       model_5: combined 85\n",
    "       model_6: combined NDSI\n",
    "       model_7: eurasia \n",
    "       model_8: eurasia 85\n",
    "       model_9: eurasia ndsi\n",
    "       out_path: str of where to save\n",
    "    '''\n",
    "    \n",
    "    os.makedirs(out_path, exist_ok = True)\n",
    "        \n",
    "    #function to predict ecoregions\n",
    "    #for the grids I have two ids, FID which is the fishnet grid cells to loop through, and ID which is teh good anna polygon nids\n",
    "    fish_good = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/model_iou_spatial/ecoregions.shp')\n",
    "\n",
    "    #all the fishnet ids to loop through\n",
    "    all_fid = fish_good['Grid_ID'].unique().tolist()\n",
    "\n",
    "    #get all the testing full pathways to predict on, will need to filter fish good with this\n",
    "    testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "\n",
    "\n",
    "     #now I need to get the chunked files which match the fire ids to make new training, validation and testing times\n",
    "    #path to the chunked files\n",
    "    chunked_85 =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128')\n",
    "    chunked_old =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_old_subs_0_128')\n",
    "    chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "\n",
    "\n",
    "    def filter_chunked_85(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked_ndsi(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "    \n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "    \n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "        \n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked2(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        # filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked_old(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_old_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "\n",
    "    #new dnbr composite method\n",
    "    testing_names_85 = filter_chunked_85(testing_names, chunked_85)\n",
    "    testing_names_old = filter_chunked_old(testing_names, chunked_old)\n",
    "    testing_names_ndsi = filter_chunked_ndsi(testing_names, chunked_ndsi)\n",
    "\n",
    "\n",
    "\n",
    "    #save all dataframes\n",
    "    final = []\n",
    "\n",
    "    for fid in all_fid:\n",
    "\n",
    "        # print(f\"Processing {fid}\")\n",
    "\n",
    "        #sub shapefile for the grid\n",
    "        sub_grid = fish_good[fish_good['Grid_ID'] == fid]\n",
    "\n",
    "        #get the anna fire ids in this fid\n",
    "        anna_in_fid = sub_grid['Fire_ID'].unique().tolist()\n",
    "\n",
    "        count = len(anna_in_fid)\n",
    "\n",
    "        #get full pathway to the anna ids in the fids for new dnbr method\n",
    "        model_test_85 = filter_chunked2(anna_in_fid, testing_names_85)\n",
    "        model_test_old = filter_chunked2(anna_in_fid, testing_names_old)\n",
    "        model_test_ndsi = filter_chunked2(anna_in_fid, testing_names_ndsi)\n",
    "\n",
    "        #get the batch sie\n",
    "        if len(model_test_85) <= 45:\n",
    "\n",
    "            batch_size_85 = len(model_test_85)\n",
    "\n",
    "        else:\n",
    "            batch_size_85 = 45\n",
    "\n",
    "        if len(model_test_old) <= 45:\n",
    "\n",
    "            batch_size_old = len(model_test_old)\n",
    "\n",
    "        else:\n",
    "            batch_size_old = 45\n",
    "\n",
    "        if len(model_test_ndsi) <= 45:\n",
    "\n",
    "            batch_size_ndsi = len(model_test_ndsi)\n",
    "            \n",
    "        else:\n",
    "            batch_size_ndsi = 45\n",
    "\n",
    "\n",
    "        if len(model_test_85) > 0 and len(model_test_old) > 0 and len(model_test_ndsi) > 0:\n",
    "\n",
    "            #create an image generator for this fid and then predict\n",
    "            models_vi_gen_85 =  img_gen_vi(batch_size_85, img_size, model_test_85)\n",
    "            models_vi_gen_old =  img_gen_vi(batch_size_old, img_size, model_test_old)\n",
    "            models_vi_gen_ndsi =  img_gen_vi(batch_size_ndsi, img_size, model_test_ndsi)\n",
    "\n",
    "\n",
    "            mtbs_nbac = predict_model(model_1, models_vi_gen_old, 'MTBS_NBAC', fid, count)\n",
    "            mtbs_nbac_85 = predict_model(model_2, models_vi_gen_85, 'MTBS_NBAC_85', fid, count)\n",
    "            mtbs_nbac_ndsi = predict_model(model_3, models_vi_gen_ndsi, 'MTBS_NBAC_NDSI', fid, count)\n",
    "\n",
    "            combined = predict_model(model_4, models_vi_gen_old, 'Combined', fid, count)\n",
    "            combined_85 = predict_model(model_5, models_vi_gen_85, 'Combined_85', fid, count)\n",
    "            combined_ndsi = predict_model(model_6, models_vi_gen_ndsi, 'Combined_NDSI', fid, count)\n",
    "\n",
    "            \n",
    "            russia = predict_model(model_7, models_vi_gen_old, 'Russia', fid, count)\n",
    "            russia_85 = predict_model(model_8, models_vi_gen_85, 'Russia_85', fid, count)\n",
    "            russia_ndsi = predict_model(model_9, models_vi_gen_ndsi, 'Russia_NDSI', fid, count)\n",
    "\n",
    "\n",
    "            final.append(mtbs_nbac)\n",
    "            final.append(mtbs_nbac_85)\n",
    "            final.append(mtbs_nbac_ndsi)\n",
    "\n",
    "            final.append(combined)\n",
    "            final.append(combined_85)\n",
    "            final.append(combined_ndsi)\n",
    "\n",
    "            final.append(russia)\n",
    "            final.append(russia_85)\n",
    "            final.append(russia_ndsi)\n",
    "\n",
    "    final = pd.concat(final).reset_index(drop=True)\n",
    "\n",
    "    final.to_csv(os.path.join(out_path, f\"{fold}.csv\"), index = False)\n",
    "    \n",
    "    return print(f'Done Processing Ecoregion grid Fold {fold}')\n",
    "\n",
    "\n",
    "#function to save individual shapefiles for the fishnet grid\n",
    "def grid_shapes(in_path, out_path, fold):\n",
    "    \n",
    "    '''\n",
    "    in_path: str to pathway where csv is stored\n",
    "    out_path: str to pathway to save the shapefiles\n",
    "    fold: current fold to save the shapefile for\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    os.makedirs(out_path, exist_ok = True)\n",
    "    \n",
    "    from pyproj import CRS\n",
    "\n",
    "    #merge back to the original shapefile for plotting\n",
    "    grid_metrics = pd.read_csv(os.path.join(in_path, f\"{fold}.csv\"))\n",
    "\n",
    "    #fishnet good\n",
    "    # fish_good = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/model_iou_spatial/grid.shp')\n",
    "    fish_good = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/ea_grid_clip/ea_grid_clip.shp')\n",
    "    # Defining the Albers Equal Area projection parameters\n",
    "    # albers_ea_projection = CRS(\"+proj=aea +lat_0=56 +lon_0=100 +lat_1=50 +lat_2=70 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\")\n",
    "\n",
    "    #Transforming the GeoDataFrame to the new projection\n",
    "    #fish_good= fish_good.to_crs(albers_ea_projection)\n",
    "\n",
    "\n",
    "    fish_good = fish_good.rename(columns = {'Id': 'FID'})\n",
    "\n",
    "\n",
    "    # # \n",
    "    # fish_good['FID'] = fish_good['FID'].astype(int)\n",
    "\n",
    "    merged = fish_good.merge(grid_metrics, on = 'FID', how = 'inner')\n",
    "\n",
    "    merged = merged[['FID', 'Model', 'Count', 'IOU', 'Precision', 'Recall', 'F-1', 'Accuracy', 'geometry']]\n",
    "\n",
    "    #round floats to 2 digits\n",
    "    merged.loc[:, merged.select_dtypes(include=['float64']).columns] = merged.select_dtypes(include=['float64']).round(2)\n",
    "\n",
    "    #create a new column which will difference the combined model and the north america model from eurasia, do it so we subtract from russia, so larger values are better for russia\n",
    "    na = merged[merged['Model'] == 'MTBS_NBAC']\n",
    "    na_85 = merged[merged['Model'] == 'MTBS_NBAC_85']\n",
    "    na_ndsi = merged[merged['Model'] == 'MTBS_NBAC_NDSI']\n",
    "\n",
    "    combined = merged[merged['Model'] == 'Combined']\n",
    "    combined_85 = merged[merged['Model'] == 'Combined_85']\n",
    "    combined_ndsi = merged[merged['Model'] == 'Combined_NDSI']\n",
    "\n",
    "    \n",
    "    russ = merged[merged['Model'] == 'Russia']\n",
    "    russ_85 = merged[merged['Model'] == 'Russia_85']\n",
    "    russ_ndsi = merged[merged['Model'] == 'Russia_NDSI']\n",
    "\n",
    "\n",
    "    #determine difference in na vs eruasia models\n",
    "    na_russ_diff = russ['IOU'].values - na['IOU'].values\n",
    "    na_russ_85_diff = russ_85['IOU'].values - na_85['IOU'].values\n",
    "    na_russ_ndsi_diff = russ_ndsi['IOU'].values - na_ndsi['IOU'].values\n",
    "\n",
    "    combined_russ_diff = russ['IOU'].values - combined['IOU'].values\n",
    "    combined_russ_85_diff = russ_85['IOU'].values - combined_85['IOU'].values\n",
    "    combined_russ_ndsi_diff = russ_ndsi['IOU'].values - combined_ndsi['IOU'].values\n",
    "\n",
    "    #within the same na or eurasia models look at difference in IOU too, for instance na old and na 85 or ndsi\n",
    "    na_85_diff = na['IOU'].values - na_85['IOU'].values\n",
    "    na_ndsi_diff = na['IOU'].values - na_ndsi['IOU'].values\n",
    "    na_85_ndsi_diff = na_85['IOU'].values - na_ndsi['IOU'].values\n",
    "\n",
    "    combined_85_diff = combined['IOU'].values - combined_85['IOU'].values\n",
    "    combined_ndsi_diff = combined['IOU'].values - combined_ndsi['IOU'].values\n",
    "    combined_85_ndsi_diff = combined_85['IOU'].values - combined_ndsi['IOU'].values\n",
    "\n",
    "    russ_85_diff = russ['IOU'].values - russ_85['IOU'].values\n",
    "    russ_ndsi_diff = russ['IOU'].values - russ_ndsi['IOU'].values\n",
    "    russ_85_ndsi_diff = russ_85['IOU'].values - russ_ndsi['IOU'].values\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #for each unique model loop through and save individual files, this is for making maps in arc later easier\n",
    "    models = merged['Model'].unique()\n",
    "\n",
    "    for m in models:\n",
    "        \n",
    "        sub = merged[merged['Model'] == m]\n",
    "\n",
    "        #difference across models\n",
    "        sub['na_russ_IOU_diff'] = na_russ_diff\n",
    "        sub['na_russ_85_IOU_diff'] = na_russ_85_diff\n",
    "        sub['na_russ_ndsi_IOU_diff'] = na_russ_ndsi_diff\n",
    "\n",
    "        sub['combined_russ_IOU_diff'] = combined_russ_diff\n",
    "        sub['combined_russ_85_IOU_diff'] = combined_russ_85_diff\n",
    "        sub['combined_russ_ndsi_IOU_diff'] = combined_russ_ndsi_diff\n",
    "\n",
    "        #within same location differences\n",
    "        sub['na_85_diff'] = na_85_diff\n",
    "        sub['na_ndsi_diff'] = na_ndsi_diff\n",
    "        sub['na_85_ndsi_diff'] = na_85_ndsi_diff\n",
    "\n",
    "        sub['combined_85_diff'] = combined_85_diff\n",
    "        sub['combined_ndsi_diff'] = combined_ndsi_diff\n",
    "        sub['combined_85_ndsi_diff'] = combined_85_ndsi_diff\n",
    "\n",
    "        sub['russ_85_diff'] = russ_85_diff\n",
    "        sub['russ_ndsi_diff'] = russ_ndsi_diff\n",
    "        sub['russ_85_ndsi_diff'] = russ_85_ndsi_diff\n",
    "\n",
    "        sub.to_file(os.path.join(out_path,  f\"{m}.shp\"))\n",
    "\n",
    "    return print(f\"Done Processing Fishnet grid Shape {fold}\")\n",
    "\n",
    "def ecoregion_shapes(in_path, out_path, fold):   \n",
    "                               \n",
    "    '''\n",
    "    in_path: str to pathway where csv is stored\n",
    "    out_path: str to pathway to save the shapefiles\n",
    "    fold: current fold to save the \n",
    "    shapefile for\n",
    "    '''\n",
    "    os.makedirs(out_path, exist_ok = True)\n",
    "\n",
    "                               \n",
    "    #function to save for the ecoregion grid\n",
    "    from pyproj import CRS\n",
    "\n",
    "    #merge back to the original shapefile for plotting\n",
    "    grid_metrics = pd.read_csv(os.path.join(in_path, f\"{fold}.csv\"))\n",
    "\n",
    "    #fishnet good\n",
    "    # fish_good = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/model_iou_spatial/grid.shp')\n",
    "    fish_good = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/raw_files/EcoRegion_AlbEAadj/EcoRegion_AlbEAadj/EcoRegion_g.shp')\n",
    "    # Defining the Albers Equal Area projection parameters\n",
    "    # albers_ea_projection = CRS(\"+proj=aea +lat_0=56 +lon_0=100 +lat_1=50 +lat_2=70 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\")\n",
    "\n",
    "    # Transforming the GeoDataFrame to the new projection\n",
    "    # fish_good= fish_good.to_crs(albers_ea_projection)\n",
    "\n",
    "    fish_good = fish_good.rename(columns = {'ecoregion': 'FID'})\n",
    "\n",
    "\n",
    "    # # \n",
    "    # fish_good['FID'] = fish_good['FID'].astype(int)\n",
    "\n",
    "    merged = fish_good.merge(grid_metrics, on = 'FID', how = 'inner')\n",
    "\n",
    "    merged = merged[['FID', 'Model', 'Count', 'IOU', 'Precision', 'Recall', 'F-1', 'Accuracy', 'geometry']]\n",
    "\n",
    "    #round floats to 2 digits\n",
    "    merged.loc[:, merged.select_dtypes(include=['float64']).columns] = merged.select_dtypes(include=['float64']).round(2)\n",
    "\n",
    "    #create a new column which will difference the combined model and the north america model from eurasia, do it so we subtract from russia, so larger values are better for russia\n",
    "    na = merged[merged['Model'] == 'MTBS_NBAC']\n",
    "    na_85 = merged[merged['Model'] == 'MTBS_NBAC_85']\n",
    "    na_ndsi = merged[merged['Model'] == 'MTBS_NBAC_NDSI']\n",
    "\n",
    "    combined = merged[merged['Model'] == 'Combined']\n",
    "    combined_85 = merged[merged['Model'] == 'Combined_85']\n",
    "    combined_ndsi = merged[merged['Model'] == 'Combined_NDSI']\n",
    "\n",
    "    \n",
    "    russ = merged[merged['Model'] == 'Russia']\n",
    "    russ_85 = merged[merged['Model'] == 'Russia_85']\n",
    "    russ_ndsi = merged[merged['Model'] == 'Russia_NDSI']\n",
    "\n",
    "\n",
    "    #determine difference in na vs eruasia models\n",
    "    na_russ_diff = russ['IOU'].values - na['IOU'].values\n",
    "    na_russ_85_diff = russ_85['IOU'].values - na_85['IOU'].values\n",
    "    na_russ_ndsi_diff = russ_ndsi['IOU'].values - na_ndsi['IOU'].values\n",
    "\n",
    "    combined_russ_diff = russ['IOU'].values - combined['IOU'].values\n",
    "    combined_russ_85_diff = russ_85['IOU'].values - combined_85['IOU'].values\n",
    "    combined_russ_ndsi_diff = russ_ndsi['IOU'].values - combined_ndsi['IOU'].values\n",
    "\n",
    "    #within the same na or eurasia models look at difference in IOU too, for instance na old and na 85 or ndsi\n",
    "    na_85_diff = na['IOU'].values - na_85['IOU'].values\n",
    "    na_ndsi_diff = na['IOU'].values - na_ndsi['IOU'].values\n",
    "    na_85_ndsi_diff = na_85['IOU'].values - na_ndsi['IOU'].values\n",
    "\n",
    "    combined_85_diff = combined['IOU'].values - combined_85['IOU'].values\n",
    "    combined_ndsi_diff = combined['IOU'].values - combined_ndsi['IOU'].values\n",
    "    combined_85_ndsi_diff = combined_85['IOU'].values - combined_ndsi['IOU'].values\n",
    "\n",
    "    russ_85_diff = russ['IOU'].values - russ_85['IOU'].values\n",
    "    russ_ndsi_diff = russ['IOU'].values - russ_ndsi['IOU'].values\n",
    "    russ_85_ndsi_diff = russ_85['IOU'].values - russ_ndsi['IOU'].values\n",
    "    \n",
    "    #for each unique model loop through and save individual files, this is for making maps in arc later easier\n",
    "    models = merged['Model'].unique()\n",
    "\n",
    "    for m in models:\n",
    "        \n",
    "        sub = merged[merged['Model'] == m]\n",
    "\n",
    "        #difference across models\n",
    "        sub['na_russ_IOU_diff'] = na_russ_diff\n",
    "        sub['na_russ_85_IOU_diff'] = na_russ_85_diff\n",
    "        sub['na_russ_ndsi_IOU_diff'] = na_russ_ndsi_diff\n",
    "\n",
    "        sub['combined_russ_IOU_diff'] = combined_russ_diff\n",
    "        sub['combined_russ_85_IOU_diff'] = combined_russ_85_diff\n",
    "        sub['combined_russ_ndsi_IOU_diff'] = combined_russ_ndsi_diff\n",
    "\n",
    "        #within same location differences\n",
    "        sub['na_85_diff'] = na_85_diff\n",
    "        sub['na_ndsi_diff'] = na_ndsi_diff\n",
    "        sub['na_85_ndsi_diff'] = na_85_ndsi_diff\n",
    "\n",
    "        sub['combined_85_diff'] = combined_85_diff\n",
    "        sub['combined_ndsi_diff'] = combined_ndsi_diff\n",
    "        sub['combined_85_ndsi_diff'] = combined_85_ndsi_diff\n",
    "\n",
    "        sub['russ_85_diff'] = russ_85_diff\n",
    "        sub['russ_ndsi_diff'] = russ_ndsi_diff\n",
    "        sub['russ_85_ndsi_diff'] = russ_85_ndsi_diff\n",
    "\n",
    "        sub.to_file(os.path.join(out_path,  f\"{m}.shp\"))\n",
    "\n",
    "    return print(f\"Done Processing Ecoregion Shape {fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f86ecf-25d2-455f-943a-ecfee5029d68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 4\n",
      "Done Processing Fishnet grid Fold 0\n",
      "Done Processing Fishnet grid Shape 0\n",
      "Done Processing Ecoregion grid Fold 0\n",
      "Done Processing Ecoregion Shape 0\n",
      "Done Processing Fishnet grid Fold 1\n",
      "Done Processing Fishnet grid Shape 1\n",
      "Done Processing Ecoregion grid Fold 1\n",
      "Done Processing Ecoregion Shape 1\n",
      "Done Processing Fishnet grid Fold 2\n",
      "Done Processing Fishnet grid Shape 2\n",
      "Done Processing Ecoregion grid Fold 2\n",
      "Done Processing Ecoregion Shape 2\n",
      "Done Processing Fishnet grid Fold 3\n",
      "Done Processing Fishnet grid Shape 3\n",
      "Done Processing Ecoregion grid Fold 3\n",
      "Done Processing Ecoregion Shape 3\n",
      "Done Processing Fishnet grid Fold 4\n",
      "Done Processing Fishnet grid Shape 4\n",
      "Done Processing Ecoregion grid Fold 4\n",
      "Done Processing Ecoregion Shape 4\n"
     ]
    }
   ],
   "source": [
    "#batch size and img size\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "#folds to loop through\n",
    "folds = range(0, 5)\n",
    "\n",
    "# folds = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "#don't need to read in loops as no folds here\n",
    "#nbac mtbs model\n",
    "model_1 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_regularize_50_global_norm.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "\n",
    "#nbac mtbs model with 85% dnbr threshold\n",
    "model_2 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_regularize_50_global_norm_85.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "#nbac mtbs with NDSI\n",
    "model_3 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_regularize_50_global_norm_ndsi.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "\n",
    "for fold in folds:\n",
    "#loop through the folds\n",
    "\n",
    "     #combined original dnbr method\n",
    "    model_4 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_old_dnbr_{fold}_t2.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "    #combined 85\n",
    "    model_5 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_t2.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "\n",
    "    #combined ndsi\n",
    "    model_6 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_{fold}.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "    \n",
    "\n",
    "    #russia original\n",
    "    model_7 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_{fold}_t2.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "    #russia old dnbr method\n",
    "    model_8 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_old_dnbr_{fold}_t2.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "\n",
    "    #russia dsi method\n",
    "    model_9 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_{fold}.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "    \n",
    "    #predict the fishnet\n",
    "    fish_out = \"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results_t2/grid/csvs\"\n",
    "    fish_shp_out = f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results_t2/grid/shapefiles/{fold}\"\n",
    "    \n",
    "    grid_predict(model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, fish_out, fold)\n",
    "    grid_shapes(fish_out, fish_shp_out, fold)\n",
    "    \n",
    "    #predict the ecoregoins\n",
    "    ecoregion_out = \"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results_t2/ecoregion/csvs\"\n",
    "    ecoregion_shp_out = f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results_t2/ecoregion/shapefiles/{fold}\"\n",
    "    \n",
    "    ecoregion_predict(model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, ecoregion_out, fold)\n",
    "    ecoregion_shapes(ecoregion_out, ecoregion_shp_out, fold)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681fe699-b845-462c-b1ec-f15acb6bdab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524921a-c227-49a9-8add-3d5983110848",
   "metadata": {},
   "source": [
    "Function to get the mean IoU regardless of spatial location, don't need to use the shapefiles for this, just predict IoU on all the val files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062c738f-9006-41b0-9ee1-4724856d3924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 4\n",
      "Done Processing Final Median for fold 0\n",
      "Done Processing Final Median for fold 1\n",
      "Done Processing Final Median for fold 2\n",
      "Done Processing Final Median for fold 3\n",
      "Done Processing Final Median for fold 4\n"
     ]
    }
   ],
   "source": [
    "#batch size and img size\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "folds = range(0, 5)\n",
    "def predict_model_single(model, generator, name):\n",
    "    \n",
    "    '''\n",
    "    model: tensorflow model to predict\n",
    "    generator: keras generator with the images to predict on\n",
    "    name: string, model name\\\n",
    "    fid: variable I was looping through\n",
    "    count: count retained earlier\n",
    "    '''\n",
    "    #get the results from the nbac and mtbs model\n",
    "    model_1_res = model.evaluate_generator(generator, 100)\n",
    "\n",
    "    iou = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "\n",
    "    #make new dataframe with scores\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU': [iou],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "                        }, index=[0])  # Explicitly setting index to [0] for a single row\n",
    "\n",
    "    return in_df\n",
    "\n",
    "def get_final_mean(model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, out_path, fold):\n",
    "    \n",
    "    '''\n",
    "       model_1: nbac/mtbs\n",
    "       model_2: nbac/mtbs 85\n",
    "       model_3: nbac/mtbs NDSI\n",
    "       model_4: combined\n",
    "       model_5: combined 85\n",
    "       model_6: combined NDSI\n",
    "       model_7: eurasia \n",
    "       model_8: eurasia 85\n",
    "       model_9: eurasia ndsi\n",
    "       out_path: str of where to save\n",
    "    '''\n",
    "    \n",
    "    os.makedirs(out_path, exist_ok = True)\n",
    "        \n",
    "    #get all the testing full pathways to predict on, will need to filter fish good with this\n",
    "    testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "\n",
    "\n",
    "    #now I need to get the chunked files which match the fire ids to make new training, validation and testing times\n",
    "    #path to the chunked files\n",
    "    chunked_85 =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128')\n",
    "    # chunked_old =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_old_subs_0_128')\n",
    "    chunked_old =  os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_old_subs_0_128')\n",
    "    chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "\n",
    "    def filter_chunked_85(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked_ndsi(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "    \n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "    \n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "        \n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked2(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        # filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_training_85_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "    def filter_chunked_old(in_names, chunked):\n",
    "        \"\"\"\n",
    "        Filters items in the 'chunked' list based on whether the specified part of\n",
    "        each item (extracted by splitting the item's string) is in 'training_names'.\n",
    "\n",
    "        Parameters:\n",
    "        - training_names: List of integers to filter against.\n",
    "        - chunked: List of strings, where each string is a filename that contains numbers.\n",
    "\n",
    "        Returns:\n",
    "        - List of strings from 'chunked' that match the filtering criteria.\n",
    "        \"\"\"\n",
    "        # Filter the 'chunked' list\n",
    "        filtered_chunked = [\n",
    "            name for name in chunked \n",
    "            if int(name.split('_')[-1].split('.')[0]) in in_names\n",
    "        ]\n",
    "\n",
    "        filtered_chunked = ['/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_old_subs_0_128/' + i for i in filtered_chunked]\n",
    "        return filtered_chunked\n",
    "\n",
    "\n",
    "    #new dnbr composite method\n",
    "    testing_names_85 = filter_chunked_85(testing_names, chunked_85)\n",
    "    testing_names_old = filter_chunked_old(testing_names, chunked_old)\n",
    "    testing_names_ndsi = filter_chunked_ndsi(testing_names, chunked_ndsi)\n",
    "    \n",
    "    batch_size = 45\n",
    "    \n",
    "    models_vi_gen_85 =  img_gen_vi(batch_size, img_size, testing_names_85)\n",
    "    models_vi_gen_old =  img_gen_vi(batch_size, img_size, testing_names_old)\n",
    "    models_vi_gen_ndsi =  img_gen_vi(batch_size, img_size, testing_names_ndsi)\n",
    "\n",
    "\n",
    "    mtbs_nbac = predict_model_single(model_1, models_vi_gen_old, 'MTBS_NBAC')\n",
    "    mtbs_nbac_85 = predict_model_single(model_2, models_vi_gen_85, 'MTBS_NBAC_85')\n",
    "    mtbs_nbac_ndsi = predict_model_single(model_3, models_vi_gen_ndsi, 'MTBS_NBAC_NDSI')\n",
    "\n",
    "    combined = predict_model_single(model_4, models_vi_gen_old, 'Combined')\n",
    "    combined_85 = predict_model_single(model_5, models_vi_gen_85, 'Combined_85')\n",
    "    combined_ndsi = predict_model_single(model_6, models_vi_gen_ndsi, 'Combined_NDSI')\n",
    "\n",
    "    \n",
    "    russia = predict_model_single(model_7, models_vi_gen_old, 'Russia')\n",
    "    russia_85 = predict_model_single(model_8, models_vi_gen_85, 'Russia_85')\n",
    "    russia_ndsi = predict_model_single(model_9, models_vi_gen_ndsi, 'Russia_NDSI')\n",
    "\n",
    "    \n",
    "    final = pd.concat([mtbs_nbac, mtbs_nbac_85, mtbs_nbac_ndsi, combined, combined_85, combined_ndsi, russia, russia_85, russia_ndsi])\n",
    "    \n",
    "    final['Fold'] = fold\n",
    "    \n",
    "    final.to_csv(os.path.join(out_path, f\"{fold}.csv\"), index = False)\n",
    "    \n",
    "    return print(f'Done Processing Final Median for fold {fold}')\n",
    "\n",
    "out_path =  \"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results/final_means_try2\"\n",
    "\n",
    "    \n",
    "#don't need to read in loops as no folds here\n",
    "#nbac mtbs model\n",
    "model_1 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_regularize_50_global_norm.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "\n",
    "#nbac mtbs model with 85% dnbr threshold\n",
    "model_2 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_regularize_50_global_norm_85.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "#nbac mtbs with NDSI\n",
    "model_3 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_regularize_50_global_norm_ndsi.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "\n",
    "for fold in folds:\n",
    "#loop through the folds\n",
    "\n",
    "     #combined original dnbr method\n",
    "    # model_4 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_old_dnbr_{fold}_t2.tf\", \n",
    "    #                                            custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                            'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "    #                                                             'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                              'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "    #                                                               'accuracy': 'accuracy'})\n",
    "    \n",
    "    model_4 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_old.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "    #combined 85\n",
    "    model_5 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_t2.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "\n",
    "    #combined ndsi\n",
    "    model_6 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_{fold}.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "    \n",
    "\n",
    "    #russia monthly\n",
    "    # model_7 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_{fold}_t2.tf\", \n",
    "    #                                            custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                            'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "    #                                                             'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                              'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "    #                                                               'accuracy': 'accuracy'})\n",
    "\n",
    "    #russia old\n",
    "    model_7 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_{fold}_old.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "\n",
    "        #russia monthly\n",
    "    model_8 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_{fold}_t2.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "    #russia old old\n",
    "    # model_8 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_old_dnbr_{fold}_t2.tf\", \n",
    "    #                                            custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                            'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "    #                                                             'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                              'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "    #                                                               'accuracy': 'accuracy'})\n",
    "\n",
    "    #russia ndsi method\n",
    "    model_9 = tensorflow.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_{fold}.tf\", \n",
    "                                               custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                               'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                                'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                                 'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                                  'accuracy': 'accuracy'})\n",
    "    \n",
    "    \n",
    "    get_final_mean(model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, out_path, fold)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f83217-b7db-4e95-a588-4bc188c52044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38d1eb6-4ed5-40b3-a712-cc70a22045c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the IOU:             Model    F-1\n",
      "0        Combined  0.786\n",
      "1     Combined_85  0.796\n",
      "2   Combined_NDSI  0.830\n",
      "3       MTBS_NBAC  0.814\n",
      "4    MTBS_NBAC_85  0.780\n",
      "5  MTBS_NBAC_NDSI  0.804\n",
      "6          Russia  0.832\n",
      "7       Russia_85  0.814\n",
      "8     Russia_NDSI  0.812\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Set the directory where your CSV files are stored\n",
    "directory_path = \"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results/final_means_try2\"\n",
    "\n",
    "# Create a single DataFrame from all CSV files\n",
    "combined_df = pd.concat(\n",
    "    [pd.read_csv(f) for f in glob.glob(f\"{directory_path}/*.csv\")],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Calculate the mean of the 'IoU' column\n",
    "mean_iou = combined_df.groupby('Model')['F-1'].mean().reset_index()\n",
    "\n",
    "print(f\"The mean of the IOU: {mean_iou}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b32883-a49f-45f3-8130-240bbea43e7e",
   "metadata": {},
   "source": [
    "Now I need some functions to get the median for the shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bae4e8e-2017-4adc-8211-2faaef05e0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Processing final shapefiles\n",
      "Done Processing final shapefiles\n"
     ]
    }
   ],
   "source": [
    "def spatial_median(in_path, out_path):\n",
    "    \n",
    "    # Set the directory where your CSV files are stored\n",
    "    directory_path = in_path\n",
    "    os.makedirs(out_path, exist_ok = True)\n",
    "\n",
    "    folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    for_comb = []\n",
    "    for fold in folds:\n",
    "\n",
    "        in_files = os.listdir(os.path.join(directory_path, str(fold)))\n",
    "\n",
    "        for f in in_files:\n",
    "\n",
    "            if f.endswith('.shp'):\n",
    "                f_name = f.split('.')[0]\n",
    "                in_file = gpd.read_file(os.path.join(directory_path, str(fold), f))\n",
    "                in_file = in_file[['FID', 'Model', 'IOU', 'geometry']]\n",
    "                in_file['Fold'] = fold\n",
    "                for_comb.append(in_file)\n",
    "\n",
    "\n",
    "    # Ensure the concatenation retains GeoDataFrame structure\n",
    "    merged = gpd.GeoDataFrame(pd.concat(for_comb, ignore_index=True))\n",
    "\n",
    "    # Use dissolve to aggregate by 'FID' and 'Model' while calculating mean 'IOU'\n",
    "    # and keeping the last geometry for each group (adjust according to needs)\n",
    "    merged = merged.dissolve(by=['FID', 'Model'], aggfunc={'IOU': 'mean'}).reset_index()\n",
    "    \n",
    "    #round numeric columns to 2 digits\n",
    "    numeric_cols = merged.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Round all numeric columns to 2 decimal places\n",
    "    merged[numeric_cols] = merged[numeric_cols].round(2)\n",
    "\n",
    "\n",
    "     ##create a new column which will difference the combined model and the north america model from eurasia, do it so we subtract from russia, so larger values are better for russia\n",
    "    na = merged[merged['Model'] == 'MTBS_NBAC']\n",
    "    na_85 = merged[merged['Model'] == 'MTBS_NBAC_85']\n",
    "    na_ndsi = merged[merged['Model'] == 'MTBS_NBAC_NDSI']\n",
    "    \n",
    "    combined = merged[merged['Model'] == 'Combined']\n",
    "    combined_85 = merged[merged['Model'] == 'Combined_85']\n",
    "    combined_ndsi = merged[merged['Model'] == 'Combined_NDSI']\n",
    "    \n",
    "    russ = merged[merged['Model'] == 'Russia']\n",
    "    russ_85 = merged[merged['Model'] == 'Russia_85']\n",
    "    russ_ndsi = merged[merged['Model'] == 'Russia_NDSI']\n",
    "\n",
    "    #determine difference in na vs eruasia models\n",
    "    na_russ_diff = russ['IOU'].values - na['IOU'].values\n",
    "    na_russ_85_diff = russ_85['IOU'].values - na_85['IOU'].values\n",
    "    na_russ_ndsi_diff = russ_ndsi['IOU'].values - na_ndsi['IOU'].values\n",
    "\n",
    "    combined_russ_diff = russ['IOU'].values - combined['IOU'].values\n",
    "    combined_russ_85_diff = russ_85['IOU'].values - combined_85['IOU'].values\n",
    "    combined_russ_ndsi_diff = russ_ndsi['IOU'].values - combined_ndsi['IOU'].values\n",
    "\n",
    "    #within the same na or eurasia models look at difference in IOU too, for instance na old and na 85 or ndsi\n",
    "    na_85_diff = na['IOU'].values - na_85['IOU'].values\n",
    "    na_ndsi_diff = na['IOU'].values - na_ndsi['IOU'].values\n",
    "    na_85_ndsi_diff = na_85['IOU'].values - na_ndsi['IOU'].values\n",
    "\n",
    "    combined_85_diff = combined['IOU'].values - combined_85['IOU'].values\n",
    "    combined_ndsi_diff = combined['IOU'].values - combined_ndsi['IOU'].values\n",
    "    combined_85_ndsi_diff = combined_85['IOU'].values - combined_ndsi['IOU'].values\n",
    "\n",
    "    russ_85_diff = russ['IOU'].values - russ_85['IOU'].values\n",
    "    russ_ndsi_diff = russ['IOU'].values - russ_ndsi['IOU'].values\n",
    "    russ_85_ndsi_diff = russ_85['IOU'].values - russ_ndsi['IOU'].values\n",
    "    \n",
    "    #for each unique model loop through and save individual files, this is for making maps in arc later easier\n",
    "    models = merged['Model'].unique()\n",
    "\n",
    "    for m in models:\n",
    "        \n",
    "        sub = merged[merged['Model'] == m]\n",
    "\n",
    "        #difference across models\n",
    "        sub['na_russ_IOU_diff'] = na_russ_diff\n",
    "        sub['na_russ_85_IOU_diff'] = na_russ_85_diff\n",
    "        sub['na_russ_ndsi_IOU_diff'] = na_russ_ndsi_diff\n",
    "\n",
    "        sub['combined_russ_IOU_diff'] = combined_russ_diff\n",
    "        sub['combined_russ_85_IOU_diff'] = combined_russ_85_diff\n",
    "        sub['combined_russ_ndsi_IOU_diff'] = combined_russ_ndsi_diff\n",
    "\n",
    "        #within same location differences\n",
    "        sub['na_85_diff'] = na_85_diff\n",
    "        sub['na_ndsi_diff'] = na_ndsi_diff\n",
    "        sub['na_85_ndsi_diff'] = na_85_ndsi_diff\n",
    "\n",
    "        sub['combined_85_diff'] = combined_85_diff\n",
    "        sub['combined_ndsi_diff'] = combined_ndsi_diff\n",
    "        sub['combined_85_ndsi_diff'] = combined_85_ndsi_diff\n",
    "\n",
    "        sub['russ_85_diff'] = russ_85_diff\n",
    "        sub['russ_ndsi_diff'] = russ_ndsi_diff\n",
    "        sub['russ_85_ndsi_diff'] = russ_85_ndsi_diff\n",
    "\n",
    "        sub.to_file(os.path.join(out_path,  f\"{m}.shp\"))\n",
    "\n",
    "    print(\"Done Processing final shapefiles\")\n",
    "\n",
    "    \n",
    "#grid final shapefiles\n",
    "spatial_median(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results_t2/grid/shapefiles\",  \"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results_t2/grid/shapefiles_final\")\n",
    "\n",
    "#ecoregion final shapefiles\n",
    "spatial_median(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results_t2/ecoregion/shapefiles\",  \"/explore/nobackup/people/spotter5/cnn_mapping/Russia/cv_results_t2/ecoregion/shapefiles_final\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07beaf92-4d32-4a00-8769-a60d3a3846d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ec11ec-b302-4579-ab8e-32fa6e2df1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file(\"/explore/nobackup/people/spotter5/viirs_nrt/shared_data/shapefiles/ak_ca.shp\")\n",
    "\n",
    "gdf.to_file(\"/explore/nobackup/people/spotter5/viirs_nrt/shared_data/shapefiles/ak_ca.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229010a-fb76-4291-9ab9-812811f0c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
