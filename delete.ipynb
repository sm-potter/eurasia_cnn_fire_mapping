{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7f1a1e-f8bc-4cb5-84dd-eaedc5144382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/gee_ml/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/spotter5/.conda/envs/gee_ml/lib/python3.10/site-packages/ee/deprecation.py:204: DeprecationWarning: \n",
      "\n",
      "Attention required for MODIS/006/MOD10A1! You are using a deprecated asset.\n",
      "To ensure continued functionality, please update it.\n",
      "Learn more: https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD10A1\n",
      "\n",
      "  warnings.warn(warning, category=DeprecationWarning)\n",
      "/tmp/ipykernel_2027181/1800174688.py:164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  l5['band.or.si']=pd.Categorical(l5['band.or.si'],categories=bands)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 15570.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import ee\n",
    "import numpy as np\n",
    "from geeml.extract import extractor\n",
    "import pandas as pd\n",
    "import random\n",
    "# import geemap\n",
    "# Authenticate GEE\n",
    "# ee.Authenticate()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\"\n",
    "\n",
    "service_account = 'gee-serdp-upload@appspot.gserviceaccount.com'\n",
    "credentials = ee.ServiceAccountCredentials(service_account, \"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\")\n",
    "ee.Initialize(credentials)\n",
    "# Initialize GEE with high-volume end-point\n",
    "# ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "ee.Initialize()\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import geemap\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import client\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"gee-serdp-upload\"\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\"\n",
    "storage_client = storage.Client.from_service_account_json(\"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\")\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"gee-serdp-upload\"\n",
    "storage_client = storage.Client()\n",
    "# bucket_name = 'smp-scratch/mtbs_1985'\n",
    "bucket_name = 'smp-scratch'\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "\n",
    "# Import assetts of interest\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "geometry = ee.FeatureCollection('users/spotter/fire_cnn/raw/eurasia') #area of interest\n",
    "mod1 = ee.ImageCollection(\"MODIS/061/MOD10A1\") #active fire\n",
    "mod2 = ee.ImageCollection(\"MODIS/061/MYD10A1\") #active fire\n",
    "fire_cci = ee.ImageCollection(\"ESA/CCI/FireCCI/5_1\") #active fire\n",
    "mod_burn = ee.ImageCollection(\"MODIS/061/MCD64A1\") #mcd64a1\n",
    "snow = ee.ImageCollection('MODIS/006/MOD10A1') #modis snow cover\n",
    "# Load the MODIS water mask image and invert it.\n",
    "water_mask = ee.Image('MODIS/MOD44W/MOD44W_005_2000_02_24').select('water_mask').Not()\n",
    "\n",
    "\n",
    "sent_2A = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") #sentinel 2\n",
    "s2Clouds = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY') #cloud masking for sentinel\n",
    "# s2Clouds = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY') #cloud masking for sentinel\n",
    "# lfdb = ee.FeatureCollection(\"users/spotter/fire_cnn/raw/nbac_1985\") #nbac_fire_polygons, this can be any polygon shapefile, final version would be nbac and mtbs\n",
    "lfdb = ee.FeatureCollection(\"users/spotter/fire_cnn/ann_w_id\") #anna polygons \n",
    "\n",
    "# input_grid = ee.FeatureCollection('users/spotter/fire_cnn/raw/eurasia_dnbr_grid_clip') #grid to loop through to get around memory errors\n",
    "\n",
    "\n",
    "# Cloud masking Sentinel 2\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#probability of clouds\n",
    "MAX_CLOUD_PROBABILITY = 50\n",
    "\n",
    "def sent_maskcloud(image):\n",
    "\n",
    "\n",
    "    image = image.select(['B2', 'B3', 'B4', 'B8', 'B11', 'B12'], ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7'])# rename bands to match landsat\n",
    "\n",
    "    image =  image.toShort()\n",
    "\n",
    "    clouds = ee.Image(image.get('cloud_mask')).select('probability')\n",
    "\n",
    "    isNotCloud = clouds.lt(MAX_CLOUD_PROBABILITY)\n",
    "\n",
    "    image = image.updateMask(isNotCloud)\n",
    "\n",
    "    #reproject 30m but remember b1, b2 and b3 are 10 and the rest are 20\n",
    "    image1 = image.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4'])\n",
    "    image2 = image.select(['SR_B5', 'SR_B7'])\n",
    "\n",
    "\n",
    "    image1 = image1.reproject(\n",
    "    crs = image1.projection().crs(),\n",
    "    scale = 30) #resample for landsat\n",
    "\n",
    "\n",
    "    image2 = image2.reproject(\n",
    "    crs = image2.projection().crs(),\n",
    "    scale = 30) #resample for landsat\n",
    "\n",
    "    image = image1.addBands(image2)\n",
    "\n",
    "    return image\n",
    "\n",
    "#Join S2 SR with cloud probability dataset to add cloud mask.\n",
    "s2SrWithCloudMask = ee.Join.saveFirst('cloud_mask').apply(\n",
    "\n",
    "  primary=sent_2A,\n",
    "  secondary=s2Clouds,\n",
    "  condition=ee.Filter.equals(leftField='system:index', rightField='system:index'))\n",
    "\n",
    "#apply cloud masking\n",
    "sent_2A = ee.ImageCollection(s2SrWithCloudMask).map(sent_maskcloud)\n",
    "\n",
    "\n",
    "# Correct landsat scale factor and sentinel scale factor\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def mask(image):\n",
    "    qa = image.select('QA_PIXEL')\n",
    "    mask = qa.bitwiseAnd(8).eq(0).And(qa.bitwiseAnd(10).eq(0)).And(qa.bitwiseAnd(32).eq(0))\n",
    "    return(image.updateMask(mask))\n",
    "\n",
    "def land_scale(image):\n",
    "\n",
    "    return(image.multiply(0.0000275).add(-0.2))\n",
    "\n",
    "def sent_scale(image):\n",
    "    return(image.multiply(0.0001))\n",
    "\n",
    "\n",
    "# Logan Coefficients to apply\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "coeffs = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/raw_files/boreal_xcal_regression_coefficients.csv\").fillna(0)\n",
    "#l5\n",
    "def landsat_correct(sat, bands):\n",
    "\n",
    "    \"\"\"argument 1 is which sattelite, LANDASAT_5 or LANDSAT_8\n",
    "    argument 2 is bands of interest.  Bands must be in same order as EE,\n",
    "    \n",
    "    regression is of form,\n",
    "    L7 = B0 + (B1 * L5/8) + (B2 * L^2) + (B3 * L^3)\n",
    "    \"\"\"\n",
    "\n",
    "    #bands of interest in order of interest\n",
    "    l5 = coeffs[(coeffs['satellite'] == sat) & (coeffs['band.or.si'] .isin (bands))] \n",
    "\n",
    "    #arrange the band or si column\n",
    "    l5['band.or.si']=pd.Categorical(l5['band.or.si'],categories=bands)\n",
    "    l5=l5.sort_values('band.or.si')\n",
    "\n",
    "    b0 = l5['B0'].values.tolist()\n",
    "    b1 = l5['B1'].values.tolist()\n",
    "    b2 = l5['B2'].values.tolist()\n",
    "    b3 = l5['B3'].values.tolist()\n",
    "\n",
    "    return (b0, b1, b2, b3)\n",
    "\n",
    "#get the corrections, each output is a list at one of the four locations\n",
    "l8_corr = landsat_correct(sat = 'LANDSAT_8', bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'nbr', 'ndvi', 'ndii'])\n",
    "l5_corr = landsat_correct(sat = 'LANDSAT_5', bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'nbr', 'ndvi', 'ndii'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# I might need to convert to float\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def to_float(image):\n",
    "\n",
    "    b1 = image.select('SR_B1').cast({'SR_B1':'float'}) #0\n",
    "    b2 = image.select('SR_B2').cast({'SR_B2':'float'}) #1\n",
    "    b3 = image.select('SR_B3').cast({'SR_B3':'float'}) #2\n",
    "    b4 = image.select('SR_B4').cast({'SR_B4':'float'}) #3\n",
    "    b5 = image.select('SR_B5').cast({'SR_B5':'float'}) #4\n",
    "    b6 = image.select('SR_B7').cast({'SR_B7':'float'}) #5\n",
    "\n",
    "    image = b1.addBands(b2).addBands(b3).addBands(b4).addBands(b5).addBands(b6)\n",
    "\n",
    "    return image\n",
    "\n",
    "def filter_cl(image):\n",
    "    cl = image.select('ConfidenceLevel')\n",
    "    image = image.updateMask(cl.gte(50)).select('BurnDate')\n",
    "    return image\n",
    "\n",
    "def mod_act_map(image):\n",
    "    image = image.addBands(ee.Image(ee.Date(image.get('system:time_start')).getRelative('day','year').add(1)).clamp(1,366)).updateMask(image.select('FireMask').gte(7))\n",
    "    return image.updateMask(image.select('constant').gt(60));\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"gee-serdp-upload\"\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\"\n",
    "storage_client = storage.Client.from_service_account_json(\"/explore/nobackup/people/spotter5/cnn_mapping/gee-serdp-upload-7cd81da3dc69.json\")\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"gee-serdp-upload\"\n",
    "storage_client = storage.Client()\n",
    "# bucket_name = 'smp-scratch/mtbs_1985'\n",
    "bucket_name = 'smp-scratch'\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "\n",
    "# Function to return the pre_fire and post_fire landsat and sentinel 2 data\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def get_pre_post(pre_start, pre_end, post_start, post_end, geometry):\n",
    "\n",
    "    \"\"\"parameters are:\n",
    "    pre_start: the start date for pre fire imagery\n",
    "    pre_end: the end date for pre fire imagery\n",
    "    post_start: the start date for post fire imagery\n",
    "    post_end: the end date for post_fire imagery\n",
    "    geometry: the geometry to filter by\n",
    "    \"\"\"\n",
    "\n",
    "    #landsat 5\n",
    "    lt5 = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2').filterDate(pre_start, post_end).filterBounds(geometry)\n",
    "    #landsat 7\n",
    "    le7 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2').filterDate(pre_start, post_end).filterBounds(geometry)\n",
    "    #landsat 8\n",
    "    lc8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').filterDate(pre_start, post_end).filterBounds(geometry)\n",
    "    #sentinel 2\n",
    "    sent = sent_2A.filterDate(pre_start, post_end).filterBounds(geometry)\n",
    "\n",
    "    #select bands\n",
    "    pre_lt5 = lt5.filterDate(pre_start, pre_end).map(mask).map(land_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if pre_lt5.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        pre_lt5 = pre_lt5.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        pre_lt5_nbr = pre_lt5.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        pre_lt5_ndvi = pre_lt5.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        pre_lt5_ndii = pre_lt5.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        pre_lt5 = pre_lt5.addBands(pre_lt5_nbr).addBands(pre_lt5_ndvi).addBands(pre_lt5_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        l5_pre_corr = pre_lt5.multiply(l5_corr[1]).add(pre_lt5.pow(2).multiply(l5_corr[2])).add(pre_lt5.pow(3).multiply(l5_corr[3])).add(l5_corr[0])\n",
    "\n",
    "    #-------now do post-fire\n",
    "    #select bands\n",
    "    post_lt5 = lt5.filterDate(post_start, post_end).map(mask).map(land_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if post_lt5.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        post_lt5 = post_lt5.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        post_lt5_nbr = post_lt5.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        post_lt5_ndvi = post_lt5.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        post_lt5_ndii = post_lt5.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        post_lt5 = post_lt5.addBands(post_lt5_nbr).addBands(post_lt5_ndvi).addBands(post_lt5_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        l5_post_corr = post_lt5.multiply(l5_corr[1]).add(post_lt5.pow(2).multiply(l5_corr[2])).add(post_lt5.pow(3).multiply(l5_corr[3])).add(l5_corr[0])\n",
    "\n",
    "\n",
    "      #         #------------------------------------------Landsat 7, no corrections but get things clipped and do pre fire/post_fire stuff\n",
    "\n",
    "\n",
    "    #select bands\n",
    "    pre_le7 = le7.filterDate(pre_start, pre_end).map(mask).map(land_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if pre_le7.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        pre_le7 = pre_le7.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        pre_le7_nbr = pre_le7.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        pre_le7_ndvi = pre_le7.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        pre_le7_ndii = pre_le7.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        pre_le72 = pre_le7.addBands(pre_le7_nbr).addBands(pre_le7_ndvi).addBands(pre_le7_ndii)\n",
    "\n",
    "    #-------now do post-fire\n",
    "    #select bands\n",
    "    post_le7 = le7.filterDate(post_start, post_end).map(mask).map(land_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if post_le7.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        post_le7 = post_le7.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        post_le7_nbr = post_le7.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        post_le7_ndvi = post_le7.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        post_le7_ndii = post_le7.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        post_le72 = post_le7.addBands(post_le7_nbr).addBands(post_le7_ndvi).addBands(post_le7_ndii)\n",
    "\n",
    "    #------------------------------------------Landsat 8 corrections\n",
    "\n",
    "\n",
    "    #-------first do pre-fire\n",
    "\n",
    "    #select bands\n",
    "    pre_lc8 = lc8.filterDate(pre_start, pre_end).map(mask).map(land_scale).select(['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7'],['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']) .map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if pre_lc8.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        pre_lc8 = pre_lc8.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        pre_lc8_nbr = pre_lc8.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        pre_lc8_ndvi = pre_lc8.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        pre_lc8_ndii = pre_lc8.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        pre_lc8 = pre_lc8.addBands(pre_lc8_nbr).addBands(pre_lc8_ndvi).addBands(pre_lc8_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        l8_pre_corr = pre_lc8.multiply(l8_corr[1]).add(pre_lc8.pow(2).multiply(l8_corr[2])).add(pre_lc8.pow(3).multiply(l8_corr[3])).add(l8_corr[0])\n",
    "\n",
    "    #-------now do post-fire\n",
    "    #select bands\n",
    "    post_lc8 = lc8.filterDate(post_start, post_end).map(mask).map(land_scale).select(['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7'],['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']) .map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if post_lc8.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        post_lc8 = post_lc8.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        post_lc8_nbr = post_lc8.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        post_lc8_ndvi = post_lc8.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        post_lc8_ndii = post_lc8.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        post_lc8 = post_lc8.addBands(post_lc8_nbr).addBands(post_lc8_ndvi).addBands(post_lc8_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        l8_post_corr = post_lc8.multiply(l8_corr[1]).add(post_lc8.pow(2).multiply(l8_corr[2])).add(post_lc8.pow(3).multiply(l8_corr[3])).add(l8_corr[0])\n",
    "\n",
    "        # #          #------------------------------------------Sentinel 2 corrections, use landsat 8 coefficients\n",
    "\n",
    "\n",
    "    ##-------first do pre-fire\n",
    "\n",
    "    #select bands\n",
    "    pre_sent = sent_2A.filterDate(pre_start, pre_end).map(sent_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if pre_sent.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "          #take the median\n",
    "        pre_sent = pre_sent.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        pre_sent_nbr = pre_sent.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        pre_sent_ndvi = pre_sent.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        pre_sent_ndii = pre_sent.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        pre_sent = pre_sent.addBands(pre_sent_nbr).addBands(pre_sent_ndvi).addBands(pre_sent_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        sent_pre_corr = pre_sent.multiply(l8_corr[1]).add(pre_sent.pow(2).multiply(l8_corr[2])).add(pre_sent.pow(3).multiply(l8_corr[3])).add(l8_corr[0])\n",
    "\n",
    "    #-------now do post-fire\n",
    "    #select bands\n",
    "    post_sent = sent_2A.filterDate(post_start, post_end).map(sent_scale).select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']).map(to_float)\n",
    "\n",
    "    #       #ensure we have imagery for the sensor\n",
    "    if post_sent.size().getInfo() > 0 :\n",
    "\n",
    "\n",
    "\n",
    "        #take the median\n",
    "        post_sent = post_sent.median().clip(geometry)\n",
    "\n",
    "        #calculate nbr, ndvi and ndii\n",
    "        post_sent_nbr = post_sent.normalizedDifference(['SR_B4', 'SR_B7']).select([0], ['NBR']).cast({'NBR': 'float'})\n",
    "        post_sent_ndvi = post_sent.normalizedDifference(['SR_B4', 'SR_B3']).select([0], ['NDVI']).cast({'NDVI': 'float'})\n",
    "        post_sent_ndii = post_sent.normalizedDifference(['SR_B4', 'SR_B5']).select([0], ['NDII']).cast({'NDII': 'float'})\n",
    "\n",
    "        #add the bands back\n",
    "        post_sent = post_sent.addBands(post_sent_nbr).addBands(post_sent_ndvi).addBands(post_sent_ndii)\n",
    "\n",
    "        #apply the corrections\n",
    "\n",
    "        sent_post_corr = post_sent.multiply(l8_corr[1]).add(post_sent.pow(2).multiply(l8_corr[2])).add(post_sent.pow(3).multiply(l8_corr[3])).add(l8_corr[0])\n",
    "\n",
    "\n",
    "    #try to see if image exists, if so append\n",
    "\n",
    "    #----------------------all prefire\n",
    "\n",
    "    #       #empty list for pre-fire, use this to combine if we have land 5, 7, 8 or sentinel\n",
    "    pre_input = []\n",
    "\n",
    "    try:\n",
    "        l5_pre_corr.getInfo()\n",
    "        pre_input.append(l5_pre_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        pre_le72.getInfo()\n",
    "        pre_input.append(pre_le72)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        l8_pre_corr.getInfo()\n",
    "        pre_input.append(l8_pre_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        sent_pre_corr.getInfo()\n",
    "        pre_input.append(sent_pre_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    #----------------------all postfire\n",
    "\n",
    "    #         #       #empty list for post-fire, use this to combine if we have land 5, 7, 8 or sentinel\n",
    "    post_input = []\n",
    "\n",
    "    try:\n",
    "        l5_post_corr.getInfo()\n",
    "        post_input.append(l5_post_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        post_le72.getInfo()\n",
    "        post_input.append(post_le72)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        l8_post_corr.getInfo()\n",
    "        post_input.append(l8_post_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        sent_post_corr.getInfo()\n",
    "        post_input.append(sent_post_corr)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #return the two lists of pre input and post input\n",
    "    return pre_input, post_input\n",
    "\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "#masking snow cover functions\n",
    "#reference https://developers.google.com/earth-engine/tutorials/community/identifying-first-day-no-snow\n",
    "def mask_snow(img):\n",
    "    return img.gte(10)\n",
    "\n",
    "def add_date_bands(img):\n",
    "    global start_date, start_year_var\n",
    "    date = img.date()\n",
    "    cal_doy = date.getRelative('day', 'year')\n",
    "    rel_doy = date.difference(start_date, 'day')\n",
    "    millis = date.millis()\n",
    "    date_bands = ee.Image.constant([cal_doy, rel_doy, millis, start_year_var]).rename(['calDoy', 'relDoy', 'millis', 'year'])\n",
    "    return img.addBands(date_bands).cast({'calDoy': 'int', 'relDoy': 'int', 'millis': 'long', 'year': 'int'}).set('millis', millis)\n",
    "\n",
    "\n",
    "def process_year(year):\n",
    "    global start_doy, start_date, start_year_var\n",
    "    start_year_var = year\n",
    "    first_doy = ee.Date.fromYMD(year, 1, 1)\n",
    "    start_date = first_doy.advance(start_doy - 1, 'day')\n",
    "    end_date = start_date.advance(1, 'year').advance(1, 'day')\n",
    "    year_col = snow.filterDate(start_date, end_date)\n",
    "    # no_snow_img = year_col.map(add_date_bands).sort('millis').reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).updateMask(analysis_mask).set('year', year)\n",
    "    no_snow_img = year_col.map(add_date_bands).sort('millis').reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).set('year', year)\n",
    "\n",
    "    return no_snow_img.updateMask(no_snow_img.select('snowCover').eq(0))\n",
    "\n",
    "\n",
    "def process_year_fall(year):\n",
    "    global start_doy, start_date, start_year_var\n",
    "    start_year_var = year\n",
    "    first_doy = ee.Date.fromYMD(year, 1, 1)\n",
    "    start_date = first_doy.advance(start_doy - 1, 'day')\n",
    "    end_date = start_date.advance(1, 'year').advance(1, 'day')\n",
    "    year_col = snow.filterDate(start_date, end_date)\n",
    "    # no_snow_img = year_col.map(add_date_bands).sort('millis').reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).updateMask(analysis_mask).set('year', year)\n",
    "    no_snow_img = year_col.map(add_date_bands).sort('millis', False).reduce(ee.Reducer.min(5)).rename(['snowCover', 'calDoy', 'relDoy', 'millis', 'year']).set('year', year)\n",
    "\n",
    "    return no_snow_img.updateMask(no_snow_img.select('snowCover').eq(0))\n",
    "\n",
    "\n",
    "#good and best snow quality flags\n",
    "# Define a function to mask out poor quality snow cover data based on the NDSI_Snow_Cover_Basic_QA band\n",
    "def mask_quality_snow(image):\n",
    "    # Select the NDSI_Snow_Cover_Basic_QA band\n",
    "    qa = image.select('NDSI_Snow_Cover_Basic_QA')\n",
    "    \n",
    "    # Create a mask for good and best quality (bit 0 and 1 set to 0)\n",
    "    quality_mask = qa.bitwiseAnd(3).eq(0)\n",
    "    \n",
    "    # Update the image with the quality mask\n",
    "    return image.updateMask(quality_mask)\n",
    "\n",
    "snow = snow.map(mask_quality_snow).select('NDSI_Snow_Cover')\n",
    "\n",
    "def add_lat_long(img):\n",
    "    lat = ee.Image.pixelLonLat().select('latitude').rename('lat')\n",
    "    lon = ee.Image.pixelLonLat().select('longitude').rename('lon')\n",
    "    return img.addBands([lat, lon])\n",
    "\n",
    "\n",
    "# Placeholder from chat gpt\n",
    "\n",
    "# Now loop through each fire id, and for each pixel prefire and during fire I will find the last day of year of snow cover.  This will result in 2 images.  I will then take\n",
    "# the maximum value of these two images and use that to start my composites.  For instance if a pixel has a value of 120 (April 1st) and 130 (April 10th) I will use 130 for that pixel.  If the fire \n",
    "# occured in 2019 I will then use prefire as 2019-131 to 2019-161 and postfire as 2020-131 to 2019-161.  This will require looping through individual pixels and re-aggregating the data. \n",
    "# \n",
    "# In addition to this I will continue to use my old compositing method which is 1 year summer pre and one year summer post.  I will than take the max value through the snow-pixel based method and the old method. \n",
    "# \n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "#get all the ids within the lfdb shapefile\n",
    "all_ids = ee.List(lfdb.distinct([\"ID\"]).aggregate_array(\"ID\"))\n",
    "all_ids = all_ids.getInfo()\n",
    "\n",
    "all_ids = [15570]\n",
    "\n",
    "# all_ids = [15193]\n",
    "# all_ids = [1, 15193]\n",
    "\n",
    "\n",
    "pre_months = ['-06-01']\n",
    "end_months = ['-08-31']\n",
    "\n",
    "all_months = dict(zip(pre_months, end_months))\n",
    "\n",
    "\n",
    "out_path = '/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_ndsi_csvs'\n",
    "os.makedirs(out_path, exist_ok = True)\n",
    "\n",
    "# all_ids = [1]\n",
    "#loop through each fire polygon\n",
    "for i in all_ids:\n",
    "    \n",
    "\n",
    "\n",
    "    #name of output file\n",
    "    fname = f\"{i}.csv\"\n",
    "\n",
    "    #check if file exists on my bucket, if it does skip\n",
    "    \n",
    "    if not os.path.exists(os.path.join(out_path, fname)):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            #get the fire polygon of interest\n",
    "            sub_shape = lfdb.filter(ee.Filter.eq(\"ID\", i))\n",
    "\n",
    "            #get all other fire ids that are not this one\n",
    "            not_fires = lfdb.filter(ee.Filter.neq(\"ID\", i))\n",
    "\n",
    "\n",
    "            #first get the bounding box of the fire\n",
    "            bbox = sub_shape.geometry().bounds()\n",
    "\n",
    "\n",
    "            #offset the bounding box by a random number\n",
    "            # all_rands = [0.00, 0.02, -0.02]\n",
    "            all_rands = [0.00]\n",
    "\n",
    "\n",
    "            rand1 = random.sample(all_rands, 1)[0]\n",
    "            rand2 = random.sample(all_rands, 1)[0]\n",
    "\n",
    "            #offset applied\n",
    "            proj = ee.Projection(\"EPSG:4326\").translate(rand1, rand2)\n",
    "\n",
    "            #for the bounding box apply the randomly selected offset\n",
    "            final_buffer = ee.Geometry.Polygon(bbox.coordinates(), proj).transform(proj)\n",
    "\n",
    "            #this is a bit of a hack but we have two different bounding box sizes because when we export we need to use some additonal area to avoid cuttoffs\n",
    "            final_buffer2 = final_buffer.buffer(distance= 5000).bounds()\n",
    "\n",
    "            final_buffer = final_buffer.buffer(distance= 40000)#.bounds().transform(proj='EPSG:3413', maxError=1)\n",
    "            # final_buffer = final_buffer.buffer(distance= 10000)#.bounds().transform(proj='EPSG:3413', maxError=1)\n",
    "\n",
    "\n",
    "            #get the year of this fire\n",
    "            this_year = ee.Number(sub_shape.aggregate_array('Year').get(0))\n",
    "\n",
    "            year = this_year.getInfo() \n",
    "\n",
    "            pre_start = ee.Date.fromYMD(this_year.subtract(1), 6, 1)\n",
    "            pre_end = ee.Date.fromYMD(this_year.subtract(1), 8, 31)\n",
    "            post_start = pre_start.advance(2, 'year')\n",
    "            post_end = pre_end.advance(2, 'year')\n",
    "\n",
    "            #just getting some date info here to ensure pre fire is one  year before and post fire is one year after the fire year of interest\n",
    "            startYear = pre_start.get('year')\n",
    "\n",
    "            #convert to client side\n",
    "            startYear = startYear.getInfo()  # local string\n",
    "            endYear = str(int(startYear) + 2)\n",
    "            startYear = str(startYear)\n",
    "\n",
    "\n",
    "            #loop through all the months and use 85th percentile to download all data\n",
    "            all_months_images = []\n",
    "\n",
    "             #loop through all months \n",
    "            for m1, m2 in all_months.items():\n",
    "\n",
    "                if m1 == '-06-01' and m2 == '-08-31':\n",
    "\n",
    "                    start_year = year - 1\n",
    "                    end_year = year + 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    start_year = year - 1\n",
    "                    end_year = year\n",
    "\n",
    "\n",
    "\n",
    "                #get pre dates\n",
    "                pre_start = str(start_year) + m1\n",
    "                pre_end = str(start_year) + m2\n",
    "\n",
    "                #get post dates\n",
    "\n",
    "\n",
    "                post_start = str(end_year) + m1\n",
    "                post_end = str(end_year) + m2\n",
    "\n",
    "\n",
    "\n",
    "                #apply the function to get the pre_fire image and post_fire image\n",
    "                all_imagery = get_pre_post(pre_start, pre_end, post_start, post_end, final_buffer)\n",
    "\n",
    "                #return the pre and post fire input imagery lists\n",
    "                pre_input = all_imagery[0]\n",
    "                post_input = all_imagery[1]\n",
    "\n",
    "                #if the lists each are larger than 1 we have imagery\n",
    "                if (len(pre_input) >0) and (len(post_input) > 0):\n",
    "\n",
    "                    #take the median of the image collections\n",
    "                    pre_input = ee.ImageCollection(pre_input)\n",
    "                    post_input = ee.ImageCollection(post_input)\n",
    "\n",
    "                    #get median of images\n",
    "                    pre_input = pre_input.median()\n",
    "                    post_input= post_input.median()\n",
    "\n",
    "                    #difference the bands\n",
    "                    raw_bands = pre_input.subtract(post_input).multiply(1000)\n",
    "\n",
    "                    b1 = raw_bands.select('SR_B1').cast({'SR_B1':'short'})\n",
    "                    b2 = raw_bands.select('SR_B2').cast({'SR_B2':'short'})\n",
    "                    b3 = raw_bands.select('SR_B3').cast({'SR_B3':'short'})\n",
    "                    b4 = raw_bands.select('SR_B4').cast({'SR_B4':'short'})\n",
    "                    b5 = raw_bands.select('SR_B5').cast({'SR_B5':'short'})\n",
    "                    b6 = raw_bands.select('SR_B7').cast({'SR_B7':'short'})\n",
    "                    b7 = raw_bands.select('NBR').cast({'NBR':'short'})\n",
    "                    b8 = raw_bands.select('NDVI').cast({'NDVI':'short'})\n",
    "                    b9 = raw_bands.select('NDII').cast({'NDII':'short'})\n",
    "\n",
    "                    #if using all bands\n",
    "                    raw_bands = b7.addBands(b8).addBands(b9)\n",
    "\n",
    "\n",
    "                    raw_bands = raw_bands.clip(final_buffer)\n",
    "\n",
    "                    #we need to see which image ids from the entire lfdb are already included in the buffer\n",
    "                    lfdb_filtered_orig = lfdb.filterBounds(final_buffer)\n",
    "\n",
    "                    #ensure all fires are within the actual year of interest (this_year) and two years prior, otherwise ignore, this is to ensure we don't have nearby fires from previous years\n",
    "                    first_year =  int(startYear) + 1\n",
    "                    second_year =  int(startYear)\n",
    "                    third_year =  int(startYear) - 1\n",
    "                    fourth_year = int(startYear) + 2\n",
    "\n",
    "                    lfdb_filtered = lfdb_filtered_orig.filter(ee.Filter.eq(\"Year\", year))\n",
    "\n",
    "                    bad_filtered = lfdb_filtered_orig.filter(ee.Filter.Or(ee.Filter.eq(\"Year\", second_year), ee.Filter.eq(\"Year\", third_year), ee.Filter.eq(\"Year\", fourth_year)))\n",
    "\n",
    "\n",
    "                    #get ids which are in image\n",
    "                    all_ids_new = ee.List(lfdb_filtered.distinct([\"ID\"]).aggregate_array(\"ID\")).getInfo()\n",
    "\n",
    "\n",
    "                    #remove ids from all dates which we do not need anymore\n",
    "                    all_ids2 = [i for i in all_ids if i not in all_ids_new]\n",
    "\n",
    "                    #area we have good fires\n",
    "                    fire_rast = lfdb_filtered.reduceToImage(properties= ['ID'], reducer = ee.Reducer.first())\n",
    "\n",
    "                    #areas we have fires from other years or nearby we don't want to use\n",
    "                    bad_fire_rast = bad_filtered.reduceToImage(properties= ['ID'], reducer = ee.Reducer.first())\n",
    "\n",
    "                    #change values to 1 for fire of interest\n",
    "                    fire_rast = fire_rast.where(fire_rast.gt(0), 1)\n",
    "\n",
    "                    #change values for bad fire raster to 1 as well\n",
    "                    bad_fire_rast = bad_fire_rast.where(bad_fire_rast.gt(0), 1)\n",
    "\n",
    "                    #if the fires overlap we want to keep those locations\n",
    "                    bad_fire_rast = bad_fire_rast.where(bad_fire_rast.eq(1).And(fire_rast.eq(1)), 2).unmask(-999)\n",
    "\n",
    "                    #rename to y for the fire raster\n",
    "                    fire_rast = fire_rast.rename(['y'])\n",
    "\n",
    "                    #copy the first values of raw_bands\n",
    "                    y = raw_bands.select(['NBR'], ['y'])\n",
    "\n",
    "                    #turn all values of y to 0\n",
    "                    y  = y.where(y.gt(-10000), 0)\n",
    "\n",
    "                    #turn values to 1 where fire_rast is 1\n",
    "                    y  = y.where(fire_rast.eq(1), 1)\n",
    "\n",
    "                    b10 = y.select('y').cast({'y':'short'})\n",
    "\n",
    "                    #add another band which is day of year, this value is 999\n",
    "                    old_composite_day = ee.Image.constant(999).rename('day')\n",
    "\n",
    "                    # raw_bands = raw_bands.addBands(old_composite_day)\n",
    "\n",
    "                    #combine all the bands for predictors\n",
    "                    raw_bands = raw_bands.updateMask(bad_fire_rast.neq(1))\n",
    "\n",
    "                    #add in the target variable\n",
    "                    # raw_bands = raw_bands.addBands(b10)\n",
    "\n",
    "    #                 #-------------------------------------------------------------------------now composite based on snow cover, which will require looping through individual pixels\n",
    "                    start_doy = 1\n",
    "                    start_year_snow = year - 1\n",
    "                    end_year_snow = year\n",
    "\n",
    "                    #all years we are interested in for snow\n",
    "                    snow_years = ee.List.sequence(start_year_snow, end_year_snow + 1)\n",
    "\n",
    "                    annual_list = snow_years.map(process_year) #first day no snow\n",
    "                    annual_list_fall = snow_years.map(process_year_fall) #first day snow\n",
    "\n",
    "                    annual_col = ee.ImageCollection.fromImages(annual_list)\n",
    "                    annual_col_fall = ee.ImageCollection.fromImages(annual_list_fall)\n",
    "\n",
    "\n",
    "                    #get the images with the last day of snow in the start year and end year\n",
    "                    # Subset the year of interest.\n",
    "                    start_year_first_day_no_snow = annual_col.filter(ee.Filter.eq('year', start_year)).first().select('calDoy').clip(final_buffer)\n",
    "                    end_year_first_day_no_snow = annual_col.filter(ee.Filter.eq('year', end_year)).first().select('calDoy').clip(final_buffer)\n",
    "\n",
    "                    start_year_first_day_no_snow_fall = annual_col_fall.filter(ee.Filter.eq('year', start_year)).first().select('calDoy').clip(final_buffer)\n",
    "                    end_year_first_day_no_snow_fall = annual_col_fall.filter(ee.Filter.eq('year', end_year)).first().select('calDoy').clip(final_buffer)\n",
    "\n",
    "\n",
    "                    #get the max day across both \n",
    "                    max_snow_day = ee.ImageCollection([start_year_first_day_no_snow, end_year_first_day_no_snow]).max()\n",
    "\n",
    "                    #get the minimum for fall\n",
    "                    max_snow_day_fall = ee.ImageCollection([start_year_first_day_no_snow_fall, end_year_first_day_no_snow_fall]).min()\n",
    "\n",
    "                    #Use reduceRegion to find the maximum value within the region\n",
    "                    max_value = max_snow_day.reduceRegion(\n",
    "                        reducer=ee.Reducer.max(),\n",
    "                        geometry=final_buffer2,\n",
    "                        scale=500,\n",
    "                        maxPixels=1e13\n",
    "                    ).getInfo()['calDoy']\n",
    "\n",
    "                    start_day = max_value + 7\n",
    "\n",
    "                    #Use reduceRegion to find the maximum value within the region\n",
    "                    min_value = max_snow_day_fall.reduceRegion(\n",
    "                        reducer=ee.Reducer.min(),\n",
    "                        geometry=final_buffer2,\n",
    "                        scale=500,\n",
    "                        maxPixels=1e13\n",
    "                    ).getInfo()['calDoy']\n",
    "\n",
    "                    end_day = min_value - 7\n",
    "\n",
    "\n",
    "    #                 #get all the unique days\n",
    "    #                  # #get all unique burn days from final\n",
    "    #                 freqHist = ee.Dictionary(max_snow_day.reduceRegion(reducer = ee.Reducer.frequencyHistogram().unweighted(), geometry = final_buffer2, scale = 500, maxPixels = 1e13).get('calDoy'))\n",
    "    #                 freqHist_fall = ee.Dictionary(max_snow_day_fall.reduceRegion(reducer = ee.Reducer.frequencyHistogram().unweighted(), geometry = final_buffer2, scale = 500, maxPixels = 1e13).get('calDoy'))\n",
    "\n",
    "    #                 #get all unique burned days\n",
    "    #                 all_days = freqHist.keys().getInfo()\n",
    "\n",
    "    #                 all_days_fall = freqHist_fall.keys().getInfo()\n",
    "\n",
    "                    # print('Done')\n",
    "\n",
    "\n",
    "    # #                 all_days = ['162']\n",
    "\n",
    "    #                 # print(all_days)\n",
    "\n",
    "    #                 #keep track of which days are done already\n",
    "                    all_days_done = []\n",
    "\n",
    "    #                 # #now we need to loop through each one of these dates and get the VI's for each one\n",
    "                    all_days_vi = []\n",
    "\n",
    "    #                 for day in all_days:\n",
    "\n",
    "    #                     #keep track of in this day which value is being selected\n",
    "    #                     # this_day_for_select = []\n",
    "\n",
    "    #                     # this_day_for_vi = []\n",
    "\n",
    "    #                     if day != 'null':\n",
    "\n",
    "    #                         # print(f\"This day is {day}\")\n",
    "\n",
    "    #                         #generate two lists, which will be the monthly intervals we will loop through given this day, we will never surpass 244 as we don't want to go past september for this\n",
    "    #                         # Initialize the starting value and the maximum limit\n",
    "    #                         start_value = int(day) + 1\n",
    "    #                         max_value = 244\n",
    "    #                         increment = 30\n",
    "\n",
    "    #                         #never go past september 1 which is 244 - 31, to ensure we have ability to composite\n",
    "    #                         if int(day) < (244 - 31):\n",
    "\n",
    "                    current_day = start_day\n",
    "                    increment = 30\n",
    "\n",
    "                    list1 = []\n",
    "                    list2 = []\n",
    "\n",
    "                    while current_day < end_day:\n",
    "                        next_day = min(current_day + increment, end_day)\n",
    "                        list1.append(current_day)\n",
    "                        list2.append(next_day)\n",
    "                        current_day = next_day\n",
    "\n",
    "    #                 print(\"list1 =\", list1)\n",
    "    #                 print(\"list2 =\", list2)\n",
    "\n",
    "                    # Print the results\n",
    "                    # print(\"list1 =\", list1)\n",
    "                    # print(\"list2 =\", list2)\n",
    "\n",
    "                    #iterate through zipped lists so we can get all necessary composites\n",
    "                    all_days_for_composite = dict(zip(list1, list2))\n",
    "\n",
    "                    #loop through all days\n",
    "                    for d1, d2 in all_days_for_composite.items():\n",
    "\n",
    "                        # print(d1, d2)\n",
    "\n",
    "                        #ensure the month is less than September as I don't want these pixels\n",
    "                        # this_date = ee.Date.fromYMD(year, 1, 1).advance(int(day) -1 , 'day')\n",
    "\n",
    "                        # if this_date.get('month').getInfo() < 9:\n",
    "\n",
    "                        #mask out all other days but this one in max_snow_day\n",
    "                        # this_max_snow_day = max_snow_day.updateMask(max_snow_day.eq(int(day)))\n",
    "\n",
    "                        #we want to get 30 day intervals so long as the last one doesnt suprass September, and save each one of these\n",
    "\n",
    "\n",
    "                        #get the dates for the prefire and post fire composites \n",
    "                        #convert the year and day of interest to yyyy/mm/dd\n",
    "                        this_date_pre_start = ee.Date.fromYMD(start_year_snow, 1, 1).advance(int(d1) +0 , 'day')\n",
    "                        this_date_pre_end = ee.Date.fromYMD(start_year_snow, 1, 1).advance(int(d2) +0 , 'day')\n",
    "\n",
    "\n",
    "                        #post fire dates\n",
    "                        this_date_post_start = ee.Date.fromYMD(end_year_snow, 1, 1).advance(int(d1) +0 , 'day')\n",
    "                        this_date_post_end = ee.Date.fromYMD(end_year_snow, 1, 1).advance(int(d2) +0 , 'day')\n",
    "\n",
    "\n",
    "                        #pre dates for filtering\n",
    "                        snow_pre_year = this_date_pre_start.get('year').getInfo()\n",
    "                        snow_pre_month_start = this_date_pre_start.get('month').getInfo()\n",
    "                        snow_pre_day_start = this_date_pre_start.get('day').getInfo()\n",
    "\n",
    "                        snow_pre_month_end = this_date_pre_end.get('month').getInfo()\n",
    "                        snow_pre_day_end = this_date_pre_end.get('day').getInfo()\n",
    "\n",
    "                        pre_start_snow = f\"{snow_pre_year}-{snow_pre_month_start}-{snow_pre_day_start}\"\n",
    "                        pre_end_snow = f\"{snow_pre_year}-{snow_pre_month_end}-{snow_pre_day_end}\"\n",
    "\n",
    "                        # print(f\"Pre start is {pre_start_snow}\")\n",
    "                        # print(f\"Pre end is {pre_end_snow}\")\n",
    "\n",
    "                        #post dates for filtering\n",
    "                        snow_post_year = this_date_post_start.get('year').getInfo()\n",
    "                        snow_post_month_start = this_date_post_start.get('month').getInfo()\n",
    "                        snow_post_day_start = this_date_post_start.get('day').getInfo()\n",
    "\n",
    "                        snow_post_month_end = this_date_post_end.get('month').getInfo()\n",
    "                        snow_post_day_end = this_date_post_end.get('day').getInfo()\n",
    "\n",
    "                        post_start_snow = f\"{snow_post_year}-{snow_post_month_start}-{snow_post_day_start}\"\n",
    "                        post_end_snow = f\"{snow_post_year}-{snow_post_month_end}-{snow_post_day_end}\"\n",
    "\n",
    "    #                             print(f\"Post start is {post_start_snow}\")\n",
    "    #                             print(f\"Post end is {post_end_snow}\")\n",
    "\n",
    "\n",
    "                        #geet the imagery\n",
    "                        all_imagery_snow = get_pre_post(pre_start_snow, pre_end_snow, post_start_snow, post_end_snow, final_buffer)\n",
    "\n",
    "                        #return the pre and post fire input imagery lists\n",
    "                        pre_input_snow = all_imagery_snow[0]\n",
    "                        post_input_snow = all_imagery_snow[1]\n",
    "\n",
    "                        #if the lists each are larger than 1 we have imagery\n",
    "                        if (len(pre_input_snow) >0) and (len(post_input_snow) > 0):\n",
    "\n",
    "                            #take the median of the image collections\n",
    "                            pre_input_snow = ee.ImageCollection(pre_input_snow)\n",
    "                            post_input_snow = ee.ImageCollection(post_input_snow)\n",
    "\n",
    "                            #get median of images\n",
    "                            pre_input_snow = pre_input_snow.median()\n",
    "                            post_input_snow= post_input_snow.median()\n",
    "\n",
    "                            #difference the bands\n",
    "                            raw_bands_snow = pre_input_snow.subtract(post_input_snow).multiply(1000)\n",
    "\n",
    "                            b1_snow = raw_bands_snow.select('SR_B1').cast({'SR_B1':'short'})\n",
    "                            b2_snow = raw_bands_snow.select('SR_B2').cast({'SR_B2':'short'})\n",
    "                            b3_snow = raw_bands_snow.select('SR_B3').cast({'SR_B3':'short'})\n",
    "                            b4_snow = raw_bands_snow.select('SR_B4').cast({'SR_B4':'short'})\n",
    "                            b5_snow = raw_bands_snow.select('SR_B5').cast({'SR_B5':'short'})\n",
    "                            b6_snow = raw_bands_snow.select('SR_B7').cast({'SR_B7':'short'})\n",
    "                            b7_snow = raw_bands_snow.select('NBR').cast({'NBR':'short'})\n",
    "                            b8_snow = raw_bands_snow.select('NDVI').cast({'NDVI':'short'})\n",
    "                            b9_snow = raw_bands_snow.select('NDII').cast({'NDII':'short'})\n",
    "\n",
    "                            #make an image of all value of day\n",
    "                            # day_band = ee.Image.constant(int(day)).rename('day')\n",
    "                            #combine all the bands\n",
    "                            # raw_bands = b1.addBands(b2).addBands(b3).addBands(b4).addBands(b5).addBands(b6).addBands(b7).addBands(b8).addBands(b9)\n",
    "\n",
    "                            #select bands of interest, and mask to the fire only areas for this tile and this date\n",
    "                            # raw_bands_snow = b7_snow.addBands(b8_snow).addBands(b9_snow).addBands(day_band)#.updateMask(final_sub)\n",
    "                            raw_bands_snow = b7_snow.addBands(b8_snow).addBands(b9_snow)#.updateMask(final_sub)\n",
    "\n",
    "\n",
    "                            #apply the mask\n",
    "                            # raw_bands_snow = raw_bands_snow.updateMask(this_max_snow_day)\n",
    "\n",
    "                            #append to all_days vi\n",
    "                            # this_day_for_vi.append(raw_bands_snow.select('NBR'))\n",
    "                            # this_day_for_select.append(d1)\n",
    "\n",
    "                            all_days_vi.append(raw_bands_snow)\n",
    "                            all_days_done.append(d1)\n",
    "\n",
    "\n",
    "                # print(len(this_day_for_select))\n",
    "            #append the old composite method to aLl_days_vi\n",
    "            all_days_vi.append(raw_bands)\n",
    "\n",
    "            #999 is for old compositing method\n",
    "            all_days_done.append(999)\n",
    "            \n",
    "            #first get the NBR values for all the monthly composites\n",
    "            dnbr_arrays = []\n",
    "\n",
    "            for index, image in enumerate(all_days_vi):\n",
    "\n",
    "                this_loop_day = str(all_days_done[index])\n",
    "\n",
    "                # Specify the scale in meters\n",
    "                scale = 30\n",
    "\n",
    "                # Use reduceRegion to get the pixel values within the ROI\n",
    "                data = image.unmask(-999).reduceRegion(\n",
    "                    reducer=ee.Reducer.toList(),\n",
    "                    geometry=final_buffer2,\n",
    "                    scale=scale,\n",
    "                    maxPixels=1e13,\n",
    "                    bestEffort=False\n",
    "                ).getInfo()\n",
    "\n",
    "                # Extract the dictionary from the NumPy array\n",
    "                data_dict = np.array([data])[0]\n",
    "\n",
    "                # Extract the 'NBR' values from the dictionary\n",
    "                band_data = data_dict['NBR']\n",
    "\n",
    "                # Convert the list of values to a flat NumPy array\n",
    "                data_array = np.array(band_data)\n",
    "\n",
    "                # Convert the NumPy array to a Pandas DataFrame column\n",
    "                df = pd.DataFrame(data_array, columns= [this_loop_day])\n",
    "\n",
    "                if not df.empty:\n",
    "\n",
    "                    # Print the first 5 rows of the DataFrame\n",
    "                    # print(df.head())\n",
    "                    dnbr_arrays.append(df)\n",
    "\n",
    "            monthly_nbr = pd.concat(dnbr_arrays, axis = 1).rename(columns=lambda x: int(x))\n",
    "\n",
    "\n",
    "\n",
    "            #now combine all days and old compositing method and reduce with max\n",
    "            all_days_vi_image = ee.ImageCollection(all_days_vi).max()\n",
    "\n",
    "\n",
    "            all_days_vi_image = all_days_vi_image.updateMask(bad_fire_rast.neq(1))\n",
    "\n",
    "            #add y back\n",
    "            all_days_vi_image = all_days_vi_image.addBands(b10)\n",
    "\n",
    "\n",
    "            # Specify the scale in meters\n",
    "            scale = 30\n",
    "\n",
    "            all_days_vi_image_lat_lon = add_lat_long(all_days_vi_image.unmask(-999).select(['NBR', 'y']))\n",
    "\n",
    "            data_final = all_days_vi_image_lat_lon.select('NBR').reduceRegion(\n",
    "                reducer=ee.Reducer.toList(),\n",
    "                geometry=final_buffer2,\n",
    "                scale=scale,\n",
    "                maxPixels=1e13,\n",
    "                bestEffort=False\n",
    "            ).getInfo()\n",
    "\n",
    "\n",
    "\n",
    "            # Extract the dictionary from the NumPy array\n",
    "            data_dict_final = np.array([data_final])[0]\n",
    "\n",
    "            # Extract the values\n",
    "            data_nbr_final = data_dict_final['NBR']\n",
    "\n",
    "            data_final = all_days_vi_image_lat_lon.select('y').reduceRegion(\n",
    "                reducer=ee.Reducer.toList(),\n",
    "                geometry=final_buffer2,\n",
    "                scale=scale,\n",
    "                maxPixels=1e13,\n",
    "                bestEffort=False\n",
    "            ).getInfo()\n",
    "\n",
    "\n",
    "\n",
    "            # Extract the dictionary from the NumPy array\n",
    "            data_dict_final = np.array([data_final])[0]\n",
    "\n",
    "\n",
    "\n",
    "            data_y_final = data_dict_final['y']\n",
    "\n",
    "            data_final = all_days_vi_image_lat_lon.select('lat').reduceRegion(\n",
    "                reducer=ee.Reducer.toList(),\n",
    "                geometry=final_buffer2,\n",
    "                scale=scale,\n",
    "                maxPixels=1e13,\n",
    "                bestEffort=False\n",
    "            ).getInfo()\n",
    "\n",
    "\n",
    "\n",
    "            # Extract the dictionary from the NumPy array\n",
    "            data_dict_final = np.array([data_final])[0]\n",
    "\n",
    "            lat_data_final = data_dict_final['lat']\n",
    "\n",
    "            data_final = all_days_vi_image_lat_lon.select('lon').reduceRegion(\n",
    "                reducer=ee.Reducer.toList(),\n",
    "                geometry=final_buffer2,\n",
    "                scale=scale,\n",
    "                maxPixels=1e13,\n",
    "                bestEffort=False\n",
    "            ).getInfo()\n",
    "\n",
    "\n",
    "\n",
    "            # Extract the dictionary from the NumPy array\n",
    "            data_dict_final = np.array([data_final])[0]\n",
    "\n",
    "            lon_data_final = data_dict_final['lon']\n",
    "\n",
    "\n",
    "            # Convert the list of values to a flat NumPy array\n",
    "            lat_data_array_final = np.array(lat_data_final)\n",
    "            lon_data_array_final = np.array(lon_data_final)\n",
    "            nbr_data_array_final = np.array(data_nbr_final)\n",
    "            y_data_array_final = np.array(data_y_final)\n",
    "\n",
    "\n",
    "\n",
    "            # Convert the NumPy array to a Pandas DataFrame column\n",
    "            df_lat_lon_final = pd.DataFrame({'final_nbr': nbr_data_array_final, 'y': y_data_array_final, 'lat': lat_data_array_final, 'lon': lon_data_array_final})\n",
    "\n",
    "            #merge with the monthly values so I have y, lat and lon\n",
    "            df_lat_lon_final = df_lat_lon_final[['y', 'lat', 'lon']]\n",
    "\n",
    "            for_export = pd.concat([monthly_nbr, df_lat_lon_final], axis =1)\n",
    "\n",
    "\n",
    "\n",
    "            for_export.to_csv(os.path.join(out_path, fname), index = False)\n",
    "\n",
    "          \n",
    "\n",
    "\n",
    "            #start download\n",
    "            print(f\"Downloading {fname}\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(f\"{fname} is too large\")\n",
    "            pass\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dc600-5dd5-4f8b-bd82-aa188c977176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-gee_ml]",
   "language": "python",
   "name": "conda-env-.conda-gee_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
