{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will run the ak and canada models together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "import os\n",
    "import segmentation_models as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import concatenate, Conv2DTranspose, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Input, AvgPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_unet_collection import models\n",
    "import tensorflow_addons as tfa\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "# from tensorflow import tensorflow.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the testing data for all four models, plus all the mtbs data for models 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get all the pathways\n",
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_validation_files.csv')['Files'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "model_test = testing_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image generator for 3 bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "min_max = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "min_max = min_max[['6', '7', '8']]\n",
    "\n",
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    #will need pre defined variables batch_size, img_size, input_img_paths and target_img_paths\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "\t    self.batch_size = batch_size\n",
    "\t    self.img_size = img_size\n",
    "\t    self.input_img_paths = input_img_paths\n",
    "\t    self.target_img_paths = input_img_paths\n",
    "\n",
    "    #number of batches the generator is supposed to produceis the length of the paths divided by the batch siize\n",
    "    def __len__(self):\n",
    "\t    return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (x)\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (y)\n",
    "\t\t\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\") #create matrix of zeros which will have the dimension height, wideth, n_bands), 8 is the n_bands\n",
    "        \n",
    "  \n",
    "         #start populating x by enumerating over the input img paths\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, :-1]\n",
    "\n",
    "            # img = img * 1000\n",
    "            img = img.astype(float)\n",
    "            img = np.round(img, 3)\n",
    "            img[img == 0] = -999\n",
    "\n",
    "            img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "            img[img == -999] = np.nan\n",
    "\n",
    "            in_shape = img.shape\n",
    "            \n",
    "            #turn to dataframe to normalize\n",
    "            img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "\t\t\t\n",
    "            img = pd.DataFrame(img)\n",
    "\t\t\t\n",
    "            img.columns = min_max.columns\n",
    "\t\t\t\n",
    "            img = pd.concat([min_max, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "            #normalize 0 to 1\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\t\t\t\n",
    "            img = img.iloc[2:]\n",
    "#\n",
    "#             img = img.values.reshape(in_shape)\n",
    "            img = img.values.reshape(in_shape)\n",
    "\n",
    "#             replace nan with -1\n",
    "            img[np.isnan(img)] = -1\n",
    "\n",
    "#apply standardization\n",
    "# img = normalize(img, axis=(0,1))\n",
    "\n",
    "            img = np.round(img, 3)\n",
    "            #populate x\n",
    "            x[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "\n",
    "        #do tthe same thing for y\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "            img = img.astype(int)\n",
    "\n",
    "            img[img < 0] = 0\n",
    "            img[img >1] = 0\n",
    "            img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "            img[np.isnan(img)] = 0\n",
    "            img = img.astype(int)\n",
    "\n",
    "            # img =  tf.keras.utils.to_categorical(img, num_classes = 2)\n",
    "            # y[j] = np.expand_dims(img, 2) \n",
    "            y[j] = img\n",
    "  \n",
    "       \n",
    "    #Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "    # y[j] -= 1\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the images based on the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "#batch size and img size\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "#russia 2015 2019 testing generator\n",
    "models_vi_gen =  img_gen_vi(batch_size, img_size, model_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #nbac mtbs\n",
    "# model_1 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/nbac_mtbs_2015_2019.tf\", \n",
    "#                                            custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "#                                                            'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "#                                                             'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "#                                                              'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "#                                                               'accuracy': 'accuracy'})\n",
    "\n",
    "# #russia\n",
    "# model_2 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_2015_2019.tf\", \n",
    "#                                            custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "#                                                            'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "#                                                             'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "#                                                              'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "# #combined\n",
    "# model_3 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_2015_2019.tf\", \n",
    "#                                            custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "#                                                            'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "#                                                             'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "#                                                              'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "#---------------new try\n",
    "#nbac mtbs\n",
    "model_1 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_regularize.tf\", \n",
    "                                           custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "                                                           'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "                                                            'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                             'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                                                              'accuracy': 'accuracy'})\n",
    "\n",
    "# #russia\n",
    "# model_2 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_2015_2019_no_regularize.tf\", \n",
    "#                                            custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "#                                                            'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "#                                                             'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "#                                                              'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "# #combined\n",
    "# model_3 = tensorflow.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_no_reg_2015_2019.tf\", \n",
    "#                                            custom_objects={'precision':sm.metrics.Precision(threshold=0.5), \n",
    "#                                                            'recall':sm.metrics.Recall(threshold = 0.5),\n",
    "#                                                             'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "#                                                              'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 testing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/216819093.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_1_res = model_1.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 testing IoU is 0.8034630417823792\n",
      "Model 1 testing precision is 0.9299473762512207\n",
      "Model 1 testing recall is 0.8552743792533875\n",
      "Model 1 testing f1 is 0.8903196454048157\n",
      "Model 1 testing accuracy is 0.9543337821960449\n"
     ]
    }
   ],
   "source": [
    "model_1_res = model_1.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_1_res[-2]\n",
    "precision = model_1_res[-5]\n",
    "recall = model_1_res[-4]\n",
    "f1 = model_1_res[-3]\n",
    "accuracy = model_1_res[-1]\n",
    "\n",
    "print(f\"Model 1 testing IoU is {iou}\")\n",
    "print(f\"Model 1 testing precision is {precision}\")\n",
    "print(f\"Model 1 testing recall is {recall}\")\n",
    "print(f\"Model 1 testing f1 is {f1}\")\n",
    "print(f\"Model 1 testing accuracy is {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_1_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 testing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1343492/1093850466.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_2_res = model_2.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 testing IoU is 0.5864999294281006\n",
      "Model 2 testing precision is 0.7678250670433044\n",
      "Model 2 testing recall is 0.7140936851501465\n",
      "Model 2 testing f1 is 0.7334617376327515\n",
      "Model 2 testing accuracy is 0.8893437385559082\n"
     ]
    }
   ],
   "source": [
    "model_2_res = model_2.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_2_res[-2]\n",
    "precision = model_2_res[-5]\n",
    "recall = model_2_res[-4]\n",
    "f1 = model_2_res[-3]\n",
    "accuracy = model_2_res[-1]\n",
    "\n",
    "print(f\"Model 2 testing IoU is {iou}\")\n",
    "print(f\"Model 2 testing precision is {precision}\")\n",
    "print(f\"Model 2 testing recall is {recall}\")\n",
    "print(f\"Model 2 testing f1 is {f1}\")\n",
    "print(f\"Model 2 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 testing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1343492/121600142.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 testing IoU is 0.5609645247459412\n",
      "Model 3 testing precision is 0.8144935369491577\n",
      "Model 3 testing recall is 0.6458084583282471\n",
      "Model 3 testing f1 is 0.7122849822044373\n",
      "Model 3 testing accuracy is 0.8887165784835815\n"
     ]
    }
   ],
   "source": [
    "model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_3_res[-2]\n",
    "precision = model_3_res[-5]\n",
    "recall = model_3_res[-4]\n",
    "f1 = model_3_res[-3]\n",
    "accuracy = model_3_res[-1]\n",
    "\n",
    "print(f\"Model 3 testing IoU is {iou}\")\n",
    "print(f\"Model 3 testing precision is {precision}\")\n",
    "print(f\"Model 3 testing recall is {recall}\")\n",
    "print(f\"Model 3 testing f1 is {f1}\")\n",
    "print(f\"Model 3 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4 testing scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2015 Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#get all the pathways\n",
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_validation_files.csv')['Files'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "# #shapefile\n",
    "in_shape = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_polygons.shp')\n",
    "\n",
    "in_shape['Year'] = in_shape['Year'].astype(int)\n",
    "\n",
    "remove = in_shape[in_shape['Year'] .isin ([2015])]\n",
    "\n",
    "remove = remove[remove['ID'] .isin (['29690000000000-0000000000', '29690000000000-0000023296']) == False]\n",
    "remove['ID'] = remove['ID'].astype(str) + '.npy'\n",
    "\n",
    "remove = remove['ID'].tolist()\n",
    "\n",
    "\n",
    "def filter_items_by_ending(original_list, unwanted_endings):\n",
    "    \"\"\"\n",
    "    Filters out items from the original list that end with any of the specified unwanted endings.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_list: List of strings to be filtered.\n",
    "    - unwanted_endings: List of string endings based on which items will be removed from the original list.\n",
    "    \n",
    "    Returns:\n",
    "    - List of filtered items not ending with any of the unwanted endings.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to filter out unwanted items\n",
    "    return [item for item in original_list if not any(item.endswith(ending) for ending in unwanted_endings)]\n",
    "\n",
    "training_names = filter_items_by_ending(training_names,remove)\n",
    "validation_names = filter_items_by_ending(validation_names,remove)\n",
    "testing_names = filter_items_by_ending(testing_names, remove)\n",
    "training_names = training_names + testing_names\n",
    "\n",
    "#get all the pathways fro making a new testin set\n",
    "training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_validation_files.csv')['Files'].tolist()\n",
    "testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "\n",
    "def keep_items_by_ending(original_list, wanted_endings):\n",
    "    \"\"\"\n",
    "    Keeps only the items from the original list that end with any of the specified wanted endings.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_list: List of strings to be filtered.\n",
    "    - wanted_endings: List of string endings based on which items will be kept in the original list.\n",
    "    \n",
    "    Returns:\n",
    "    - List of items ending with any of the wanted endings.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to filter and keep wanted items\n",
    "    return [item for item in original_list if any(item.endswith(ending) for ending in wanted_endings)]\n",
    "\n",
    "training_names2 = keep_items_by_ending(training_names2,remove)\n",
    "validation_names2 = keep_items_by_ending(validation_names2,remove)\n",
    "testing_names2 = keep_items_by_ending(testing_names2, remove)\n",
    "\n",
    "model_test = training_names2 + validation_names2 + testing_names2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "#batch size and img size\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "#russia 2015 2019 testing generator\n",
    "models_vi_gen =  img_gen_vi(batch_size, img_size, model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/21262767.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_1_res = model_1.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 testing IoU is 0.5112721920013428\n",
      "Model 1 testing precision is 0.8420653343200684\n",
      "Model 1 testing recall is 0.5660997629165649\n",
      "Model 1 testing f1 is 0.6733208298683167\n",
      "Model 1 testing accuracy is 0.8786551356315613\n"
     ]
    }
   ],
   "source": [
    "model_1_res = model_1.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_1_res[-2]\n",
    "precision = model_1_res[-5]\n",
    "recall = model_1_res[-4]\n",
    "f1 = model_1_res[-3]\n",
    "accuracy = model_1_res[-1]\n",
    "\n",
    "print(f\"Model 1 testing IoU is {iou}\")\n",
    "print(f\"Model 1 testing precision is {precision}\")\n",
    "print(f\"Model 1 testing recall is {recall}\")\n",
    "print(f\"Model 1 testing f1 is {f1}\")\n",
    "print(f\"Model 1 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/1093850466.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_2_res = model_2.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 testing IoU is 0.5882929563522339\n",
      "Model 2 testing precision is 0.7685894966125488\n",
      "Model 2 testing recall is 0.7149486541748047\n",
      "Model 2 testing f1 is 0.7382818460464478\n",
      "Model 2 testing accuracy is 0.8881325721740723\n"
     ]
    }
   ],
   "source": [
    "model_2_res = model_2.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_2_res[-2]\n",
    "precision = model_2_res[-5]\n",
    "recall = model_2_res[-4]\n",
    "f1 = model_2_res[-3]\n",
    "accuracy = model_2_res[-1]\n",
    "\n",
    "print(f\"Model 2 testing IoU is {iou}\")\n",
    "print(f\"Model 2 testing precision is {precision}\")\n",
    "print(f\"Model 2 testing recall is {recall}\")\n",
    "print(f\"Model 2 testing f1 is {f1}\")\n",
    "print(f\"Model 2 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/121600142.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 testing IoU is 0.5640661716461182\n",
      "Model 3 testing precision is 0.8235633373260498\n",
      "Model 3 testing recall is 0.6427218914031982\n",
      "Model 3 testing f1 is 0.7184184789657593\n",
      "Model 3 testing accuracy is 0.8889491558074951\n"
     ]
    }
   ],
   "source": [
    "model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_3_res[-2]\n",
    "precision = model_3_res[-5]\n",
    "recall = model_3_res[-4]\n",
    "f1 = model_3_res[-3]\n",
    "accuracy = model_3_res[-1]\n",
    "\n",
    "print(f\"Model 3 testing IoU is {iou}\")\n",
    "print(f\"Model 3 testing precision is {precision}\")\n",
    "print(f\"Model 3 testing recall is {recall}\")\n",
    "print(f\"Model 3 testing f1 is {f1}\")\n",
    "print(f\"Model 3 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018 only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#get all the pathways\n",
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_validation_files.csv')['Files'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "# #shapefile\n",
    "in_shape = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_polygons.shp')\n",
    "\n",
    "in_shape['Year'] = in_shape['Year'].astype(int)\n",
    "\n",
    "remove = in_shape[in_shape['Year'] .isin ([2018])]\n",
    "\n",
    "remove = remove[remove['ID'] .isin (['29690000000000-0000000000', '29690000000000-0000023296']) == False]\n",
    "remove['ID'] = remove['ID'].astype(str) + '.npy'\n",
    "\n",
    "remove = remove['ID'].tolist()\n",
    "\n",
    "\n",
    "def filter_items_by_ending(original_list, unwanted_endings):\n",
    "    \"\"\"\n",
    "    Filters out items from the original list that end with any of the specified unwanted endings.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_list: List of strings to be filtered.\n",
    "    - unwanted_endings: List of string endings based on which items will be removed from the original list.\n",
    "    \n",
    "    Returns:\n",
    "    - List of filtered items not ending with any of the unwanted endings.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to filter out unwanted items\n",
    "    return [item for item in original_list if not any(item.endswith(ending) for ending in unwanted_endings)]\n",
    "\n",
    "training_names = filter_items_by_ending(training_names,remove)\n",
    "validation_names = filter_items_by_ending(validation_names,remove)\n",
    "testing_names = filter_items_by_ending(testing_names, remove)\n",
    "training_names = training_names + testing_names\n",
    "\n",
    "#get all the pathways fro making a new testin set\n",
    "training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_validation_files.csv')['Files'].tolist()\n",
    "testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "\n",
    "def keep_items_by_ending(original_list, wanted_endings):\n",
    "    \"\"\"\n",
    "    Keeps only the items from the original list that end with any of the specified wanted endings.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_list: List of strings to be filtered.\n",
    "    - wanted_endings: List of string endings based on which items will be kept in the original list.\n",
    "    \n",
    "    Returns:\n",
    "    - List of items ending with any of the wanted endings.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to filter and keep wanted items\n",
    "    return [item for item in original_list if any(item.endswith(ending) for ending in wanted_endings)]\n",
    "\n",
    "training_names2 = keep_items_by_ending(training_names2,remove)\n",
    "validation_names2 = keep_items_by_ending(validation_names2,remove)\n",
    "testing_names2 = keep_items_by_ending(testing_names2, remove)\n",
    "\n",
    "model_test = training_names2 + validation_names2 + testing_names2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "#batch size and img size\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "#russia 2015 2019 testing generator\n",
    "models_vi_gen =  img_gen_vi(batch_size, img_size, model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/21262767.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_1_res = model_1.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 testing IoU is 0.6879123449325562\n",
      "Model 1 testing precision is 0.8904786705970764\n",
      "Model 1 testing recall is 0.7526465654373169\n",
      "Model 1 testing f1 is 0.8140120506286621\n",
      "Model 1 testing accuracy is 0.9214476943016052\n"
     ]
    }
   ],
   "source": [
    "model_1_res = model_1.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_1_res[-2]\n",
    "precision = model_1_res[-5]\n",
    "recall = model_1_res[-4]\n",
    "f1 = model_1_res[-3]\n",
    "accuracy = model_1_res[-1]\n",
    "\n",
    "print(f\"Model 1 testing IoU is {iou}\")\n",
    "print(f\"Model 1 testing precision is {precision}\")\n",
    "print(f\"Model 1 testing recall is {recall}\")\n",
    "print(f\"Model 1 testing f1 is {f1}\")\n",
    "print(f\"Model 1 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/1093850466.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_2_res = model_2.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 testing IoU is 0.8963956832885742\n",
      "Model 2 testing precision is 0.9531925916671753\n",
      "Model 2 testing recall is 0.9369732737541199\n",
      "Model 2 testing f1 is 0.9449449181556702\n",
      "Model 2 testing accuracy is 0.9750679135322571\n"
     ]
    }
   ],
   "source": [
    "model_2_res = model_2.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_2_res[-2]\n",
    "precision = model_2_res[-5]\n",
    "recall = model_2_res[-4]\n",
    "f1 = model_2_res[-3]\n",
    "accuracy = model_2_res[-1]\n",
    "\n",
    "print(f\"Model 2 testing IoU is {iou}\")\n",
    "print(f\"Model 2 testing precision is {precision}\")\n",
    "print(f\"Model 2 testing recall is {recall}\")\n",
    "print(f\"Model 2 testing f1 is {f1}\")\n",
    "print(f\"Model 2 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/121600142.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 testing IoU is 0.788118839263916\n",
      "Model 3 testing precision is 0.9034174084663391\n",
      "Model 3 testing recall is 0.8606230616569519\n",
      "Model 3 testing f1 is 0.8807426691055298\n",
      "Model 3 testing accuracy is 0.9467858672142029\n"
     ]
    }
   ],
   "source": [
    "model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_3_res[-2]\n",
    "precision = model_3_res[-5]\n",
    "recall = model_3_res[-4]\n",
    "f1 = model_3_res[-3]\n",
    "accuracy = model_3_res[-1]\n",
    "\n",
    "print(f\"Model 3 testing IoU is {iou}\")\n",
    "print(f\"Model 3 testing precision is {precision}\")\n",
    "print(f\"Model 3 testing recall is {recall}\")\n",
    "print(f\"Model 3 testing f1 is {f1}\")\n",
    "print(f\"Model 3 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2010 only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/geopandas/geodataframe.py:1543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "in_shape = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_polygons.shp')\n",
    "\n",
    "in_shape['Year'] = in_shape['Year'].astype(int)\n",
    "\n",
    "remove = in_shape[in_shape['Year'] .isin ([2010])]\n",
    "\n",
    "emove = remove[remove['ID'] .isin (['29690000000000-0000000000', '29690000000000-0000023296']) == False]\n",
    "remove['ID'] = remove['ID'].astype(str) + '.npy'\n",
    "\n",
    "remove = remove['ID'].tolist()\n",
    "\n",
    "\n",
    "def filter_items_by_ending(original_list, unwanted_endings):\n",
    "    \"\"\"\n",
    "    Filters out items from the original list that end with any of the specified unwanted endings.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_list: List of strings to be filtered.\n",
    "    - unwanted_endings: List of string endings based on which items will be removed from the original list.\n",
    "    \n",
    "    Returns:\n",
    "    - List of filtered items not ending with any of the unwanted endings.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to filter out unwanted items\n",
    "    return [item for item in original_list if not any(item.endswith(ending) for ending in unwanted_endings)]\n",
    "\n",
    "training_names = filter_items_by_ending(training_names,remove)\n",
    "validation_names = filter_items_by_ending(validation_names,remove)\n",
    "testing_names = filter_items_by_ending(testing_names, remove)\n",
    "training_names = training_names + testing_names\n",
    "\n",
    "#get all the pathways fro making a new testin set\n",
    "training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_validation_files.csv')['Files'].tolist()\n",
    "testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "\n",
    "def keep_items_by_ending(original_list, wanted_endings):\n",
    "    \"\"\"\n",
    "    Keeps only the items from the original list that end with any of the specified wanted endings.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_list: List of strings to be filtered.\n",
    "    - wanted_endings: List of string endings based on which items will be kept in the original list.\n",
    "    \n",
    "    Returns:\n",
    "    - List of items ending with any of the wanted endings.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to filter and keep wanted items\n",
    "    return [item for item in original_list if any(item.endswith(ending) for ending in wanted_endings)]\n",
    "\n",
    "training_names2 = keep_items_by_ending(training_names2,remove)\n",
    "validation_names2 = keep_items_by_ending(validation_names2,remove)\n",
    "testing_names2 = keep_items_by_ending(testing_names2, remove)\n",
    "\n",
    "model_test = training_names2 + validation_names2 + testing_names2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "#batch size and img size\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "#russia 2015 2019 testing generator\n",
    "models_vi_gen =  img_gen_vi(batch_size, img_size, model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/121600142.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 testing IoU is 0.825490415096283\n",
      "Model 3 testing precision is 0.959470272064209\n",
      "Model 3 testing recall is 0.8551809191703796\n",
      "Model 3 testing f1 is 0.9037761092185974\n",
      "Model 3 testing accuracy is 0.9658539295196533\n"
     ]
    }
   ],
   "source": [
    "model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_3_res[-2]\n",
    "precision = model_3_res[-5]\n",
    "recall = model_3_res[-4]\n",
    "f1 = model_3_res[-3]\n",
    "accuracy = model_3_res[-1]\n",
    "\n",
    "print(f\"Model 3 testing IoU is {iou}\")\n",
    "print(f\"Model 3 testing precision is {precision}\")\n",
    "print(f\"Model 3 testing recall is {recall}\")\n",
    "print(f\"Model 3 testing f1 is {f1}\")\n",
    "print(f\"Model 3 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2011 Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/deeplearning3/lib/python3.10/site-packages/geopandas/geodataframe.py:1543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "in_shape = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_polygons.shp')\n",
    "\n",
    "in_shape['Year'] = in_shape['Year'].astype(int)\n",
    "\n",
    "remove = in_shape[in_shape['Year'] .isin ([2011])]\n",
    "\n",
    "emove = remove[remove['ID'] .isin (['29690000000000-0000000000', '29690000000000-0000023296']) == False]\n",
    "remove['ID'] = remove['ID'].astype(str) + '.npy'\n",
    "\n",
    "remove = remove['ID'].tolist()\n",
    "\n",
    "\n",
    "def filter_items_by_ending(original_list, unwanted_endings):\n",
    "    \"\"\"\n",
    "    Filters out items from the original list that end with any of the specified unwanted endings.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_list: List of strings to be filtered.\n",
    "    - unwanted_endings: List of string endings based on which items will be removed from the original list.\n",
    "    \n",
    "    Returns:\n",
    "    - List of filtered items not ending with any of the unwanted endings.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to filter out unwanted items\n",
    "    return [item for item in original_list if not any(item.endswith(ending) for ending in unwanted_endings)]\n",
    "\n",
    "training_names = filter_items_by_ending(training_names,remove)\n",
    "validation_names = filter_items_by_ending(validation_names,remove)\n",
    "testing_names = filter_items_by_ending(testing_names, remove)\n",
    "training_names = training_names + testing_names\n",
    "\n",
    "#get all the pathways fro making a new testin set\n",
    "training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_training_files.csv')['Files'].tolist()\n",
    "validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_validation_files.csv')['Files'].tolist()\n",
    "testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "\n",
    "def keep_items_by_ending(original_list, wanted_endings):\n",
    "    \"\"\"\n",
    "    Keeps only the items from the original list that end with any of the specified wanted endings.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_list: List of strings to be filtered.\n",
    "    - wanted_endings: List of string endings based on which items will be kept in the original list.\n",
    "    \n",
    "    Returns:\n",
    "    - List of items ending with any of the wanted endings.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to filter and keep wanted items\n",
    "    return [item for item in original_list if any(item.endswith(ending) for ending in wanted_endings)]\n",
    "\n",
    "training_names2 = keep_items_by_ending(training_names2,remove)\n",
    "validation_names2 = keep_items_by_ending(validation_names2,remove)\n",
    "testing_names2 = keep_items_by_ending(testing_names2, remove)\n",
    "\n",
    "model_test = training_names2 + validation_names2 + testing_names2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "#batch size and img size\n",
    "BATCH_SIZE = 45\n",
    "GPUS = [\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"]\n",
    "strategy = tensorflow.distribute.MirroredStrategy() #can add GPUS here to select specific ones\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "#image size\n",
    "img_size = (128, 128)\n",
    "\n",
    "#number of classes to predict\n",
    "num_classes = 1\n",
    "\n",
    "#russia 2015 2019 testing generator\n",
    "models_vi_gen =  img_gen_vi(batch_size, img_size, model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998823/121600142.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 testing IoU is 0.8015790581703186\n",
      "Model 3 testing precision is 0.9532454609870911\n",
      "Model 3 testing recall is 0.8342726826667786\n",
      "Model 3 testing f1 is 0.8891804218292236\n",
      "Model 3 testing accuracy is 0.9497191905975342\n"
     ]
    }
   ],
   "source": [
    "model_3_res = model_3.evaluate_generator(models_vi_gen, 100)\n",
    "\n",
    "iou = model_3_res[-2]\n",
    "precision = model_3_res[-5]\n",
    "recall = model_3_res[-4]\n",
    "f1 = model_3_res[-3]\n",
    "accuracy = model_3_res[-1]\n",
    "\n",
    "print(f\"Model 3 testing IoU is {iou}\")\n",
    "print(f\"Model 3 testing precision is {precision}\")\n",
    "print(f\"Model 3 testing recall is {recall}\")\n",
    "print(f\"Model 3 testing f1 is {f1}\")\n",
    "print(f\"Model 3 testing accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
